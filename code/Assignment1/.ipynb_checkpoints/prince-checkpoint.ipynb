{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"tocheading\">Table of Contents</h3>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import random\n",
    "import spacy\n",
    "import string\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize & Lowercase\n",
    "\n",
    "__Note__: Tokenizer modified to return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "## tokenizer modified for ngrams\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent,n=None):\n",
    "    \"\"\"n = n-gram's n\"\"\"\n",
    "    tokens = tokenizer(sent)\n",
    "    assert n >= 1, \"n should be greater than or equal to 1\"\n",
    "    \n",
    "    ## for unigrams\n",
    "    if n == 1:\n",
    "        start_unigrams = [token.text.lower() \\\n",
    "                          for token in tokens if (token.text not in punctuations)]\n",
    "        return start_unigrams\n",
    "    \n",
    "    ## for n > 1 n-grams\n",
    "    else:\n",
    "        start_unigrams = [token.text.lower() \\\n",
    "                          for token in tokens if (token.text not in punctuations)]\n",
    "        ## get copy to preserve original unigram list \n",
    "        start_unigrams_copy = start_unigrams.copy()\n",
    "        ## start from 2\n",
    "        ngram = 2\n",
    "        while ngram <= n:\n",
    "            ngram_tokens = [\" \".join(start_unigrams_copy[x:x+ngram])\\\n",
    "                            for x in [*range(len(start_unigrams_copy)-ngram+1)]]\n",
    "            ## union \n",
    "            start_unigrams.extend(ngram_tokens)\n",
    "            ## increase n until specified ngrams\n",
    "            ngram += 1\n",
    "            \n",
    "        return start_unigrams\n",
    "\n",
    "def tokenize_dataset(dataset,n=None):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample,n)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# max_vocab_size = 10000\n",
    "# # save index 0 for unk and 1 for pad\n",
    "# PAD_IDX = 0\n",
    "# UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens,size=max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(size))\n",
    "    id2token = [*vocab]\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "# token2id, id2token = build_vocab(all_train_tokens,size=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 5270 ; token as some\n",
      "Token as some; token id 5270\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert token to id in the dataset\n",
    "## copied from lab3 notebook\n",
    "def token2index_dataset(tokens_data,idx=None):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [idx[token] if token in \\\n",
    "                      idx else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "# train_data_indices = token2index_dataset(train_data_tokens)\n",
    "# val_data_indices = token2index_dataset(val_data_tokens)\n",
    "# test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# # double checking\n",
    "# print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "# print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "# print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def imdb_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_data_labels)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_data_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_data_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfNgrams(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfNgrams classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfNgrams, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "# emb_dim = 100\n",
    "# model = BagOfNgrams(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "## try both sgd and adam\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#         model.train()\n",
    "#         data_batch, length_batch, label_batch = data, lengths, labels\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(data_batch, length_batch)\n",
    "#         loss = criterion(outputs, label_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # validate every 100 iterations\n",
    "#         if i > 0 and i % 100 == 0:\n",
    "#             # validate\n",
    "#             val_acc = test_model(val_loader, model)\n",
    "#             print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "#                        epoch+1, num_epochs, i+1, len(train_loader), val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Seach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you try different preprocessing and hyperparameters for the\n",
    "model, including but not limited to:\n",
    "\n",
    "- Tokenization schemes of the dataset.\n",
    "- Model hyperparameters: Vary n for n-gram (n=1; 2; 3; 4), vocabulary size\n",
    "and embedding size.\n",
    "- Optimization hyperparameters: Optimizer itself (SGD vs Adam), learning\n",
    "rate and whether or not you use linear annealing of learning rate (learning\n",
    "rate is reduced linearly over the course of training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ngrams\n",
    "## learning rate\n",
    "## vocab size\n",
    "## embedding size\n",
    "## optimizer sgd vs adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-44ff9a297486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m### -- FOR ONCE FOR ADAM AND ONCE FOR SGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m optimizers = [torch.optim.Adam(model.parameters(), lr=learning_rate),\\\n\u001b[0m\u001b[1;32m     16\u001b[0m              torch.optim.SGD(model.parameters(), lr=learning_rate)]\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading NGRAM_DATASETS\n",
      "Unigrams train = ['', 'just', '\"ppvd\"', 'this', 'i']\n",
      "Unigrams all = ['baseketball', 'is', 'indeed', 'a', 'really']\n",
      "Unigrams Val = ['', 'oh', 'where', 'are', 'you']\n",
      "Bigrams read\n",
      "Trigrams read\n",
      "NGRAM_DATASETS READING DONE\n"
     ]
    }
   ],
   "source": [
    "print (\"Reading NGRAM_DATASETS\")\n",
    "\n",
    "train_size = 20000\n",
    "val_size = 5000\n",
    "def read_ngram_dataset(path=None):\n",
    "    ## clean the strings - train\n",
    "    datagrams_train = pd.DataFrame(pd.read_csv(path,header=None))[1][0]\\\n",
    "    .replace(\"/\",\"\").replace(\"[\",\"\").split(\"]\")\n",
    "    \n",
    "    datagrams_train = [x.replace(\"'\",\"\").split(\", \") for x in datagrams_train][:train_size]\n",
    "    datagrams_train = [x if x not in [\"\",\" \"] else None for x in datagrams_train]\n",
    "    ## clean the strings - all\n",
    "    datagrams_all = pd.DataFrame(pd.read_csv(path,header=None))[1][1]\\\n",
    "    .replace(\"/\",\"\").replace(\"[\",\"\").split(\"]\")\n",
    "    \n",
    "    datagrams_all = [x if x not in [\"\",\" \"] else None for x in datagrams_all]\n",
    "    ## clean the strings - val\n",
    "    datagrams_val = pd.DataFrame(pd.read_csv(path,header=None))[1][2]\\\n",
    "    .replace(\"/\",\"\").replace(\"[\",\"\").split(\"]\")\n",
    "    datagrams_val = [x.replace(\"'\",\"\").replace(\"'\",\"\").split(\", \") for x in datagrams_val][:val_size]\n",
    "    datagrams_val = [x if x not in [\"\",\" \"] else None for x in datagrams_val]\n",
    "    \n",
    "    return [datagrams_train,datagrams_all,datagrams_val]\n",
    "\n",
    "unigrams_train, unigrams_all, unigrams_val = read_ngram_dataset(\"Unigrams.csv\")\n",
    "bigrams_train, bigrams_all, bigrams_val = read_ngram_dataset(\"bigrams.csv\")\n",
    "trigrams_train, trigrams_all, trigrams_val = read_ngram_dataset(\"trigrams.csv\")\n",
    "\n",
    "NGRAM_DATASETS = {1:[unigrams_train,unigrams_all,unigrams_val],\n",
    "                  2:[bigrams_train,bigrams_all,bigrams_val],\n",
    "                  3:[trigrams_train,trigrams_all,trigrams_val]}\n",
    "\n",
    "print (\"NGRAM_DATASETS READING DONE\")\n",
    "## HYPERPARAMETER SEARCH ON VALIDATION SET\n",
    "train_data_labels = list(pd.DataFrame(pd.read_csv(\"train_data_labels.csv\"))[\"0\"])\n",
    "val_data_labels = list(pd.DataFrame(pd.read_csv(\"val_data_labels.csv\"))[\"0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPERPARAMETER SEARCH ON VALIDATION SET\n",
    "\n",
    "def hyperparameter_search(hyperparameter_space=params):\n",
    "    \"\"\"Takes as input a list of parameter search space lists.\"\"\"\n",
    "    ## CRITERION: ONLY CROSS ENTROPY LOSS FOR NOW\n",
    "    param_space = [*itertools.product(*params)]\n",
    "    \n",
    "    ## val loss dict\n",
    "    param_losses = {}\n",
    "    \n",
    "    for i in param_space:\n",
    "        print (i)\n",
    "        \n",
    "        ## will append validation losses here\n",
    "        param_losses[i] = []\n",
    "        \n",
    "        NUM_EPOCHS = 10\n",
    "        \n",
    "        step = i[0] ## learning rate\n",
    "        grams = i[1] ## n-grams\n",
    "        vocab_size = i[2] ## vocab size\n",
    "        embed_dimension = i[3] ## embedding size\n",
    "        max_sentence_length = i[4] ## max sentence length of data loader\n",
    "        batch_size = i[5]\n",
    "\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        ## tokenize training and validation data\n",
    "        \n",
    "        train_data_tokens = NGRAM_DATASETS[grams][0]\n",
    "        all_train_tokens = NGRAM_DATASETS[grams][1]\n",
    "        val_data_tokens = NGRAM_DATASETS[grams][2]\n",
    "            \n",
    "        train_data_tokens = NGRAM_DATASETS[grams][0]\n",
    "        all_train_tokens = NGRAM_DATASETS[grams][1]\n",
    "        val_data_tokens = NGRAM_DATASETS[grams][2]\n",
    "\n",
    "        ## build vocab for the specified vocab size\n",
    "        token2id, id2token = build_vocab(all_train_tokens,\n",
    "                                        size=vocab_size)\n",
    "\n",
    "        train_data_indices = token2index_dataset(train_data_tokens,\n",
    "                                                 idx=token2id)\n",
    "        val_data_indices = token2index_dataset(val_data_tokens,\n",
    "                                              idx=token2id)\n",
    "\n",
    "        ## assign max sentence length and batch size from \n",
    "        ## parameter space\n",
    "        MAX_SENTENCE_LENGTH = max_sentence_length\n",
    "        BATCH_SIZE = batch_size\n",
    "\n",
    "        ## load train and val data\n",
    "        train_dataset = NewsGroupDataset(train_data_indices, train_data_labels)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        val_dataset = NewsGroupDataset(val_data_indices, val_data_labels)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        print (\"Datasets ready.\")\n",
    "        ## assign embedding dimension\n",
    "        ## from parameter space    \n",
    "\n",
    "        ## model\n",
    "        model = BagOfNgrams(len(id2token), embed_dimension)\n",
    "#         print (\"Model ready.\")\n",
    "#         optimizers = [torch.optim.Adam(model.parameters(), lr=step),\n",
    "#                       torch.optim.SGD(model.parameters(), lr=step)]\n",
    "    \n",
    "        optimizers = [torch.optim.Adam(model.parameters(), lr=step)]\n",
    "        \n",
    "        for optimizer in optimizers:\n",
    "#             print (\"Optimizer type: \"+str(optimizer))\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                for x, (data, lengths, labels) in enumerate(train_loader):\n",
    "                    model.train()\n",
    "                    data_batch, length_batch, label_batch = data, lengths, labels\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(data_batch, length_batch)\n",
    "                    loss = criterion(outputs, label_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # validate every 100 iterations\n",
    "                    if x > 0 and x % 100 == 0:\n",
    "                        # validate\n",
    "                        val_acc = test_model(val_loader, model)\n",
    "                        param_losses[i].append(val_acc)\n",
    "                        \n",
    "    print (\"Hyperparameter search done!\")\n",
    "    return param_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out of 10'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGRAM_DATASETS[3][0][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.001, 3, 500000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.001, 3, 500000, 100, 200, 64)\n",
      "Datasets ready.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-d888bb57c637>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m          ]\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mparams_val_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameter_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# params_accuracy_df = pd.DataFrame(param_val_losses_unigrams,columns=[\"\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-c45807085803>\u001b[0m in \u001b[0;36mhyperparameter_search\u001b[0;34m(hyperparameter_space)\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0;31m# validate every 100 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## START WITH UNIGRAMS\n",
    "\n",
    "params = [[1e-3,1e-2,1e-1,1,10], ## learning rates\n",
    "          [3], ## ngrams\n",
    "          [500000], ## vocab size\n",
    "          [100,200], ## embedding size\n",
    "          [200], ## max sentence length\n",
    "          [32,64] ## batch size\n",
    "         ]\n",
    "\n",
    "params_val_losses = hyperparameter_search(params)\n",
    "# params_accuracy_df = pd.DataFrame(param_val_losses_unigrams,columns=[\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [*params_val_losses.keys()]:\n",
    "    params_val_losses[key] = str(params_val_losses[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(params_val_losses,index=range(len([*params_val_losses.keys()]))).to_csv(\"trigram_losses.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer = SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPERPARAMETER SEARCH ON VALIDATION SET\n",
    "\n",
    "def hyperparameter_search(hyperparameter_space=params):\n",
    "    \"\"\"Takes as input a list of parameter search space lists.\"\"\"\n",
    "    ## CRITERION: ONLY CROSS ENTROPY LOSS FOR NOW\n",
    "    param_space = [*itertools.product(*params)]\n",
    "    \n",
    "    ## val loss dict\n",
    "    param_losses = {}\n",
    "    \n",
    "    for i in param_space:\n",
    "        print (i)\n",
    "        \n",
    "        ## will append validation losses here\n",
    "        param_losses[i] = []\n",
    "        \n",
    "        NUM_EPOCHS = 10\n",
    "        \n",
    "        step = i[0] ## learning rate\n",
    "        grams = i[1] ## n-grams\n",
    "        vocab_size = i[2] ## vocab size\n",
    "        embed_dimension = i[3] ## embedding size\n",
    "        max_sentence_length = i[4] ## max sentence length of data loader\n",
    "        batch_size = i[5]\n",
    "\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        ## tokenize training and validation data\n",
    "        \n",
    "        train_data_tokens = NGRAM_DATASETS[grams][0]\n",
    "        all_train_tokens = NGRAM_DATASETS[grams][1]\n",
    "        val_data_tokens = NGRAM_DATASETS[grams][2]\n",
    "            \n",
    "        train_data_tokens = NGRAM_DATASETS[grams][0]\n",
    "        all_train_tokens = NGRAM_DATASETS[grams][1]\n",
    "        val_data_tokens = NGRAM_DATASETS[grams][2]\n",
    "\n",
    "        ## build vocab for the specified vocab size\n",
    "        token2id, id2token = build_vocab(all_train_tokens,\n",
    "                                        size=vocab_size)\n",
    "\n",
    "        train_data_indices = token2index_dataset(train_data_tokens,\n",
    "                                                 idx=token2id)\n",
    "        val_data_indices = token2index_dataset(val_data_tokens,\n",
    "                                              idx=token2id)\n",
    "\n",
    "        ## assign max sentence length and batch size from \n",
    "        ## parameter space\n",
    "        MAX_SENTENCE_LENGTH = max_sentence_length\n",
    "        BATCH_SIZE = batch_size\n",
    "\n",
    "        ## load train and val data\n",
    "        train_dataset = NewsGroupDataset(train_data_indices, train_data_labels)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        val_dataset = NewsGroupDataset(val_data_indices, val_data_labels)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        print (\"Datasets ready.\")\n",
    "        ## assign embedding dimension\n",
    "        ## from parameter space    \n",
    "\n",
    "        ## model\n",
    "        model = BagOfNgrams(len(id2token), embed_dimension)\n",
    "#         print (\"Model ready.\")\n",
    "#         optimizers = [torch.optim.Adam(model.parameters(), lr=step),\n",
    "#                       torch.optim.SGD(model.parameters(), lr=step)]\n",
    "    \n",
    "        optimizers = [torch.optim.SGD(model.parameters(), lr=step)]\n",
    "        \n",
    "        for optimizer in optimizers:\n",
    "#             print (\"Optimizer type: \"+str(optimizer))\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                for x, (data, lengths, labels) in enumerate(train_loader):\n",
    "                    model.train()\n",
    "                    data_batch, length_batch, label_batch = data, lengths, labels\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(data_batch, length_batch)\n",
    "                    loss = criterion(outputs, label_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # validate every 100 iterations\n",
    "                    if x > 0 and x % 100 == 0:\n",
    "                        # validate\n",
    "                        val_acc = test_model(val_loader, model)\n",
    "                        param_losses[i].append(val_acc)\n",
    "                        \n",
    "    print (\"Hyperparameter search done!\")\n",
    "    return param_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1e-05, 1, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(1e-05, 1, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(1e-05, 1, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(1e-05, 1, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.0001, 1, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.0001, 1, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.0001, 1, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.0001, 1, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.001, 1, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.001, 1, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.001, 1, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.001, 1, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.01, 1, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.01, 1, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.01, 1, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.01, 1, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.1, 1, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.1, 1, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.1, 1, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.1, 1, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "Hyperparameter search done!\n"
     ]
    }
   ],
   "source": [
    "## START WITH UNIGRAMS - SGD\n",
    "\n",
    "params = [[1e-5,1e-4,1e-3,1e-2,1e-1], ## learning rates\n",
    "          [1], ## ngrams\n",
    "          [25000,50000], ## vocab size\n",
    "          [100,200], ## embedding size\n",
    "          [200], ## max sentence length\n",
    "          [32] ## batch size\n",
    "         ]\n",
    "\n",
    "params_val_losses = hyperparameter_search(params)\n",
    "# params_accuracy_df = pd.DataFrame(param_val_losses_unigrams,columns=[\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [*params_val_losses.keys()]:\n",
    "    params_val_losses[key] = str(params_val_losses[key])\n",
    "\n",
    "pd.DataFrame(params_val_losses,index=range(len([*params_val_losses.keys()]))).to_csv(\"unigram_SGD_losses_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_val_losses_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRIGRAMS\n",
    "\n",
    "params_trigrams = [[1e-3,1e-2,1e-1,1,10], ## learning rates\n",
    "          [*range(3,4)], ## ngrams\n",
    "          [500000], ## vocab size\n",
    "          [100,200], ## embedding size\n",
    "          [200], ## max sentence length\n",
    "          [32,64] ## batch size\n",
    "         ]\n",
    "\n",
    "param_val_losses_trigrams = hyperparameter_search(params_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [*params_val_losses.keys()]:\n",
    "    params_val_losses[key] = str(params_val_losses[key])\n",
    "    \n",
    "pd.DataFrame(params_val_losses,index=range(len([*params_val_losses.keys()]))).to_csv(\"trigram_ADAM_large_vocab_SGD_losses.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
