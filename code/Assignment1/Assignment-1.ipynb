{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"tocheading\">Table of Contents</h3>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import random\n",
    "import spacy\n",
    "import string\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_review(path):\n",
    "    \"\"\"reads all reviews in a category (e.g. train, pos),\n",
    "    and appends them to a list as text (not tokens, yet)\n",
    "    \n",
    "    arg: where you keep your examples for that cagetory,\n",
    "    fot example, path = data/aclImdb/train/neg/\"\"\"\n",
    "    all_reviews = []\n",
    "    file_list = os.listdir(path)\n",
    "    for file_path in file_list:\n",
    "        f = open(path+file_path)\n",
    "        all_reviews.append(f.read())\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_pos = read_review(\"data/aclImdb/train/pos/\")\n",
    "trn_neg = read_review(\"data/aclImdb/train/neg/\")\n",
    "\n",
    "test_pos = read_review(\"data/aclImdb/test/pos/\")\n",
    "test_neg = read_review(\"data/aclImdb/test/pos/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label & Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pos = 1, neg = 0\n",
    "trn_pos = [t+\" 1\" for t in trn_pos]\n",
    "trn_neg = [t+\" 0\" for t in trn_neg]\n",
    "\n",
    "test_pos = [t+\" 1\" for t in test_pos]\n",
    "test_neg = [t+\" 0\" for t in test_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train = trn_pos + trn_neg\n",
    "test_data = test_pos + test_neg\n",
    "\n",
    "## shuffle train data to make sure both pos and neg\n",
    "## examples are represented in val data\n",
    "data_train = [*np.random.permutation(data_train)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train - Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_split = 20000\n",
    "\n",
    "train_data = data_train[:train_split]\n",
    "val_data = data_train[train_split:]\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save and remove labels from the end for each review\n",
    "train_data_labels = [int(x[-1]) for x in train_data]\n",
    "val_data_labels = [int(x[-1]) for x in val_data]\n",
    "test_data_labels = [int(x[-1]) for x in test_data]\n",
    "\n",
    "train_data = [t[:-2] for t in train_data] ## account for the extra space that we put\n",
    "val_data = [t[:-2] for t in val_data]\n",
    "test_data = [t[:-2] for t in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels size is 20000\n",
      "Val labels size is 5000\n",
      "Test labels size is 25000\n"
     ]
    }
   ],
   "source": [
    "## check\n",
    "print (\"Train labels size is {}\".format(len(train_data_labels)))\n",
    "print (\"Val labels size is {}\".format(len(val_data_labels)))\n",
    "print (\"Test labels size is {}\".format(len(test_data_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize & Lowercase\n",
    "\n",
    "__Note__: Tokenizer modified to return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "## tokenizer modified for ngrams\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent,n=None):\n",
    "    \"\"\"n = n-gram's n\"\"\"\n",
    "    tokens = tokenizer(sent)\n",
    "    assert n >= 1, \"n should be greater than or equal to 1\"\n",
    "    \n",
    "    ## for unigrams\n",
    "    if n == 1:\n",
    "        start_unigrams = [token.text.lower() \\\n",
    "                          for token in tokens if (token.text not in punctuations)]\n",
    "        return start_unigrams\n",
    "    \n",
    "    ## for n > 1 n-grams\n",
    "    else:\n",
    "        start_unigrams = [token.text.lower() \\\n",
    "                          for token in tokens if (token.text not in punctuations)]\n",
    "        ## get copy to preserve original unigram list \n",
    "        start_unigrams_copy = start_unigrams.copy()\n",
    "        ## start from 2\n",
    "        ngram = 2\n",
    "        while ngram <= n:\n",
    "            ngram_tokens = [\" \".join(start_unigrams_copy[x:x+ngram])\\\n",
    "                            for x in [*range(len(start_unigrams_copy)-ngram+1)]]\n",
    "            ## union \n",
    "            start_unigrams.extend(ngram_tokens)\n",
    "            ## increase n until specified ngrams\n",
    "            ngram += 1\n",
    "            \n",
    "        return start_unigrams\n",
    "\n",
    "def tokenize_dataset(dataset,n=None):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample,n)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'my', 'name', 'is', 'asena']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"hi, my name is Asena\",n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'asena',\n",
       " 'hi my',\n",
       " 'my name',\n",
       " 'name is',\n",
       " 'is asena',\n",
       " 'hi my name',\n",
       " 'my name is',\n",
       " 'name is asena',\n",
       " 'hi my name is',\n",
       " 'my name is asena']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"hi, my name is Asena\",n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize All Datasets to contain n-grams\n",
    "grams = 2\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_data,n=grams)\n",
    "val_data_tokens, _ = tokenize_dataset(val_data,n=grams)\n",
    "test_data_tokens, _ = tokenize_dataset(test_data,n=grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens,size=max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(size))\n",
    "    id2token = [*vocab]\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens,size=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 5270 ; token as some\n",
      "Token as some; token id 5270\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "## convert token to id in the dataset\n",
    "## copied from lab3 notebook\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in \\\n",
    "                      token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def imdb_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_data_labels)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_data_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_data_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfNgrams(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfNgrams classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfNgrams, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfNgrams(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 75.7\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 81.14\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 83.9\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 84.8\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 85.62\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 86.6\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 86.24\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 86.12\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 85.72\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 85.38\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 85.74\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 85.34\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 85.74\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 85.56\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 85.28\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 85.32\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 85.14\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 85.26\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 85.24\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 84.12\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 84.5\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 85.08\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 84.64\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 84.16\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 84.1\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 84.38\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 84.34\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 84.02\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 84.0\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 84.1\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 83.88\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 84.04\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 83.86\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 83.84\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 83.32\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 83.46\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 83.2\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 82.98\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 83.08\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 83.4\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 83.2\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 82.86\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 83.5\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 82.88\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 82.42\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 82.58\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 82.72\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 82.46\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 82.46\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 82.28\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 82.28\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 82.42\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 82.52\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 82.54\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 82.16\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 82.26\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 82.28\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 82.56\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 82.0\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 82.18\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "## try both sgd and adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Seach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you try different preprocessing and hyperparameters for the\n",
    "model, including but not limited to:\n",
    "\n",
    "- Tokenization schemes of the dataset.\n",
    "- Model hyperparameters: Vary n for n-gram (n=1; 2; 3; 4), vocabulary size\n",
    "and embedding size.\n",
    "- Optimization hyperparameters: Optimizer itself (SGD vs Adam), learning\n",
    "rate and whether or not you use linear annealing of learning rate (learning\n",
    "rate is reduced linearly over the course of training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ngrams\n",
    "## learning rate\n",
    "## vocab size\n",
    "## embedding size\n",
    "## optimizer sgd vs adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.001, 1, 10000, 50, 100, 16),\n",
       " (0.001, 1, 10000, 50, 100, 32),\n",
       " (0.001, 1, 10000, 50, 100, 64),\n",
       " (0.001, 1, 10000, 50, 100, 128),\n",
       " (0.001, 1, 10000, 50, 200, 16)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "params = [[1e-3,1e-2,1e-1,5e-1,1,10,1e2], ## learning rates\n",
    "          [*range(1,5)], ## ngrams\n",
    "          [10000,25000,50000,75000], ## vocab size\n",
    "          [50,100,150], ## embedding size\n",
    "          [100,200,300], ## max sentence length\n",
    "          [16,32,64,128] ## batch size\n",
    "         ]\n",
    "\n",
    "### ADD OPTIMIZERS AT THE END\n",
    "### -- THIS MEANS, WE'LL DO THE SEARCH TWICE \n",
    "### -- FOR ONCE FOR ADAM AND ONCE FOR SGD\n",
    "\n",
    "optimizers = [torch.optim.Adam(model.parameters(), lr=learning_rate),\\\n",
    "             torch.optim.SGD(model.parameters(), lr=learning_rate)]\n",
    "\n",
    "[*itertools.product(*params)][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE THE NGRAM DATASETS\n",
    "\n",
    "NGRAM_DATASETS = {}\n",
    "\n",
    "for n in [*range(1,5)]:\n",
    "#     print (n)\n",
    "    grams = n\n",
    "    train_data_tokens, all_train_tokens = tokenize_dataset(train_data,n=grams)\n",
    "    val_data_tokens, _ = tokenize_dataset(val_data,n=grams)\n",
    "    test_data_tokens, _ = tokenize_dataset(test_data,n=grams)\n",
    "    \n",
    "    NGRAM_DATASETS[n] = [train_data_tokens,\n",
    "                        all_train_tokens,\n",
    "                        val_data_tokens,\n",
    "                        test_data_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPERPARAMETER SEARCH ON VALIDATION SET\n",
    "\n",
    "def hyperparameter_search(hyperparameter_space=None):\n",
    "    \"\"\"Takes as input a list of parameter search space lists.\"\"\"\n",
    "    ## CRITERION: ONLY CROSS ENTROPY LOSS FOR NOW\n",
    "    param_space = [*itertools.product(*hyperparameter_space)]\n",
    "    \n",
    "    ## val loss dict\n",
    "    param_losses = {}\n",
    "    \n",
    "    for i in param_space:\n",
    "        print (i)\n",
    "        \n",
    "        ## will append validation losses here\n",
    "        param_losses[i] = []\n",
    "        \n",
    "        NUM_EPOCHS = 10\n",
    "        \n",
    "        step = i[0] ## learning rate\n",
    "        grams = i[1] ## n-grams\n",
    "        vocab_size = i[2] ## vocab size\n",
    "        embed_dimension = i[3] ## embedding size\n",
    "        max_sentence_length = i[4] ## max sentence length of data loader\n",
    "        batch_size = i[5]\n",
    "\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        ## tokenize training and validation data\n",
    "        \n",
    "        train_data_tokens = NGRAM_DATASETS[grams][0]\n",
    "        all_train_tokens = NGRAM_DATASETS[grams][1]\n",
    "        val_data_tokens = NGRAM_DATASETS[grams][2]\n",
    "            \n",
    "        train_data_tokens = NGRAM_DATASETS[grams][0]\n",
    "        all_train_tokens = NGRAM_DATASETS[grams][1]\n",
    "        val_data_tokens = NGRAM_DATASETS[grams][2]\n",
    "\n",
    "        ## build vocab for the specified vocab size\n",
    "        token2id, id2token = build_vocab(all_train_tokens,\n",
    "                                        size=vocab_size)\n",
    "\n",
    "        train_data_indices = token2index_dataset(train_data_tokens)\n",
    "        val_data_indices = token2index_dataset(val_data_tokens)\n",
    "\n",
    "        ## assign max sentence length and batch size from \n",
    "        ## parameter space\n",
    "        MAX_SENTENCE_LENGTH = max_sentence_length\n",
    "        BATCH_SIZE = batch_size\n",
    "\n",
    "        ## load train and val data\n",
    "        train_dataset = NewsGroupDataset(train_data_indices, train_data_labels)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        val_dataset = NewsGroupDataset(val_data_indices, val_data_labels)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        print (\"Datasets ready.\")\n",
    "        ## assign embedding dimension\n",
    "        ## from parameter space\n",
    "        embed_dim = embed_dimension    \n",
    "\n",
    "        ## model\n",
    "        model = BagOfNgrams(len(id2token), emb_dim)\n",
    "#         print (\"Model ready.\")\n",
    "#         optimizers = [torch.optim.Adam(model.parameters(), lr=step),\n",
    "#                       torch.optim.SGD(model.parameters(), lr=step)]\n",
    "    \n",
    "        optimizers = [torch.optim.Adam(model.parameters(), lr=step)]\n",
    "        \n",
    "        for optimizer in optimizers:\n",
    "#             print (\"Optimizer type: \"+str(optimizer))\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                for x, (data, lengths, labels) in enumerate(train_loader):\n",
    "                    model.train()\n",
    "                    data_batch, length_batch, label_batch = data, lengths, labels\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(data_batch, length_batch)\n",
    "                    loss = criterion(outputs, label_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # validate every 100 iterations\n",
    "                    if x > 0 and x % 100 == 0:\n",
    "                        # validate\n",
    "                        val_acc = test_model(val_loader, model)\n",
    "                        param_losses[i].append(val_acc)\n",
    "                        \n",
    "    print (\"Hyperparameter search done!\")\n",
    "    return param_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out of 10'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGRAM_DATASETS[3][0][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRIGRAMS ADAM\n",
    "\n",
    "params_trigrams = [[1e-4,1e-3,1e-2,1e-1], ## learning rates\n",
    "          [*range(3,4)], ## ngrams\n",
    "          [250000,500000], ## vocab size\n",
    "          [100,200], ## embedding size\n",
    "          [200], ## max sentence length\n",
    "          [32,64] ## batch size\n",
    "         ]\n",
    "\n",
    "param_val_losses_trigrams = hyperparameter_search(params_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.001, 1, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.001, 1, 25000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(0.001, 1, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.001, 1, 25000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(0.001, 1, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.001, 1, 50000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(0.001, 1, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.001, 1, 50000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(0.01, 1, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.01, 1, 25000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(0.01, 1, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.01, 1, 25000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(0.01, 1, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.01, 1, 50000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(0.01, 1, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.01, 1, 50000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(0.1, 1, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.1, 1, 25000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(0.1, 1, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.1, 1, 25000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(0.1, 1, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.1, 1, 50000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(0.1, 1, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.1, 1, 50000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(1, 1, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(1, 1, 25000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(1, 1, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(1, 1, 25000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(1, 1, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(1, 1, 50000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(1, 1, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(1, 1, 50000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(10, 1, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(10, 1, 25000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(10, 1, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(10, 1, 25000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(10, 1, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(10, 1, 50000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(10, 1, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(10, 1, 50000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "Hyperparameter search done!\n"
     ]
    }
   ],
   "source": [
    "## START WITH UNIGRAMS - ADAM\n",
    "\n",
    "params = [[1e-3,1e-2,1e-1,1,10], ## learning rates\n",
    "          [*range(1,2)], ## ngrams\n",
    "          [25000,50000], ## vocab size\n",
    "          [100,200], ## embedding size\n",
    "          [200], ## max sentence length\n",
    "          [32,64] ## batch size\n",
    "         ]\n",
    "\n",
    "param_val_losses_unigrams = hyperparameter_search(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [*param_val_losses_unigrams.keys()]:\n",
    "    param_val_losses_unigrams[key] = str(param_val_losses_unigrams[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(param_val_losses_unigrams,index=range(len([*param_val_losses_unigrams.keys()]))).to_csv(\"unigram_losses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[50.84, 76.32, 69.96, 79.1, 77.16, 70.48, 82.54, 79.32, 80.54, 79.34, 81.36, 80.46, 78.44, 80.6, 76.02, 77.42, 80.06, 80.78, 81.9, 83.12, 81.02, 80.62, 82.18, 74.16, 83.16, 81.44, 79.42, 81.84, 81.18, 80.88]'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_val_losses_unigrams[(10, 1, 50000, 200, 200, 64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.001, 2, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.001, 2, 25000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(0.001, 2, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.001, 2, 25000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(0.001, 2, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.001, 2, 50000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(0.001, 2, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.001, 2, 50000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(0.01, 2, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.01, 2, 25000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(0.01, 2, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.01, 2, 25000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(0.01, 2, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.01, 2, 50000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(0.01, 2, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.01, 2, 50000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(0.1, 2, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.1, 2, 25000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(0.1, 2, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.1, 2, 25000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(0.1, 2, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(0.1, 2, 50000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(0.1, 2, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(0.1, 2, 50000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(1, 2, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(1, 2, 25000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(1, 2, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(1, 2, 25000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(1, 2, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(1, 2, 50000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(1, 2, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(1, 2, 50000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(10, 2, 25000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(10, 2, 25000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(10, 2, 25000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(10, 2, 25000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "(10, 2, 50000, 100, 200, 32)\n",
      "Datasets ready.\n",
      "(10, 2, 50000, 100, 200, 64)\n",
      "Datasets ready.\n",
      "(10, 2, 50000, 200, 200, 32)\n",
      "Datasets ready.\n",
      "(10, 2, 50000, 200, 200, 64)\n",
      "Datasets ready.\n",
      "Hyperparameter search done!\n"
     ]
    }
   ],
   "source": [
    "## BIGRAMS\n",
    "\n",
    "##RUNNING ON THE OTHER NOTEBOOK \n",
    "\n",
    "params_bigrams = [[1e-3,1e-2,1e-1,1,10], ## learning rates\n",
    "          [2], ## ngrams\n",
    "          [25000,50000], ## vocab size\n",
    "          [100,200], ## embedding size\n",
    "          [200], ## max sentence length\n",
    "          [32,64] ## batch size\n",
    "         ]\n",
    "\n",
    "param_val_losses_bigrams = hyperparameter_search(hyperparameter_space=params_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [*param_val_losses_bigrams.keys()]:\n",
    "    param_val_losses_bigrams[key] = str(param_val_losses_bigrams[key])\n",
    "    \n",
    "pd.DataFrame(param_val_losses_bigrams,index=range(len([*param_val_losses_bigrams.keys()]))).to_csv(\"bigram_losses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOURGRAMS\n",
    "\n",
    "params_fourgrams = [[1e-3,1e-2,1e-1,1,10], ## learning rates\n",
    "          [*range(3,4)], ## ngrams\n",
    "          [25000,5000], ## vocab size\n",
    "          [100,200], ## embedding size\n",
    "          [200], ## max sentence length\n",
    "          [32,64] ## batch size\n",
    "         ]\n",
    "\n",
    "param_val_losses_fourgrams = hyperparameter_search(hyperparameter_space=params_fourgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPERPARAMETER SEARCH ON VALIDATION SET\n",
    "\n",
    "def hyperparameter_search(hyperparameter_space=None):\n",
    "    \"\"\"Takes as input a list of parameter search space lists.\"\"\"\n",
    "    ## CRITERION: ONLY CROSS ENTROPY LOSS FOR NOW\n",
    "    param_space = [*itertools.product(*hyperparameter_space)]\n",
    "    \n",
    "    ## val loss dict\n",
    "    param_losses = {}\n",
    "    \n",
    "    for i in param_space:\n",
    "        print (i)\n",
    "        \n",
    "        ## will append validation losses here\n",
    "        param_losses[i] = []\n",
    "        \n",
    "        NUM_EPOCHS = 10\n",
    "        \n",
    "        step = i[0] ## learning rate\n",
    "        grams = i[1] ## n-grams\n",
    "        vocab_size = i[2] ## vocab size\n",
    "        embed_dimension = i[3] ## embedding size\n",
    "        max_sentence_length = i[4] ## max sentence length of data loader\n",
    "        batch_size = i[5]\n",
    "\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        ## tokenize training and validation data\n",
    "        \n",
    "        train_data_tokens = NGRAM_DATASETS[grams][0]\n",
    "        all_train_tokens = NGRAM_DATASETS[grams][1]\n",
    "        val_data_tokens = NGRAM_DATASETS[grams][2]\n",
    "            \n",
    "        train_data_tokens = NGRAM_DATASETS[grams][0]\n",
    "        all_train_tokens = NGRAM_DATASETS[grams][1]\n",
    "        val_data_tokens = NGRAM_DATASETS[grams][2]\n",
    "\n",
    "        ## build vocab for the specified vocab size\n",
    "        token2id, id2token = build_vocab(all_train_tokens,\n",
    "                                        size=vocab_size)\n",
    "\n",
    "        train_data_indices = token2index_dataset(train_data_tokens)\n",
    "        val_data_indices = token2index_dataset(val_data_tokens)\n",
    "\n",
    "        ## assign max sentence length and batch size from \n",
    "        ## parameter space\n",
    "        MAX_SENTENCE_LENGTH = max_sentence_length\n",
    "        BATCH_SIZE = batch_size\n",
    "\n",
    "        ## load train and val data\n",
    "        train_dataset = NewsGroupDataset(train_data_indices, train_data_labels)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        val_dataset = NewsGroupDataset(val_data_indices, val_data_labels)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        print (\"Datasets ready.\")\n",
    "        ## assign embedding dimension\n",
    "        ## from parameter space\n",
    "        embed_dim = embed_dimension    \n",
    "\n",
    "        ## model\n",
    "        model = BagOfNgrams(len(id2token), emb_dim)\n",
    "#         print (\"Model ready.\")\n",
    "#         optimizers = [torch.optim.Adam(model.parameters(), lr=step),\n",
    "#                       torch.optim.SGD(model.parameters(), lr=step)]\n",
    "    \n",
    "        optimizers = [torch.optim.SGD(model.parameters(), lr=step)]\n",
    "        \n",
    "        for optimizer in optimizers:\n",
    "#             print (\"Optimizer type: \"+str(optimizer))\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                for x, (data, lengths, labels) in enumerate(train_loader):\n",
    "                    model.train()\n",
    "                    data_batch, length_batch, label_batch = data, lengths, labels\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(data_batch, length_batch)\n",
    "                    loss = criterion(outputs, label_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # validate every 100 iterations\n",
    "                    if x > 0 and x % 100 == 0:\n",
    "                        # validate\n",
    "                        val_acc = test_model(val_loader, model)\n",
    "                        param_losses[i].append(val_acc)\n",
    "                        \n",
    "    print (\"Hyperparameter search done!\")\n",
    "    return param_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BIGRAMS\n",
    "\n",
    "params_bigrams = [[1e-3,1e-2,1e-1,1,10], ## learning rates\n",
    "          [*range(2,3)], ## ngrams\n",
    "          [25000,50000], ## vocab size\n",
    "          [100,200], ## embedding size\n",
    "          [200], ## max sentence length\n",
    "          [32,64] ## batch size\n",
    "         ]\n",
    "\n",
    "param_val_losses_bigrams = hyperparameter_search(params_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRIGRAMS SGD\n",
    "\n",
    "params_trigrams = [[1e-4,1e-3,1e-2,1e-1], ## learning rates\n",
    "          [*range(3,4)], ## ngrams\n",
    "          [100000,200000], ## vocab size\n",
    "          [100,200], ## embedding size\n",
    "          [200], ## max sentence length\n",
    "          [32,64] ## batch size\n",
    "         ]\n",
    "\n",
    "param_val_losses_trigrams = hyperparameter_search(params_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [*param_val_losses_trigrams.keys()]:\n",
    "    param_val_losses_trigrams[key] = str(param_val_losses_trigrams[key])\n",
    "    \n",
    "pd.DataFrame(param_val_losses_trigrams,index=range(len([*param_val_losses_trigrams.keys()]))).to_csv(\"trigram_sgd_losses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation Study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {0:\"learning_rate\",\n",
    "             1:\"ngrams\",\n",
    "             2:\"vocab_size\",\n",
    "             3:\"embedding_size\",\n",
    "             4:\"max_sentence_length\",\n",
    "             5:\"batch_size\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(path=None,n=None,delete_cols=None):\n",
    "    \"\"\"Takes as input a csv, path. Cleans and returns the\n",
    "    validation accuracy dataset.\"\"\"\n",
    "    gram_losses = pd.DataFrame(pd.read_csv(path,header=None))\n",
    "    \n",
    "    ## grams\n",
    "    hyperparam_sets_ = []\n",
    "\n",
    "    for i in [*range(1,gram_losses.shape[1])]:\n",
    "        hyperparams = [*gram_losses[i].iloc[:6]]\n",
    "        hyperparam_sets_.append(hyperparams)\n",
    "\n",
    "    hyperparam_sets_ = pd.Series(hyperparam_sets_).\\\n",
    "    apply(lambda x: [float(x[i]) for i in [*range(len(x))]])\n",
    "\n",
    "    gram_losses = gram_losses.dropna(1)\n",
    "    gram_losses = gram_losses.drop([*range(6)],0)\n",
    "    gram_losses = gram_losses.T\n",
    "    gram_losses[\"params\"] = [*hyperparam_sets_]\n",
    "    param_cols_ = {}\n",
    "\n",
    "    for key in [*param_dict.keys()]:\n",
    "        param_cols_[param_dict[key]] = gram_losses[\"params\"].\\\n",
    "        apply(lambda x: x[key])\n",
    "\n",
    "    param_cols_ = {}\n",
    "    for key in [*param_dict.keys()]:\n",
    "        param_cols_[param_dict[key]] = gram_losses[\"params\"].\\\n",
    "        apply(lambda x: x[key])\n",
    "\n",
    "    for key in [*param_cols_.keys()]:\n",
    "        gram_losses[key] = param_cols_[key]\n",
    "\n",
    "    gram_losses = gram_losses.drop(\"params\",1)\n",
    "    gram_losses[\"ngrams\"] = [n for i in [*range(len(gram_losses))]]\n",
    "    gram_losses = gram_losses.drop([*range(7,int(delete_cols))],1)\n",
    "\n",
    "    gram_losses[\"val_acc\"] = gram_losses[6].\\\n",
    "    apply(lambda x: str(x).replace(\"[\",\"\").\\\n",
    "          replace(\"]\",\"\").split(\", \"))\n",
    "\n",
    "    gram_losses = gram_losses.drop(6,1)\n",
    "    \n",
    "    return gram_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of hyperparameter combinations included in the search is: 220\n"
     ]
    }
   ],
   "source": [
    "hyperparams = pd.concat([clean_df(path=\"unigram_losses.csv\",n=1,delete_cols=46),\n",
    "                              clean_df(path=\"bigram_losses.csv\",n=2,delete_cols=46),\n",
    "                              clean_df(path=\"trigram_losses.csv\",n=3,delete_cols=46)],0)\n",
    "\n",
    "hyperparams[\"optimizer\"] = [\"Adam\" for x in [*range(len(hyperparams))]]\n",
    "\n",
    "sgd_uni_df = clean_df(path=\"unigram_SGD_losses.csv\",n=1,delete_cols=46)\n",
    "sgd_uni_new = clean_df(path=\"unigram_SGD_losses_new.csv\",n=1,delete_cols=26)\n",
    "sgd_bi_df = clean_df(path=\"bigram_sgd_losses.csv\",n=2,delete_cols=46)\n",
    "\n",
    "sgd_uni_df[\"optimizer\"] = [\"SGD\" for x in [*range(len(sgd_uni_df))]]\n",
    "sgd_uni_new[\"optimizer\"] = [\"SGD\" for x in [*range(len(sgd_uni_new))]]\n",
    "sgd_bi_df[\"optimizer\"] = [\"SGD\" for x in [*range(len(sgd_bi_df))]]\n",
    "\n",
    "hyperparams = pd.concat([hyperparams,\n",
    "                        sgd_uni_df,\n",
    "                        sgd_uni_new,\n",
    "                        sgd_bi_df],0)\n",
    "\n",
    "hyperparams[\"model_no\"] = [*range(len(hyperparams))] \n",
    "hyperparams = hyperparams.set_index(\"model_no\",drop=True)\n",
    "\n",
    "print (\"Total number of hyperparameter combinations included in the search is: \"+ str(hyperparams.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>embedding_size</th>\n",
       "      <th>max_sentence_length</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>[50.82, 60.32, 64.34, 60.58, 71.6, 73.48, 75.7...</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>[54.32, 61.56, 66.66, 68.38, 71.4, 74.14, 77.7...</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>[55.9, 60.04, 65.42, 66.98, 70.48, 72.92, 76.6...</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>[50.34, 62.54, 66.34, 70.7, 73.52, 75.42, 78.2...</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>[53.1, 59.7, 64.1, 67.38, 69.58, 73.66, 75.28,...</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          learning_rate  ngrams  vocab_size  embedding_size  \\\n",
       "model_no                                                      \n",
       "0                 0.001       1     25000.0           100.0   \n",
       "1                 0.001       1     25000.0           100.0   \n",
       "2                 0.001       1     25000.0           200.0   \n",
       "3                 0.001       1     25000.0           200.0   \n",
       "4                 0.001       1     50000.0           100.0   \n",
       "\n",
       "          max_sentence_length  batch_size  \\\n",
       "model_no                                    \n",
       "0                       200.0        32.0   \n",
       "1                       200.0        64.0   \n",
       "2                       200.0        32.0   \n",
       "3                       200.0        64.0   \n",
       "4                       200.0        32.0   \n",
       "\n",
       "                                                    val_acc optimizer  \n",
       "model_no                                                               \n",
       "0         [50.82, 60.32, 64.34, 60.58, 71.6, 73.48, 75.7...      Adam  \n",
       "1         [54.32, 61.56, 66.66, 68.38, 71.4, 74.14, 77.7...      Adam  \n",
       "2         [55.9, 60.04, 65.42, 66.98, 70.48, 72.92, 76.6...      Adam  \n",
       "3         [50.34, 62.54, 66.34, 70.7, 73.52, 75.42, 78.2...      Adam  \n",
       "4         [53.1, 59.7, 64.1, 67.38, 69.58, 73.66, 75.28,...      Adam  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams.to_csv(\"hyperparams_accuracy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct and Incorrect Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Rating Between 1-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Other Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
