{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"tocheading\">Table of Contents</h2>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import random\n",
    "import spacy\n",
    "import csv\n",
    "import string\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Upload & Preprocessing\n",
    "The datasets provided are already tokenized. Thus, without running the data through a tokenizer, we use pretrained word embeddings (e.g. fast-Text) to embed the tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Vectors\n",
    "\n",
    "The web page for recommended word vector sets can be found here: https://fasttext.cc/docs/en/english-vectors.html wiki-news-300d-1M.vec from Mikolov et al (2018, Advances in Pre-Training Distributed Word Representations) 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens) is used in this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', \n",
    "                  newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        ## convert all maps to lists\n",
    "        data[tokens[0]] = [*map(float, tokens[1:])]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the wiki word vectors\n",
    "fname = \"wiki-news-300d-1M.vec\"\n",
    "word_vectors = load_vectors(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab_tokens = [*word_vectors.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique tokens in the wiki news English vectors is 999994\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of unique tokens in the wiki news English vectors is \" + str(len(all_vocab_tokens) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from Vocab Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_df = pd.DataFrame(word_vectors).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_lookup = np.array(word_vector_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_vocab(table_df):\n",
    "    \n",
    "    token_array = np.array([*table_df.index])\n",
    "    num_index_array = np.array([*range(table_df.shape[0])])\n",
    "    \n",
    "    token2id = {}\n",
    "    id2token = {}\n",
    "    for i in [*range(len(token_array))]:\n",
    "        token2id[token_array[i]] = num_index_array[i]\n",
    "        id2token[num_index_array[i]] = token_array[i]\n",
    "\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id_wiki, id2token_wiki = index_vocab(word_vector_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check for table correctness!__\n",
    "\n",
    "Do token2id and id2token match each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93141"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id_wiki[\"Alberto\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alberto'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2token_wiki[93141]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the table fit the initial word vector vocab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(word_vectors[\"Alberto\"] == table_lookup[93141])==True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1: SNLI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\"entailment\":0,\n",
    "             \"neutral\":1,\n",
    "             \"contradiction\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train = pd.read_table(\"data/snli_train.tsv\")\n",
    "snli_val = pd.read_table(\"data/snli_val.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get tokenized training data\n",
    "snli_train[\"sentence1\"] = snli_train[\"sentence1\"].apply(lambda x: x.split(\" \"))\n",
    "snli_train[\"sentence2\"] = snli_train[\"sentence2\"].apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get labels\n",
    "snli_train[\"label_num\"] = snli_train[\"label\"].apply(lambda x: label_dict[x])\n",
    "snli_val[\"label_num\"] = snli_val[\"label\"].apply(lambda x: label_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get tokenized validation data\n",
    "snli_val[\"sentence1\"] = snli_val[\"sentence1\"].apply(lambda x: x.split(\" \"))\n",
    "snli_val[\"sentence2\"] = snli_val[\"sentence2\"].apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get label arrays\n",
    "snli_train_labels = np.array(snli_train[\"label_num\"])\n",
    "snli_val_labels = np.array(snli_val[\"label_num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Three, women, on, a, stage, ,, one, wearing, ...</td>\n",
       "      <td>[There, are, two, women, standing, on, the, st...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Four, people, sit, on, a, subway, two, read, ...</td>\n",
       "      <td>[Multiple, people, are, on, a, subway, togethe...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bicycles, stationed, while, a, group, of, peo...</td>\n",
       "      <td>[People, get, together, near, a, stand, of, bi...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  [Three, women, on, a, stage, ,, one, wearing, ...   \n",
       "1  [Four, people, sit, on, a, subway, two, read, ...   \n",
       "2  [bicycles, stationed, while, a, group, of, peo...   \n",
       "\n",
       "                                           sentence2          label  label_num  \n",
       "0  [There, are, two, women, standing, on, the, st...  contradiction          2  \n",
       "1  [Multiple, people, are, on, a, subway, togethe...     entailment          0  \n",
       "2  [People, get, together, near, a, stand, of, bi...     entailment          0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_val.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: MultiNLI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train = pd.read_table(\"data/mnli_train.tsv\")\n",
    "mnli_val = pd.read_table(\"data/mnli_val.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and now that was in fifty one that 's forty ye...</td>\n",
       "      <td>It was already a problem forty years ago but n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jon could smell baked bread on the air and his...</td>\n",
       "      <td>Jon smelt food in the air and was hungry .</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it will be like Italian basketball with the uh...</td>\n",
       "      <td>This type of Italian basketball is nothing lik...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  and now that was in fifty one that 's forty ye...   \n",
       "1  Jon could smell baked bread on the air and his...   \n",
       "2  it will be like Italian basketball with the uh...   \n",
       "\n",
       "                                           sentence2          label      genre  \n",
       "0  It was already a problem forty years ago but n...        neutral  telephone  \n",
       "1         Jon smelt food in the air and was hungry .        neutral    fiction  \n",
       "2  This type of Italian basketball is nothing lik...  contradiction  telephone  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "telephone     4270\n",
       "slate         4026\n",
       "travel        3985\n",
       "government    3883\n",
       "fiction       3836\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get tokenized training data\n",
    "mnli_train[\"sentence1\"] = mnli_train[\"sentence1\"].apply(lambda x: x.split(\" \"))\n",
    "mnli_train[\"sentence2\"] = mnli_train[\"sentence2\"].apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get tokenized validation data\n",
    "mnli_val[\"sentence1\"] = mnli_val[\"sentence1\"].apply(lambda x: x.split(\" \"))\n",
    "mnli_val[\"sentence2\"] = mnli_val[\"sentence2\"].apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get labels\n",
    "mnli_train[\"label_num\"] = mnli_train[\"label\"].apply(lambda x: label_dict[x])\n",
    "mnli_val[\"label_num\"] = mnli_val[\"label\"].apply(lambda x: label_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get train and val datasets for each __MNLI genre__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## telephone\n",
    "mnli_train_telephone = mnli_train[mnli_train[\"genre\"]==\"telephone\"]\n",
    "mnli_val_telephone = mnli_val[mnli_val[\"genre\"]==\"telephone\"]\n",
    "## slate\n",
    "mnli_train_slate = mnli_train[mnli_train[\"genre\"]==\"slate\"]\n",
    "mnli_val_slate = mnli_val[mnli_val[\"genre\"]==\"slate\"]\n",
    "## travel\n",
    "mnli_train_travel = mnli_train[mnli_train[\"genre\"]==\"travel\"]\n",
    "mnli_val_travel = mnli_val[mnli_val[\"genre\"]==\"travel\"]\n",
    "## government\n",
    "mnli_train_government = mnli_train[mnli_train[\"genre\"]==\"government\"]\n",
    "mnli_val_government = mnli_val[mnli_val[\"genre\"]==\"government\"]\n",
    "## fiction\n",
    "mnli_train_fiction = mnli_train[mnli_train[\"genre\"]==\"fiction\"]\n",
    "mnli_val_fiction = mnli_val[mnli_val[\"genre\"]==\"fiction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get label arrays for each train and val dataset\n",
    "\n",
    "## whole MNLI dataset\n",
    "mnli_train_labels = np.array(pd.get_dummies(np.array(mnli_train[\"label_num\"])))\n",
    "mnli_val_labels = np.array(pd.get_dummies(np.array(mnli_val[\"label_num\"])))\n",
    "## telephone\n",
    "mnli_train_tel_labels = np.array(pd.get_dummies(np.array(mnli_train_telephone[\"label_num\"])))\n",
    "mnli_val_tel_labels = np.array(pd.get_dummies(np.array(mnli_val_telephone[\"label_num\"])))\n",
    "## slate\n",
    "mnli_train_slate_labels = np.array(pd.get_dummies(np.array(mnli_train_slate[\"label_num\"])))\n",
    "mnli_val_slate_labels = np.array(pd.get_dummies(np.array(mnli_val_slate[\"label_num\"])))\n",
    "## travel\n",
    "mnli_train_travel_labels = np.array(pd.get_dummies(np.array(mnli_train_travel[\"label_num\"])))\n",
    "mnli_val_travel_labels = np.array(mnli_val_travel[\"label_num\"])\n",
    "## gov\n",
    "mnli_train_gov_labels = np.array(pd.get_dummies(np.array(mnli_train_government[\"label_num\"])))\n",
    "mnli_val_gov_labels = np.array(pd.get_dummies(np.array(mnli_val_government[\"label_num\"])))\n",
    "## fiction\n",
    "mnli_train_fiction_labels = np.array(pd.get_dummies(np.array(mnli_train_fiction[\"label_num\"])))\n",
    "mnli_val_fiction_labels = np.array(pd.get_dummies(np.array(mnli_val_fiction[\"label_num\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## idx = token2id_wiki\n",
    "\n",
    "def token2index_dataset(tokens_data,idx_dict=None):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        ## get index list for each sentence.\n",
    "        index_list = [idx_dict[token] if token in \\\n",
    "                      idx_dict else idx_dict[\"unk\"] for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ I am getting the indices for Sentence 1 and Sentence 2 separately (not concatenating them at first from the beginning) since, in hyperparameter search I want to try more than one ways of interacting the hidden representations of the two sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get train and val indices for both datasets\n",
    "\n",
    "## SNLI\n",
    "snli_train_sentence1_indices = token2index_dataset([*snli_train[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "snli_train_sentence2_indices = token2index_dataset([*snli_train[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "snli_val_sentence1_indices = token2index_dataset([*snli_val[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "snli_val_sentence2_indices = token2index_dataset([*snli_val[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "\n",
    "## MNLI\n",
    "mnli_train_sentence1_indices = token2index_dataset([*mnli_train[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_sentence2_indices = token2index_dataset([*mnli_train[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_sentence1_indices = token2index_dataset([*mnli_val[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_sentence2_indices = token2index_dataset([*mnli_val[\"sentence2\"]],idx_dict=token2id_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENRES\n",
    "\n",
    "## telephone\n",
    "mnli_train_s1_tel_ix = token2index_dataset([*mnli_train_telephone[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_tel_ix = token2index_dataset([*mnli_train_telephone[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_tel_ix = token2index_dataset([*mnli_val_telephone[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_tel_ix = token2index_dataset([*mnli_val_telephone[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "## slate\n",
    "mnli_train_s1_slate_ix = token2index_dataset([*mnli_train_slate[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_slate_ix = token2index_dataset([*mnli_train_slate[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_slate_ix = token2index_dataset([*mnli_val_slate[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_slate_ix = token2index_dataset([*mnli_val_slate[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "## travel\n",
    "mnli_train_s1_travel_ix = token2index_dataset([*mnli_train_travel[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_travel_ix = token2index_dataset([*mnli_train_travel[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_travel_ix = token2index_dataset([*mnli_val_travel[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_travel_ix = token2index_dataset([*mnli_val_travel[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "## gov\n",
    "mnli_train_s1_gov_ix = token2index_dataset([*mnli_train_government[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_gov_ix = token2index_dataset([*mnli_train_government[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_gov_ix = token2index_dataset([*mnli_val_government[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_gov_ix = token2index_dataset([*mnli_val_government[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "## fiction\n",
    "mnli_train_s1_fiction_ix = token2index_dataset([*mnli_train_fiction[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_fiction_ix = token2index_dataset([*mnli_train_fiction[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_fiction_ix = token2index_dataset([*mnli_val_fiction[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_fiction_ix = token2index_dataset([*mnli_val_fiction[\"sentence2\"]],idx_dict=token2id_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting training and validation set __labels__ (targets) for both datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SNLI\n",
    "snli_train_labels = np.array(snli_train[\"label_num\"])\n",
    "snli_val_labels = np.array(snli_val[\"label_num\"])\n",
    "\n",
    "## MNLI\n",
    "mnli_train_labels = np.array(mnli_train[\"label_num\"])\n",
    "mnli_val_labels = np.array(mnli_val[\"label_num\"])\n",
    "\n",
    "## GENRES\n",
    "\n",
    "## telephone\n",
    "mnli_train_tel_labels = np.array(mnli_train_telephone[\"label_num\"])\n",
    "mnli_val_tel_labels = np.array(mnli_val_telephone[\"label_num\"])\n",
    "## slate\n",
    "mnli_train_slate_labels = np.array(mnli_train_slate[\"label_num\"])\n",
    "mnli_val_slate_labels = np.array(mnli_val_slate[\"label_num\"])\n",
    "## travel\n",
    "mnli_train_travel_labels = np.array(mnli_train_travel[\"label_num\"])\n",
    "mnli_val_travel_labels = np.array(mnli_val_travel[\"label_num\"])\n",
    "## gov\n",
    "mnli_train_gov_labels = np.array(mnli_train_government[\"label_num\"])\n",
    "mnli_val_gov_labels = np.array(mnli_val_government[\"label_num\"])\n",
    "## fiction\n",
    "mnli_train_fiction_labels = np.array(mnli_train_fiction[\"label_num\"])\n",
    "mnli_val_fiction_labels = np.array(mnli_val_fiction[\"label_num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get pretrained word embeddings from the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code taken from lab3\n",
    "\n",
    "## SNLI\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "snli_train_targets = snli_train_labels\n",
    "snli_val_targets = snli_val_labels\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SNLI_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def snli_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "#         data_list.append(datum[0])\n",
    "        \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list)), \n",
    "            torch.LongTensor(length_list), \n",
    "            torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "\n",
    "# since I'm loading the sentences separately, I set shulle to False \n",
    "\n",
    "\n",
    "##### LABELS?\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "snli_train_dataset_s1 = SNLI_Dataset(snli_train_sentence1_indices,snli_train_labels)\n",
    "snli_train_dataset_s2 = SNLI_Dataset(snli_train_sentence2_indices,snli_train_labels)\n",
    "snli_train_loader_s1 = torch.utils.data.DataLoader(dataset=snli_train_dataset_s1,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=snli_func,\n",
    "                                               shuffle=False)\n",
    "snli_train_loader_s2 = torch.utils.data.DataLoader(dataset=snli_train_dataset_s2,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=snli_func,\n",
    "                                               shuffle=False)\n",
    "\n",
    "snli_val_dataset_s1 = SNLI_Dataset(snli_val_sentence1_indices, snli_val_labels)\n",
    "snli_val_dataset_s2 = SNLI_Dataset(snli_val_sentence2_indices, snli_val_labels)\n",
    "snli_val_loader_s1 = torch.utils.data.DataLoader(dataset=snli_val_dataset_s1,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=snli_func,\n",
    "                                             shuffle=False)\n",
    "snli_val_loader_s2 = torch.utils.data.DataLoader(dataset=snli_val_dataset_s2,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=snli_func,\n",
    "                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will load two strings separately, and get the hidden representations as they're output from the encoder - again separately, then interact them to get the softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "\n",
    "## code taken from lab3\n",
    "## mnli\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "mnli_train_targets = mnli_train_labels\n",
    "mnli_val_targets = mnli_val_labels\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MNLI_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def mnli_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "        \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "            \n",
    "        data_list.append(padded_vec)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list)), \n",
    "            torch.LongTensor(length_list), \n",
    "            torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "mnli_train_dataset_s1 = MNLI_Dataset(mnli_train_sentence1_indices,mnli_train_labels)\n",
    "mnli_train_loader_s1 = torch.utils.data.DataLoader(dataset=mnli_train_dataset_s1,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=mnli_func,\n",
    "                                               shuffle=False)\n",
    "mnli_train_dataset_s2 = MNLI_Dataset(mnli_train_sentence2_indices,mnli_train_labels)\n",
    "mnli_train_loader_s2 = torch.utils.data.DataLoader(dataset=mnli_train_dataset_s2,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=mnli_func,\n",
    "                                               shuffle=False)\n",
    "\n",
    "\n",
    "mnli_val_dataset_s1 = MNLI_Dataset(mnli_val_sentence1_indices, mnli_val_labels)\n",
    "mnli_val_loader_s1 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s1,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "mnli_val_dataset_s2 = MNLI_Dataset(mnli_val_sentence2_indices, mnli_val_labels)\n",
    "mnli_val_loader_s2 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s2,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"and now that was in fifty one that 's forty years ago that it was already a problem so it 's now uh\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\" \").join([id2token_wiki[x] for x in [*mnli_train_dataset_s1][0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It was already a problem forty years ago but now it 's ten times worse !\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\" \").join([id2token_wiki[x] for x in [*mnli_train_dataset_s2][0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [*mnli_train_loader_s2][29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model\n",
    "\n",
    "The model is trained on SNLI training set. The best model is chosen using SNLI validation set, then the best model is evaluated on each genre in MultiNLI validation set. \n",
    "\n",
    "We will use an encoder (either a CNN or an RNN) to map each string of text (hypothesis and premise) to a fixed-dimension vector representation. \n",
    "\n",
    "- We will interact the two hidden representations and output a __3-class softmax__. \n",
    "\n",
    "- To keep things simple, we will simply __concatenate__ the two representations, and feed them through a network of __2 fully-connected layers__. \n",
    "\n",
    "- For the encoder, we want the following:\n",
    "\n",
    "### Part 2.1: CNN\n",
    "For the CNN, \n",
    "- A 2-layer 1-D convolutional network with ReLU activations \n",
    "- A max-pool at the end to compress the hidden representation into a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify to accept hard coded arguments\n",
    "# batch_size = 8\n",
    "# batch_size = 16\n",
    "epochs = 20\n",
    "no_cuda = False\n",
    "log_interval = 1\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "# cuda = False\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_train_length = max([snli_train_dataset_s1[x][1] for x in range(len(snli_train_dataset_s1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max([snli_train_dataset_s2[x][1] for x in range(len(snli_train_dataset_s2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val_length = max([snli_val_dataset_s1[x][1] for x in range(len(snli_val_dataset_s1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max([snli_val_dataset_s2[x][1] for x in range(len(snli_val_dataset_s2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = [*snli_train_dataset]\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,emb_size,\n",
    "                 hidden_size,\n",
    "                 hidden_size_2,\n",
    "                 num_classes,\n",
    "                 vocab_size,\n",
    "                 kernel_size,\n",
    "                 padding,\n",
    "                 stride,\n",
    "                 percent_dropout,\n",
    "                 interaction_type):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "        self.stride = stride\n",
    "        self.interaction = interaction_type\n",
    "\n",
    "        ## use pretrained wiki embeddings\n",
    "        wiki_embed_table = torch.FloatTensor(table_lookup).to(device)\n",
    "        embedding = nn.Embedding.from_pretrained(wiki_embed_table)\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        ## 2 1d convolutional layers\n",
    "        ## conv1\n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size,\n",
    "                               kernel_size=self.kernel_size, \n",
    "                               padding=self.padding)\n",
    "        ## conv2\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size_2,\n",
    "                               kernel_size=self.kernel_size, \n",
    "                               padding=self.padding)\n",
    "        \n",
    "        ## ReLU activations\n",
    "        self.ReLU1 = nn.ReLU()\n",
    "        self.ReLU2 = nn.ReLU()\n",
    "        \n",
    "        ## dropout\n",
    "        self.dropout = nn.Dropout(percent_dropout)\n",
    "        \n",
    "        ## Fully Connected Layers for Concatenated \n",
    "        ## Hidden Representations\n",
    "        \n",
    "        ####\n",
    "        ####\n",
    "        ####\n",
    "        hidden_out = int(3+(hidden_size_2-self.kernel_size + 2*self.padding)/self.stride)\n",
    "\n",
    "        if self.interaction == \"concat\":\n",
    "            self.fc1 = nn.Linear(hidden_out*2, hidden_size_2)\n",
    "            self.fc2 = nn.Linear(hidden_size_2, 3)\n",
    "        else: \n",
    "            ## no need to multiply hidden size by 2\n",
    "            self.fc1 = nn.Linear(hidden_out, hidden_size_2)\n",
    "            self.fc2 = nn.Linear(hidden_size_2, 3)\n",
    "        \n",
    "\n",
    "    def forward(self, x1, x2, lengths1, lengths2):\n",
    "        batch_size_1, seq_len_1 = x1.size()\n",
    "        batch_size_2, seq_len_2 = x2.size()\n",
    "        \n",
    "        ## change embedding layer with\n",
    "#         embeds_1 = torch.FloatTensor(self.embedding(x1)).to(device)\n",
    "#         embeds_2 = torch.FloatTensor(self.embedding(x2)).to(device)\n",
    "\n",
    "        embeds_1 = []\n",
    "        embeds_2 = []\n",
    "        for arr in [*range(len(x1))]: \n",
    "            input = torch.cuda.LongTensor(x1[arr][:int(lengths1[arr])].cpu().numpy()).to(device)\n",
    "            embed = self.embedding(input).to(device)\n",
    "            ## pad again\n",
    "            length_to_pad = MAX_SENTENCE_LENGTH - embed.size()[0]\n",
    "            embed = np.vstack((embed.cpu().numpy(),np.zeros((length_to_pad,300))))\n",
    "            embeds_1.append(embed)\n",
    "            \n",
    "        embeds_1 = torch.FloatTensor(embeds_1).to(device)\n",
    "            \n",
    "        for arr in [*range(len(x2))]: \n",
    "            input = torch.cuda.LongTensor(x2[arr][:int(lengths2[arr])].cpu().numpy()).to(device)\n",
    "            embed = self.embedding(input).to(device)\n",
    "            ## pad again\n",
    "            length_to_pad = MAX_SENTENCE_LENGTH - embed.size()[0]\n",
    "            embed = np.vstack((embed.cpu().numpy(),np.zeros((length_to_pad,300))))\n",
    "            embeds_2.append(embed)\n",
    "\n",
    "        embeds_2 = torch.FloatTensor(embeds_2).to(device)\n",
    "\n",
    "        ## FIRST 1D CONVOLUTION\n",
    "        ## sentence 1 - conv1\n",
    "        s1_conv1 = self.conv1(torch.transpose(embeds_1, 1, 2)).transpose(1,2)\n",
    "        ## sentence 2 - conv 1\n",
    "        s2_conv1 = self.conv1(torch.transpose(embeds_2, 1, 2)).transpose(1,2)\n",
    "\n",
    "        ## FIRST RELU\n",
    "        ## sentence 1 - ReLU1\n",
    "        s1_ReLU1 = self.ReLU1(s1_conv1)\n",
    "        ## sentence 2 - ReLU1\n",
    "        s2_ReLU1 = self.ReLU1(s2_conv1)\n",
    "\n",
    "        ## SECOND 1D CONVOLUTION\n",
    "        ## sentence 1 - conv2\n",
    "        s1_conv2 = self.conv2(s1_ReLU1.transpose(1,2)).transpose(1,2)\n",
    "        ## sentence 2 - conv2\n",
    "        s2_conv2 = self.conv2(s2_ReLU1.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        ## SECOND RELU\n",
    "        ## sentence 1 - ReLU2\n",
    "        s1_ReLU2 = self.ReLU2(s1_conv2)\n",
    "        ## sentence 2 - ReLU2\n",
    "        s2_ReLU2 = self.ReLU2(s2_conv2)\n",
    "        \n",
    "        ## MAX-POOL - dropout\n",
    "        s1_hidden = torch.max(self.dropout(s1_ReLU2),1)\n",
    "        s2_hidden = torch.max(self.dropout(s2_ReLU2),1)\n",
    "\n",
    "        ## INTERACTION\n",
    "        if self.interaction == \"concat\":\n",
    "            ## CONCATENATION\n",
    "            hidden = torch.cat((s1_hidden[0],s2_hidden[0]),1)\n",
    "        elif self.interaction == \"mul\":\n",
    "            ## ELEM-WISE MULTP.\n",
    "            hidden = s1_hidden[0]*s2_hidden[0]\n",
    "        elif self.interaction == \"subtract\":\n",
    "            ## SUBTRACTION\n",
    "            hidden = s1_hidden[0] - s2_hidden[0]\n",
    "\n",
    "        ## FC LAYERS\n",
    "        hidden = self.fc1(hidden)\n",
    "        hidden = self.ReLU2(hidden)\n",
    "        hidden = self.fc2(hidden)\n",
    "\n",
    "        ## SOFTMAX\n",
    "        out = F.softmax(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_embed_table = torch.FloatTensor(table_lookup).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(emb_size=300, \n",
    "            hidden_size=512,\n",
    "            hidden_size_2 = 128,\n",
    "            num_classes=3, \n",
    "            vocab_size=wiki_embed_table.size()[0],\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            stride=1,\n",
    "            percent_dropout=0.1,\n",
    "            interaction_type=\"concat\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(),\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 10 # number epoch to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.NLLLoss().cuda()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [*enumerate(zip(snli_train_loader_s1,snli_train_loader_s2))][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_model(loader1, loader2, model):\n",
    "    \"\"\"\n",
    "    Takes as input validation loaders as loader1 and loader2, \n",
    "    and the model fitted to the training set.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_idx, ([data_s1, lengths_s1, labels], \n",
    "    [data_s2, lengths_s2, labels]) in enumerate(zip(loader1,\n",
    "                                                 loader2)):\n",
    "        ## validation data, length tensors, and val labels \n",
    "        ## to device\n",
    "        data_s1 = data_s1.to(device)\n",
    "        data_s2 = data_s2.to(device)\n",
    "        lengths_s1 = lengths_s1.to(device)\n",
    "        lengths_s2 = lengths_s2.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(data_s1, data_s2, lengths_s1, lengths_s2).to(device)\n",
    "        predicted = torch.max(outputs, 1)[1]\n",
    "#         print (\"preds=\"+str(predicted))\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += np.sum((predicted.cpu().detach().numpy() == labels.cpu().numpy()).astype(int))\n",
    "\n",
    "    return (correct / total)\n",
    "\n",
    "total_step = len(snli_train_loader_s1)\n",
    "# num_epochs = 10\n",
    "num_epochs = 4\n",
    "loss_hist = []\n",
    "val_accuracy = []\n",
    "train_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "#     train(epoch)\n",
    "    for batch_idx, ([data_s1, lengths_s1, labels], \n",
    "    [data_s2, lengths_s2, labels]) in enumerate(zip(snli_train_loader_s1,\n",
    "                                                 snli_train_loader_s2)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data_s1 = data_s1.to(device)\n",
    "        data_s2 = data_s2.to(device)\n",
    "        lengths_s1 = lengths_s1.to(device)\n",
    "        lengths_s2 = lengths_s2.to(device)\n",
    "        labels = labels.to(device)\n",
    "#         print (\"Data transfer complete.\")\n",
    "#         model.train()\n",
    "        outputs = model(data_s1,data_s2,lengths_s1,lengths_s2)\n",
    "        predicted = torch.max(outputs, 1)[1]\n",
    "        loss = criterion(outputs, labels).to(device)\n",
    "        loss.cuda().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Forward pass\n",
    "        \n",
    "        ## Compute accuracy\n",
    "        \n",
    "        train_acc = np.sum((predicted.cpu().detach().numpy() == labels.cpu().numpy()).\\\n",
    "                           astype(int))/predicted.size(0)\n",
    "            \n",
    "        if batch_idx % 30 == 0:\n",
    "            train_accuracy.append(train_acc)\n",
    "            loss_hist.append(loss)\n",
    "            # validate\n",
    "            ## test model with validation loaders\n",
    "            val_acc = test_model(snli_val_loader_s1,snli_val_loader_s2, model)\n",
    "            val_accuracy.append(val_acc)\n",
    "            \n",
    "        if batch_idx > 0 and batch_idx % 600 == 0:\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, batch_idx+1, len(snli_train_loader_s1), \n",
    "                val_accuracy[-1]))\n",
    "            \n",
    "            ## Plot validation accuracy\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot(val_accuracy,\n",
    "                     color=\"crimson\",linewidth=3,alpha=0.6,marker=\"o\")\n",
    "            plt.rcParams[\"font.size\"] = 16\n",
    "            plt.title(\"Validation Accuracy\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.show()      \n",
    "            \n",
    "        if batch_idx % 3000 == 0:\n",
    "            ## Plot training accuracy\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot(train_accuracy,\n",
    "                     color=\"b\",linewidth=3,alpha=0.6,marker=\"o\")\n",
    "            plt.rcParams[\"font.size\"] = 16\n",
    "            plt.title(\"Training Accuracy\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.show()\n",
    "            print (\"Cross Entropy = \"+str(loss))\n",
    "            torch.save(model.state_dict(),\"model\")\n",
    "            \n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Search\n",
    "\n",
    "The hyperparameters included in the hyperparameter search space are;\n",
    "\n",
    "- Learning rate of the optimizer\n",
    "- The size of the hidden dimension of the CNN,\n",
    "- The kernel size of the CNN,\n",
    "- Experiment with different ways of interacting the two encoded sentences (concatenation, element-wise multiplication, outer multiplication etc)\n",
    "- Dropout prob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "params = [[1e-4, 1e-3,1e-2], ## learning rates\n",
    "         [256,512], ## hidden dimension\n",
    "         [128,64,32], ## hidden dimension 2\n",
    "         [0.1,0.3,0.5], ## dropout\n",
    "         [5],\n",
    "#          [3,5,11], ## kernel size\n",
    "         [\"concat\",\"subtract\"] ## interaction type\n",
    "         ]\n",
    "\n",
    "params = [*itertools.product(*params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Set: (0.001, 256, 128, 0.5, 5, 'concat')\n",
      "Epoch = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adc563/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.602\n",
      "Parameter Set: (0.001, 256, 128, 0.5, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.528\n",
      "Parameter Set: (0.001, 256, 64, 0.1, 5, 'concat')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.6\n",
      "Parameter Set: (0.001, 256, 64, 0.1, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.575\n",
      "Parameter Set: (0.001, 256, 64, 0.3, 5, 'concat')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.616\n",
      "Parameter Set: (0.001, 256, 64, 0.3, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.574\n",
      "Parameter Set: (0.001, 256, 64, 0.5, 5, 'concat')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.608\n",
      "Parameter Set: (0.001, 256, 64, 0.5, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.592\n",
      "Parameter Set: (0.001, 256, 32, 0.1, 5, 'concat')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.613\n",
      "Parameter Set: (0.001, 256, 32, 0.1, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.579\n",
      "Parameter Set: (0.001, 256, 32, 0.3, 5, 'concat')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.621\n",
      "Parameter Set: (0.001, 256, 32, 0.3, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.612\n",
      "Parameter Set: (0.001, 256, 32, 0.5, 5, 'concat')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.597\n",
      "Parameter Set: (0.001, 256, 32, 0.5, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.595\n",
      "Parameter Set: (0.001, 512, 128, 0.1, 5, 'concat')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.585\n",
      "Parameter Set: (0.001, 512, 128, 0.1, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.5\n",
      "Parameter Set: (0.001, 512, 128, 0.3, 5, 'concat')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.583\n",
      "Parameter Set: (0.001, 512, 128, 0.3, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.531\n",
      "Parameter Set: (0.001, 512, 128, 0.5, 5, 'concat')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.59\n",
      "Parameter Set: (0.001, 512, 128, 0.5, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.542\n",
      "Parameter Set: (0.001, 512, 64, 0.1, 5, 'concat')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.607\n",
      "Parameter Set: (0.001, 512, 64, 0.1, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.583\n",
      "Parameter Set: (0.001, 512, 64, 0.3, 5, 'concat')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.565\n",
      "Parameter Set: (0.001, 512, 64, 0.3, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.583\n",
      "Parameter Set: (0.001, 512, 64, 0.5, 5, 'concat')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.615\n",
      "Parameter Set: (0.001, 512, 64, 0.5, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.555\n",
      "Parameter Set: (0.001, 512, 32, 0.1, 5, 'concat')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.595\n",
      "Parameter Set: (0.001, 512, 32, 0.1, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.587\n",
      "Parameter Set: (0.001, 512, 32, 0.3, 5, 'concat')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n",
      "Epoch = 4\n",
      "Validation Accuracy = 0.59\n",
      "Parameter Set: (0.001, 512, 32, 0.3, 5, 'subtract')\n",
      "Epoch = 0\n",
      "Epoch = 1\n",
      "Epoch = 2\n",
      "Epoch = 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-e59582f26c9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mlengths_s2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlengths_s2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_s1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_s2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths_s1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths_s2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-fc251f8b07f3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, lengths1, lengths2)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0membeds_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0membeds_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m## FIRST 1D CONVOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## cnn hyperparameter search\n",
    "\n",
    "param_losses = {}\n",
    "\n",
    "for param_set in params[40:]:\n",
    "    print (\"Parameter Set: \"+str(param_set))\n",
    "    ## INITIALIZE VALIDATION ACCURACY LIST\n",
    "    param_losses[param_set] = []\n",
    "    \n",
    "    model = CNN(emb_size=300, \n",
    "            hidden_size=param_set[1],\n",
    "            hidden_size_2 = param_set[2],\n",
    "            num_classes=3, \n",
    "            vocab_size=wiki_embed_table.size()[0],\n",
    "            kernel_size=param_set[4],\n",
    "            padding=1,\n",
    "            stride=1,\n",
    "            percent_dropout=param_set[3],\n",
    "            interaction_type=param_set[5]).to(device)\n",
    "    \n",
    "    # criterion = torch.nn.CrossEntropyLoss()\n",
    "    criterion = torch.nn.NLLLoss().cuda()\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                 lr=param_set[0],\n",
    "                                 weight_decay=1e-4)\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print (\"Epoch = \" + str(epoch))\n",
    "        \n",
    "        for batch_idx, ([data_s1, lengths_s1, labels], \n",
    "        [data_s2, lengths_s2, labels]) in enumerate(zip(snli_train_loader_s1,\n",
    "                                                     snli_train_loader_s2)):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            data_s1 = data_s1.to(device)\n",
    "            data_s2 = data_s2.to(device)\n",
    "            lengths_s1 = lengths_s1.to(device)\n",
    "            lengths_s2 = lengths_s2.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(data_s1,data_s2,lengths_s1,lengths_s2)\n",
    "            predicted = torch.max(outputs, 1)[1]\n",
    "            loss = criterion(outputs, labels).to(device)\n",
    "            loss.cuda().backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 1000 == 0:\n",
    "                # validate\n",
    "                ## test model with validation loaders\n",
    "                val_acc = test_model(snli_val_loader_s1,snli_val_loader_s2, model)\n",
    "                param_losses[param_set].append(val_acc)\n",
    "        \n",
    "            if epoch == 4 and batch_idx > 0 and batch_idx % 3000 == 0:\n",
    "                print (\"Validation Accuracy = \" + str(val_acc))\n",
    "                pd.DataFrame(param_losses).to_csv(\"param_losses_cnn_7.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Test Performance\n",
    "\n",
    "The best model was evaluated on each Multi-NLI genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the best performing model from validation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "validation_dfs = []\n",
    "\n",
    "for i in range(5,8):\n",
    "    validation_dfs.append(pd.\\\n",
    "    DataFrame(pd.read_csv(\"param_losses_cnn_\" \\\n",
    "                          + str(i)+\".csv\",header=None)).drop(0, 1))\n",
    "    \n",
    "validation_df = pd.DataFrame(pd.concat(validation_dfs,1).T)\n",
    "validation_df.columns=[\"lr\",\"hidden\",\"hidden_2\",\n",
    "                                     \"dropout\",\"kernel\",\"interaction\"]+[*range(1,21)]\n",
    "\n",
    "validation_df.index = [*range(len(validation_df))]\n",
    "\n",
    "validation_ = pd.DataFrame(validation_df).dropna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>hidden</th>\n",
       "      <th>hidden_2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>kernel</th>\n",
       "      <th>interaction</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>concat</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>subtract</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>concat</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lr hidden hidden_2 dropout kernel interaction      1      2      3  \\\n",
       "0  0.0001    256       32     0.1      5      concat  0.338  0.535  0.558   \n",
       "1  0.0001    256       32     0.1      5    subtract  0.331  0.528  0.559   \n",
       "2  0.0001    256       32     0.3      5      concat  0.331  0.535  0.567   \n",
       "\n",
       "       4  ...       11     12     13     14     15     16     17     18  \\\n",
       "0  0.579  ...    0.587  0.583  0.577  0.587  0.584  0.592   0.58  0.591   \n",
       "1  0.574  ...    0.592  0.602  0.596  0.586  0.598  0.599  0.599  0.593   \n",
       "2  0.582  ...    0.586  0.588  0.578   0.58  0.599  0.594  0.594  0.586   \n",
       "\n",
       "      19     20  \n",
       "0  0.583  0.593  \n",
       "1    0.6  0.604  \n",
       "2  0.601  0.601  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_[\"max_val_acc\"] = [max(validation_[[*range(1,21)]].iloc[i]) for i in range(len(validation_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>hidden</th>\n",
       "      <th>hidden_2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>kernel</th>\n",
       "      <th>interaction</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>max_val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>concat</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>subtract</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>concat</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lr hidden hidden_2 dropout kernel interaction      1      2      3  \\\n",
       "0  0.0001    256       32     0.1      5      concat  0.338  0.535  0.558   \n",
       "1  0.0001    256       32     0.1      5    subtract  0.331  0.528  0.559   \n",
       "2  0.0001    256       32     0.3      5      concat  0.331  0.535  0.567   \n",
       "\n",
       "       4     ...         12     13     14     15     16     17     18     19  \\\n",
       "0  0.579     ...      0.583  0.577  0.587  0.584  0.592   0.58  0.591  0.583   \n",
       "1  0.574     ...      0.602  0.596  0.586  0.598  0.599  0.599  0.593    0.6   \n",
       "2  0.582     ...      0.588  0.578   0.58  0.599  0.594  0.594  0.586  0.601   \n",
       "\n",
       "      20 max_val_acc  \n",
       "0  0.593       0.593  \n",
       "1  0.604       0.604  \n",
       "2  0.601       0.601  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([35]),)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array(validation_[\"max_val_acc\"])==max(validation_[\"max_val_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_series = validation_.iloc[35] \n",
    "\n",
    "lr = float(best_model_series[\"lr\"])\n",
    "hidden_size = int(best_model_series[\"hidden\"])\n",
    "hidden_size_2 = int(best_model_series[\"hidden_2\"])\n",
    "dropout_pr = float(best_model_series[\"dropout\"])\n",
    "kernel_size = int(best_model_series[\"kernel\"])\n",
    "interaction_type = best_model_series[\"interaction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_embed_table = torch.FloatTensor(table_lookup).to(device)\n",
    "\n",
    "model = CNN(emb_size=300, \n",
    "            hidden_size=hidden_size,\n",
    "            hidden_size_2 = hidden_size_2,\n",
    "            num_classes=3, \n",
    "            vocab_size=wiki_embed_table.size()[0],\n",
    "            kernel_size=kernel_size,\n",
    "            padding=1,\n",
    "            stride=1,\n",
    "            percent_dropout=dropout_pr,\n",
    "            interaction_type=interaction_type).to(device)\n",
    "\n",
    "learning_rate = lr\n",
    "num_epochs = 4 # number epoch to train\n",
    "\n",
    "criterion = torch.nn.NLLLoss().cuda()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def test_model(loader1, loader2, model):\n",
    "    \"\"\"\n",
    "    Takes as input validation loaders as loader1 and loader2, \n",
    "    and the model fitted to the training set.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_idx, ([data_s1, lengths_s1, labels], \n",
    "    [data_s2, lengths_s2, labels]) in enumerate(zip(loader1,\n",
    "                                                 loader2)):\n",
    "        ## validation data, length tensors, and val labels \n",
    "        ## to device\n",
    "        data_s1 = data_s1.to(device)\n",
    "        data_s2 = data_s2.to(device)\n",
    "        lengths_s1 = lengths_s1.to(device)\n",
    "        lengths_s2 = lengths_s2.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(data_s1, data_s2, lengths_s1, lengths_s2).to(device)\n",
    "        predicted = torch.max(outputs, 1)[1]\n",
    "#         print (\"preds=\"+str(predicted))\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += np.sum((predicted.cpu().detach().numpy() == labels.cpu().numpy()).astype(int))\n",
    "\n",
    "    return (correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "government    1016\n",
       "telephone     1005\n",
       "slate         1002\n",
       "fiction        995\n",
       "travel         982\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_val[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adc563/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "## train selected model\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print (\"Epoch = \"+str(epoch))\n",
    "    for batch_idx, ([data_s1, lengths_s1, labels], \n",
    "    [data_s2, lengths_s2, labels]) in enumerate(zip(snli_train_loader_s1,\n",
    "                                                 snli_train_loader_s2)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data_s1 = data_s1.to(device)\n",
    "        data_s2 = data_s2.to(device)\n",
    "        lengths_s1 = lengths_s1.to(device)\n",
    "        lengths_s2 = lengths_s2.to(device)\n",
    "        labels = labels.to(device)\n",
    "#         print (\"Data transfer complete.\")\n",
    "#         model.train()\n",
    "        outputs = model(data_s1,data_s2,lengths_s1,lengths_s2)\n",
    "        predicted = torch.max(outputs, 1)[1]\n",
    "        loss = criterion(outputs, labels).to(device)\n",
    "        loss.cuda().backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Government"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Government genre test accuracy is 0.4734251968503937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adc563/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "## data loaders\n",
    "BATCH_SIZE = mnli_val.shape[0]\n",
    "\n",
    "mnli_val_dataset_s1 = MNLI_Dataset(mnli_val_s1_gov_ix, mnli_val_gov_labels)\n",
    "\n",
    "mnli_val_loader_s1 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s1,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "mnli_val_dataset_s2 = MNLI_Dataset(mnli_val_s2_gov_ix, mnli_val_gov_labels)\n",
    "\n",
    "mnli_val_loader_s2 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s2,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "test_accuracy = test_model(mnli_val_loader_s1,\n",
    "                           mnli_val_loader_s2,\n",
    "                           model)\n",
    "\n",
    "print (\"Government genre test accuracy is \"+ str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Telephone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telephone genre test accuracy is 0.4766169154228856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adc563/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "## data loaders\n",
    "BATCH_SIZE = mnli_val.shape[0]\n",
    "\n",
    "mnli_val_dataset_s1 = MNLI_Dataset(mnli_val_s1_tel_ix, mnli_val_tel_labels)\n",
    "\n",
    "mnli_val_loader_s1 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s1,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "mnli_val_dataset_s2 = MNLI_Dataset(mnli_val_s2_tel_ix, mnli_val_tel_labels)\n",
    "\n",
    "mnli_val_loader_s2 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s2,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "test_accuracy = test_model(mnli_val_loader_s1,\n",
    "                           mnli_val_loader_s2,\n",
    "                           model)\n",
    "\n",
    "print (\"Telephone genre test accuracy is \"+ str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slate genre test accuracy is 0.40119760479041916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adc563/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "## data loaders\n",
    "BATCH_SIZE = mnli_val.shape[0]\n",
    "\n",
    "mnli_val_dataset_s1 = MNLI_Dataset(mnli_val_s1_slate_ix, mnli_val_slate_labels)\n",
    "\n",
    "mnli_val_loader_s1 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s1,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "mnli_val_dataset_s2 = MNLI_Dataset(mnli_val_s2_slate_ix, mnli_val_slate_labels)\n",
    "\n",
    "mnli_val_loader_s2 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s2,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "test_accuracy = test_model(mnli_val_loader_s1,\n",
    "                           mnli_val_loader_s2,\n",
    "                           model)\n",
    "\n",
    "print (\"Slate genre test accuracy is \"+ str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiction genre test accuracy is 0.4542713567839196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adc563/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "## data loaders\n",
    "BATCH_SIZE = mnli_val.shape[0]\n",
    "\n",
    "mnli_val_dataset_s1 = MNLI_Dataset(mnli_val_s1_fiction_ix, mnli_val_fiction_labels)\n",
    "\n",
    "mnli_val_loader_s1 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s1,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "mnli_val_dataset_s2 = MNLI_Dataset(mnli_val_s2_fiction_ix, mnli_val_fiction_labels)\n",
    "\n",
    "mnli_val_loader_s2 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s2,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "test_accuracy = test_model(mnli_val_loader_s1,\n",
    "                           mnli_val_loader_s2,\n",
    "                           model)\n",
    "\n",
    "print (\"Fiction genre test accuracy is \"+ str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travel genre test accuracy is 0.4460285132382892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adc563/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "## data loaders\n",
    "BATCH_SIZE = mnli_val.shape[0]\n",
    "\n",
    "mnli_val_dataset_s1 = MNLI_Dataset(mnli_val_s1_travel_ix, mnli_val_travel_labels)\n",
    "\n",
    "mnli_val_loader_s1 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s1,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "mnli_val_dataset_s2 = MNLI_Dataset(mnli_val_s2_travel_ix, mnli_val_travel_labels)\n",
    "\n",
    "mnli_val_loader_s2 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s2,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "test_accuracy = test_model(mnli_val_loader_s1,\n",
    "                           mnli_val_loader_s2,\n",
    "                           model)\n",
    "\n",
    "print (\"Travel genre test accuracy is \"+ str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Misclassification Examples\n",
    "\n",
    "Just changed the test model to print out predicted values and original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader1, loader2, model):\n",
    "    \"\"\"\n",
    "    Takes as input validation loaders as loader1 and loader2, \n",
    "    and the model fitted to the training set.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_idx, ([data_s1, lengths_s1, labels], \n",
    "    [data_s2, lengths_s2, labels]) in enumerate(zip(loader1,\n",
    "                                                 loader2)):\n",
    "        ## validation data, length tensors, and val labels \n",
    "        ## to device\n",
    "        data_s1 = data_s1.to(device)\n",
    "        data_s2 = data_s2.to(device)\n",
    "        lengths_s1 = lengths_s1.to(device)\n",
    "        lengths_s2 = lengths_s2.to(device)\n",
    "        labels = labels.to(device)\n",
    "        print (\"labels = \"+str(list(labels.cpu().numpy())))\n",
    "        \n",
    "        outputs = model(data_s1, data_s2, lengths_s1, lengths_s2).to(device)\n",
    "        predicted = torch.max(outputs, 1)[1]\n",
    "        print (\"predicted = \"+str(list(predicted.cpu().numpy())))\n",
    "#         print (\"preds=\"+str(predicted))\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += np.sum((predicted.cpu().detach().numpy() == labels.cpu().numpy()).astype(int))\n",
    "\n",
    "    return (correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Government"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels = tensor([2, 0, 1,  ..., 0, 0, 1], device='cuda:0')\n",
      "predicted = tensor([2, 1, 2,  ..., 0, 0, 2], device='cuda:0')\n",
      "Government genre test accuracy is 0.45866141732283466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adc563/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "## data loaders\n",
    "BATCH_SIZE = mnli_val.shape[0]\n",
    "\n",
    "mnli_val_dataset_s1 = MNLI_Dataset(mnli_val_s1_gov_ix, mnli_val_gov_labels)\n",
    "\n",
    "mnli_val_loader_s1 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s1,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "mnli_val_dataset_s2 = MNLI_Dataset(mnli_val_s2_gov_ix, mnli_val_gov_labels)\n",
    "\n",
    "mnli_val_loader_s2 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s2,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "test_accuracy = test_model(mnli_val_loader_s1,\n",
    "                           mnli_val_loader_s2,\n",
    "                           model)\n",
    "\n",
    "print (\"Government genre test accuracy is \"+ str(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IDPA', \"'s\", 'OIG', \"'s\", 'mission', 'is', 'to', 'prevent', ',', 'detect', ',', 'and', 'eliminate', 'fraud', ',', 'waste', ',', 'abuse', ',', 'and']\n",
      "['IDPA', \"'s\", 'OIG', \"'s\", 'mission', 'is', 'clear', 'and', 'cares', 'about', 'payment', 'programs', '.', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01']\n"
     ]
    }
   ],
   "source": [
    "print ([id2token_wiki[token] for token in np.array([*mnli_val_loader_s1][0][0][1])][:20])\n",
    "print ([id2token_wiki[token] for token in np.array([*mnli_val_loader_s2][0][0][1])][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Telephone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels = tensor([2, 0, 1,  ..., 1, 0, 2], device='cuda:0')\n",
      "predicted = tensor([0, 1, 0,  ..., 1, 2, 0], device='cuda:0')\n",
      "Telephone genre test accuracy is 0.4507462686567164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adc563/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "## data loaders\n",
    "BATCH_SIZE = mnli_val.shape[0]\n",
    "\n",
    "mnli_val_dataset_s1 = MNLI_Dataset(mnli_val_s1_tel_ix, mnli_val_tel_labels)\n",
    "\n",
    "mnli_val_loader_s1 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s1,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "mnli_val_dataset_s2 = MNLI_Dataset(mnli_val_s2_tel_ix, mnli_val_tel_labels)\n",
    "\n",
    "mnli_val_loader_s2 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s2,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "test_accuracy = test_model(mnli_val_loader_s1,\n",
    "                           mnli_val_loader_s2,\n",
    "                           model)\n",
    "\n",
    "print (\"Telephone genre test accuracy is \"+ str(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['because', 'like', 'Tech', 'is', 'known', 'to', 'be', 'a', 'good', 'engineering', 'school', 'and', 'A', 'and', 'M', 'maybe', 'is', 'known', 'more', 'for']\n",
      "['A', 'and', 'M', \"'s\", 'computer', 'department', 'is', 'unk', 'very', 'well', 'regarded', '.', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01']\n"
     ]
    }
   ],
   "source": [
    "print ([id2token_wiki[token] for token in np.array([*mnli_val_loader_s1][0][0][-1])][:20])\n",
    "print ([id2token_wiki[token] for token in np.array([*mnli_val_loader_s2][0][0][-1])][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels = [0, 1, 2, 0, 2, 2, 0, 1, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 0, 1, 1, 0, 1, 2, 2, 0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 0, 1, 2, 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 1, 0, 2, 2, 0, 2, 0, 1, 2, 2, 0, 2, 0, 1, 0, 1, 2, 0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 0, 1, 0, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 1, 0, 0, 2, 2, 1, 2, 1, 0, 1, 0, 2, 2, 2, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 2, 0, 0, 1, 1, 1, 2, 1, 0, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 1, 2, 1, 1, 0, 1, 1, 2, 0, 0, 0, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 2, 0, 0, 1, 1, 2, 0, 1, 1, 0, 0, 1, 1, 2, 2, 1, 2, 1, 1, 0, 0, 2, 1, 1, 2, 0, 2, 1, 1, 0, 2, 1, 0, 0, 0, 1, 2, 2, 2, 2, 0, 1, 0, 0, 2, 2, 1, 2, 0, 0, 2, 2, 1, 1, 1, 1, 1, 1, 0, 2, 0, 1, 0, 0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 2, 0, 0, 1, 0, 1, 0, 2, 0, 1, 2, 1, 1, 0, 0, 2, 0, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 2, 0, 2, 0, 2, 0, 2, 0, 1, 1, 0, 1, 1, 1, 2, 2, 0, 2, 1, 2, 1, 0, 0, 2, 0, 2, 2, 1, 1, 0, 1, 0, 1, 2, 2, 0, 0, 2, 0, 2, 1, 0, 1, 1, 0, 0, 2, 0, 2, 0, 0, 2, 2, 2, 0, 0, 2, 1, 1, 2, 2, 0, 2, 0, 2, 1, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 0, 2, 1, 1, 2, 1, 2, 2, 0, 2, 0, 1, 0, 1, 1, 1, 2, 0, 0, 1, 2, 0, 1, 2, 2, 1, 1, 1, 2, 2, 0, 1, 2, 0, 2, 0, 2, 2, 2, 0, 1, 1, 0, 2, 0, 0, 2, 2, 0, 1, 0, 1, 0, 0, 1, 0, 2, 2, 1, 2, 0, 2, 1, 0, 1, 2, 1, 0, 0, 2, 0, 2, 0, 1, 2, 2, 2, 2, 1, 0, 2, 2, 0, 2, 0, 2, 0, 1, 2, 1, 2, 1, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 0, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 0, 1, 0, 2, 0, 1, 1, 2, 0, 1, 1, 1, 0, 1, 1, 2, 2, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2, 1, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 2, 1, 1, 0, 2, 0, 1, 0, 0, 0, 2, 1, 2, 0, 1, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 2, 1, 2, 2, 0, 2, 2, 1, 1, 2, 0, 1, 2, 1, 0, 0, 1, 0, 1, 2, 2, 0, 0, 1, 0, 2, 2, 1, 0, 1, 2, 2, 0, 1, 0, 0, 2, 2, 1, 1, 0, 1, 2, 1, 0, 1, 1, 1, 0, 2, 0, 0, 2, 2, 2, 1, 1, 0, 1, 1, 0, 2, 2, 2, 0, 2, 2, 0, 0, 0, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 2, 1, 2, 0, 2, 2, 0, 1, 1, 2, 1, 0, 0, 2, 1, 2, 0, 2, 0, 0, 2, 0, 1, 1, 1, 2, 0, 2, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 2, 1, 2, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 0, 2, 1, 2, 2, 2, 0, 2, 2, 0, 2, 1, 1, 1, 1, 2, 2, 2, 2, 0, 1, 0, 1, 0, 1, 2, 2, 1, 2, 0, 1, 2, 0, 0, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 2, 1, 0, 2, 2, 0, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 1, 1, 0, 2, 1, 1, 0, 2, 1, 1, 0, 2, 0, 2, 1, 1, 1, 1, 1, 0, 2, 2, 1, 2, 1, 0, 2, 1, 2, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 1, 2, 0, 2, 0, 0, 1, 2, 0, 1, 2, 1, 2, 2, 2, 1, 0, 1, 2, 1, 0, 0, 1, 1, 1, 0, 1, 2, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 2, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0, 1, 1, 1, 0, 2, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 2, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 1, 0, 1, 0, 1, 2, 0, 2, 1, 0, 0, 1, 1, 1, 0, 1, 0, 2, 1, 0, 1, 1, 0, 1, 2, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 2, 0, 0, 1, 1]\n",
      "predicted = [0, 1, 2, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 1, 2, 1, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 0, 1, 1, 1, 2, 0, 0, 1, 2, 0, 2, 1, 1, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 1, 0, 2, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 0, 2, 1, 1, 1, 0, 1, 0, 1, 0, 1, 2, 2, 1, 1, 1, 2, 0, 2, 0, 1, 1, 2, 0, 2, 0, 1, 0, 1, 2, 0, 1, 1, 1, 0, 2, 0, 0, 1, 2, 0, 1, 1, 1, 0, 0, 0, 1, 1, 2, 0, 2, 1, 1, 0, 0, 0, 1, 2, 0, 0, 1, 1, 1, 0, 2, 2, 0, 2, 1, 1, 1, 1, 0, 2, 1, 1, 2, 2, 1, 2, 2, 0, 1, 1, 1, 1, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 2, 1, 2, 0, 1, 1, 1, 1, 1, 0, 2, 1, 0, 0, 1, 0, 2, 1, 1, 2, 0, 0, 1, 2, 1, 2, 2, 1, 0, 1, 0, 0, 0, 2, 1, 1, 0, 1, 2, 0, 0, 0, 0, 2, 2, 0, 0, 2, 2, 2, 0, 0, 2, 0, 1, 0, 2, 0, 2, 2, 1, 2, 1, 1, 2, 1, 1, 0, 2, 1, 0, 1, 0, 1, 0, 2, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 0, 0, 2, 1, 1, 2, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 2, 1, 0, 1, 1, 2, 1, 1, 1, 1, 0, 2, 1, 1, 2, 0, 0, 0, 0, 2, 1, 0, 2, 2, 0, 0, 1, 1, 2, 2, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 2, 0, 1, 0, 1, 0, 1, 2, 0, 2, 2, 1, 1, 0, 0, 1, 2, 0, 2, 0, 1, 0, 1, 2, 0, 0, 2, 2, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 2, 2, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 2, 0, 0, 2, 1, 0, 1, 0, 0, 1, 2, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 2, 0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 2, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 2, 2, 1, 0, 0, 2, 0, 2, 2, 2, 2, 2, 0, 1, 2, 0, 1, 0, 0, 1, 1, 0, 0, 0, 2, 1, 2, 0, 1, 0, 1, 0, 2, 0, 0, 1, 2, 1, 2, 2, 1, 0, 0, 1, 2, 1, 1, 0, 2, 0, 1, 1, 0, 2, 1, 0, 1, 0, 1, 2, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 1, 1, 2, 0, 1, 0, 0, 1, 2, 1, 0, 1, 0, 1, 1, 2, 0, 0, 2, 0, 2, 0, 2, 1, 0, 2, 2, 0, 1, 1, 0, 2, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 2, 1, 1, 1, 0, 0, 1, 2, 0, 1, 1, 1, 0, 2, 1, 0, 2, 0, 0, 2, 2, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 2, 2, 2, 2, 0, 0, 2, 0, 1, 2, 0, 1, 0, 1, 0, 1, 2, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 1, 0, 1, 1, 0, 0, 2, 1, 0, 1, 2, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 2, 0, 1, 2, 0, 0, 0, 0, 2, 1, 0, 2, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 2, 0, 1, 1, 2, 2, 0, 0, 0, 0, 0, 1, 2, 2, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 2, 1, 2, 0, 2, 2, 1, 1, 1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 1, 1, 2, 2, 1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 2, 0, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 1, 1, 2, 0, 0, 0, 2, 1, 1, 0, 0, 2, 2, 2, 1, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 0, 1, 2, 0, 1, 2, 1, 0, 2, 2, 2, 0, 1, 0, 2, 2, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 0, 1, 0, 1, 2, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 1, 0, 0, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 0, 2, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 1, 2, 0, 1, 0, 0, 1, 1, 1, 1, 2, 0, 0, 2, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 1, 1, 0, 1, 2]\n",
      "Slate genre test accuracy is 0.38822355289421157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adc563/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "## data loaders\n",
    "BATCH_SIZE = mnli_val.shape[0]\n",
    "\n",
    "mnli_val_dataset_s1 = MNLI_Dataset(mnli_val_s1_slate_ix, mnli_val_slate_labels)\n",
    "\n",
    "mnli_val_loader_s1 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s1,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "mnli_val_dataset_s2 = MNLI_Dataset(mnli_val_s2_slate_ix, mnli_val_slate_labels)\n",
    "\n",
    "mnli_val_loader_s2 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s2,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "test_accuracy = test_model(mnli_val_loader_s1,\n",
    "                           mnli_val_loader_s2,\n",
    "                           model)\n",
    "\n",
    "print (\"Slate genre test accuracy is \"+ str(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'button', 'on', 'the', 'Chatterbox', 'page', 'will', 'make', 'this', 'easy', ',', 'so', 'please', 'do', 'join', 'in', '.', '\\x01', '\\x01', '\\x01']\n",
      "['They', 'had', 'to', 'submit', 'a', 'written', 'request', 'before', 'being', 'accepted', '.', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01']\n"
     ]
    }
   ],
   "source": [
    "print ([id2token_wiki[token] for token in np.array([*mnli_val_loader_s1][0][0][5])][:20])\n",
    "print ([id2token_wiki[token] for token in np.array([*mnli_val_loader_s2][0][0][5])][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels = [0, 2, 1, 2, 1, 1, 2, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 1, 2, 0, 1, 1, 1, 2, 0, 2, 2, 1, 0, 0, 2, 0, 1, 2, 2, 0, 0, 1, 2, 1, 0, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 0, 2, 0, 2, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 2, 0, 1, 2, 0, 2, 1, 2, 1, 0, 2, 2, 0, 2, 0, 2, 0, 1, 1, 1, 2, 2, 0, 0, 1, 0, 1, 0, 2, 2, 0, 2, 0, 2, 2, 2, 1, 0, 1, 2, 0, 2, 2, 1, 0, 1, 0, 2, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 2, 2, 0, 0, 2, 2, 1, 0, 2, 2, 1, 0, 2, 1, 0, 1, 1, 0, 2, 0, 0, 2, 2, 0, 0, 2, 0, 2, 0, 0, 0, 2, 2, 2, 2, 0, 1, 0, 1, 0, 0, 1, 1, 2, 1, 0, 1, 2, 0, 1, 0, 0, 1, 0, 1, 2, 0, 0, 1, 2, 2, 1, 2, 0, 1, 1, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 1, 0, 1, 0, 2, 2, 1, 2, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 2, 1, 2, 1, 1, 1, 0, 1, 2, 1, 1, 1, 2, 2, 2, 0, 1, 0, 2, 2, 0, 0, 1, 2, 2, 2, 0, 2, 0, 0, 0, 2, 0, 2, 1, 1, 2, 2, 1, 1, 0, 0, 0, 2, 0, 2, 0, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 0, 0, 0, 1, 1, 2, 1, 1, 0, 1, 0, 2, 2, 0, 2, 2, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 2, 0, 0, 2, 1, 2, 1, 1, 1, 0, 0, 0, 1, 0, 2, 0, 1, 2, 1, 2, 1, 1, 1, 2, 1, 0, 1, 0, 1, 0, 1, 1, 0, 2, 2, 2, 0, 0, 2, 0, 1, 2, 0, 0, 1, 2, 2, 1, 2, 0, 0, 2, 1, 1, 2, 0, 0, 1, 2, 0, 1, 1, 2, 0, 1, 2, 0, 2, 2, 2, 0, 1, 2, 0, 2, 1, 2, 0, 1, 2, 1, 1, 0, 0, 0, 1, 1, 0, 1, 2, 0, 2, 2, 1, 2, 0, 1, 0, 2, 2, 0, 2, 2, 2, 0, 1, 0, 0, 2, 2, 2, 0, 1, 1, 0, 1, 1, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 0, 1, 1, 0, 2, 1, 1, 0, 2, 1, 0, 2, 2, 1, 2, 1, 0, 1, 2, 2, 0, 2, 0, 1, 1, 1, 0, 1, 0, 2, 2, 1, 0, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 0, 1, 1, 2, 0, 0, 2, 1, 0, 2, 1, 2, 0, 1, 1, 0, 1, 0, 2, 0, 2, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 0, 1, 0, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 1, 0, 0, 1, 2, 2, 1, 0, 0, 2, 0, 0, 2, 2, 1, 2, 2, 2, 0, 2, 0, 2, 1, 0, 2, 0, 1, 0, 2, 1, 2, 0, 0, 2, 2, 1, 0, 2, 1, 2, 2, 0, 0, 1, 0, 1, 2, 1, 0, 0, 1, 1, 2, 0, 2, 2, 1, 2, 1, 2, 0, 1, 2, 2, 0, 2, 1, 1, 0, 1, 0, 0, 2, 1, 1, 2, 2, 2, 0, 0, 2, 0, 0, 0, 2, 2, 1, 1, 2, 0, 1, 0, 1, 0, 0, 1, 1, 2, 0, 2, 0, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 0, 0, 1, 2, 2, 2, 1, 1, 1, 2, 1, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0, 2, 1, 0, 0, 1, 2, 0, 1, 1, 0, 2, 2, 0, 0, 2, 0, 0, 2, 0, 1, 1, 0, 1, 1, 2, 0, 1, 2, 2, 0, 1, 2, 2, 0, 0, 2, 0, 2, 2, 1, 1, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 2, 0, 2, 2, 2, 2, 0, 1, 1, 2, 2, 1, 2, 0, 2, 0, 1, 1, 2, 1, 2, 0, 0, 2, 2, 1, 2, 0, 2, 2, 1, 0, 0, 1, 2, 2, 0, 0, 1, 1, 1, 1, 0, 1, 2, 2, 0, 0, 1, 0, 2, 2, 0, 2, 0, 0, 0, 1, 0, 1, 0, 1, 1, 2, 2, 2, 1, 2, 0, 0, 0, 2, 2, 2, 0, 1, 2, 2, 0, 2, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 2, 0, 0, 0, 1, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 1, 0, 2, 0, 2, 1, 0, 1, 2, 0, 0, 0, 0, 2, 2, 1, 2, 1, 1, 0, 1, 1, 0, 2, 1, 2, 0, 0, 1, 2, 1, 1, 0, 1, 2, 1, 2, 0, 1, 1, 0, 1, 1, 2, 2, 0, 0, 2, 2, 0, 2, 1, 1, 0, 2, 0, 2, 2, 1, 0, 1, 0, 0, 1, 0, 2, 2, 0, 1, 1, 1, 0, 2, 2, 1, 2, 0, 0, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 0, 2, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 2, 2, 1, 1, 1, 2, 1, 0, 0, 2, 2, 1, 0, 2, 2, 2, 1, 2, 0, 2, 2, 2, 0, 2, 1, 0]\n",
      "predicted = [1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 2, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 2, 0, 1, 1, 1, 2, 1, 2, 2, 1, 2, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 0, 2, 0, 2, 1, 1, 1, 0, 0, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0, 1, 1, 2, 0, 2, 2, 2, 2, 2, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 2, 1, 0, 0, 0, 1, 0, 2, 2, 2, 1, 1, 1, 1, 0, 0, 1, 2, 0, 0, 2, 1, 2, 1, 2, 0, 2, 1, 2, 0, 0, 0, 0, 1, 2, 0, 2, 1, 0, 1, 1, 2, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 2, 0, 1, 2, 2, 0, 1, 0, 1, 0, 2, 2, 2, 1, 2, 1, 2, 0, 0, 1, 0, 2, 2, 0, 2, 1, 2, 1, 1, 0, 1, 1, 2, 2, 1, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 2, 1, 0, 0, 1, 2, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 2, 0, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1, 1, 2, 0, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 0, 2, 1, 1, 1, 2, 0, 1, 0, 1, 2, 0, 2, 2, 1, 0, 0, 0, 2, 1, 1, 1, 0, 1, 0, 1, 1, 2, 1, 1, 2, 0, 2, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 2, 0, 0, 0, 2, 1, 2, 1, 2, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 2, 1, 0, 0, 0, 0, 2, 0, 1, 2, 1, 0, 2, 2, 1, 0, 1, 1, 0, 2, 1, 2, 1, 0, 2, 2, 2, 2, 2, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0, 1, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 2, 1, 0, 2, 1, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 1, 0, 1, 1, 2, 1, 0, 0, 0, 1, 1, 2, 2, 2, 0, 2, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 2, 0, 2, 1, 0, 1, 1, 2, 2, 1, 0, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 0, 2, 2, 0, 1, 2, 0, 0, 1, 1, 2, 1, 1, 1, 0, 1, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 0, 2, 1, 2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 2, 1, 2, 1, 0, 0, 2, 0, 1, 2, 0, 1, 0, 1, 1, 0, 0, 2, 1, 2, 0, 1, 0, 0, 0, 0, 1, 2, 1, 2, 1, 0, 1, 2, 2, 1, 1, 1, 0, 0, 1, 2, 2, 2, 1, 2, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 2, 1, 1, 2, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1, 0, 2, 1, 1, 2, 0, 2, 2, 0, 2, 2, 1, 0, 1, 1, 0, 2, 1, 1, 2, 1, 1, 0, 2, 1, 0, 0, 0, 1, 2, 1, 0, 0, 1, 0, 2, 0, 1, 0, 1, 2, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 2, 0, 1, 2, 2, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 2, 2, 2, 0, 0, 1, 2, 1, 0, 0, 0, 1, 2, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 2, 2, 0, 0, 1, 1, 1, 1, 0, 1, 2, 0, 0, 1, 0, 1, 2, 1, 0, 2, 1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 2, 0, 0, 1, 0, 0, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 0, 0, 2, 1, 2, 0, 1, 2, 0, 0, 2, 2, 2, 0, 2, 1, 2, 2, 1, 1, 0, 1, 0, 0, 2, 1, 2, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 2, 1, 1, 0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 2, 0, 2, 2, 0, 0, 0, 2, 0, 0, 1, 0, 2, 1, 1, 1, 2, 1, 1, 0, 2, 2, 1, 2, 0, 1, 0, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 0, 1, 1, 1, 0, 0, 2, 2, 1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0]\n",
      "Fiction genre test accuracy is 0.40804020100502514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adc563/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "## data loaders\n",
    "BATCH_SIZE = mnli_val.shape[0]\n",
    "\n",
    "mnli_val_dataset_s1 = MNLI_Dataset(mnli_val_s1_fiction_ix, mnli_val_fiction_labels)\n",
    "\n",
    "mnli_val_loader_s1 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s1,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "mnli_val_dataset_s2 = MNLI_Dataset(mnli_val_s2_fiction_ix, mnli_val_fiction_labels)\n",
    "\n",
    "mnli_val_loader_s2 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s2,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "test_accuracy = test_model(mnli_val_loader_s1,\n",
    "                           mnli_val_loader_s2,\n",
    "                           model)\n",
    "\n",
    "print (\"Fiction genre test accuracy is \"+ str(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'if', 'they', 'did', 'come', ',', 'as', 'remote', 'as', 'that', 'is', ',', 'you', 'and', 'your', 'men', 'look', 'strong', 'enough', 'to', 'handle', 'anything', '.', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01']\n",
      "['The', 'men', 'were', 'warriors', '.', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01', '\\x01']\n"
     ]
    }
   ],
   "source": [
    "print ([id2token_wiki[token] for token in np.array([*mnli_val_loader_s1][0][0][5])][:30])\n",
    "print ([id2token_wiki[token] for token in np.array([*mnli_val_loader_s2][0][0][5])][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels = [1, 1, 2, 2, 0, 0, 2, 0, 2, 0, 1, 0, 1, 0, 1, 2, 2, 1, 1, 0, 2, 0, 0, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 1, 0, 2, 1, 0, 2, 2, 1, 0, 2, 0, 2, 1, 2, 2, 1, 2, 0, 1, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 2, 0, 0, 1, 0, 1, 0, 2, 1, 2, 0, 2, 0, 1, 0, 0, 2, 1, 2, 2, 1, 2, 2, 2, 2, 0, 1, 0, 2, 1, 1, 1, 1, 0, 0, 2, 1, 2, 1, 1, 2, 2, 1, 0, 1, 0, 0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 2, 1, 0, 1, 2, 0, 0, 0, 2, 2, 2, 1, 2, 0, 0, 1, 0, 2, 2, 2, 0, 0, 0, 2, 2, 0, 1, 0, 1, 2, 2, 1, 1, 1, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 2, 1, 2, 0, 0, 2, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 0, 1, 1, 2, 2, 1, 0, 1, 0, 0, 2, 2, 1, 2, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 1, 2, 0, 2, 1, 1, 0, 0, 2, 0, 2, 0, 1, 0, 1, 0, 2, 0, 1, 1, 1, 2, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 1, 2, 2, 2, 0, 1, 1, 2, 0, 0, 0, 2, 1, 2, 1, 0, 2, 0, 1, 2, 2, 0, 1, 0, 0, 0, 1, 1, 2, 0, 2, 0, 1, 1, 1, 2, 0, 1, 0, 1, 2, 2, 2, 0, 1, 2, 2, 2, 0, 2, 0, 1, 1, 0, 0, 0, 1, 1, 2, 2, 1, 2, 0, 0, 2, 1, 2, 1, 0, 2, 1, 2, 1, 0, 2, 0, 2, 2, 2, 0, 0, 2, 1, 2, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 0, 2, 2, 1, 2, 2, 0, 1, 0, 1, 1, 2, 1, 1, 2, 2, 1, 2, 0, 2, 0, 1, 0, 1, 1, 0, 1, 2, 0, 2, 1, 1, 1, 0, 1, 1, 1, 2, 1, 2, 1, 2, 2, 0, 2, 2, 2, 0, 1, 0, 0, 0, 2, 0, 1, 0, 1, 1, 2, 1, 1, 1, 0, 1, 2, 2, 2, 0, 0, 1, 2, 2, 2, 0, 1, 2, 1, 0, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 1, 0, 1, 1, 2, 2, 0, 1, 0, 2, 2, 0, 2, 0, 2, 0, 2, 0, 1, 2, 1, 2, 2, 1, 2, 2, 0, 0, 2, 1, 2, 2, 2, 0, 0, 0, 0, 2, 1, 0, 2, 2, 2, 1, 0, 2, 0, 2, 2, 0, 1, 0, 0, 2, 0, 0, 1, 0, 2, 0, 2, 2, 1, 0, 1, 1, 2, 2, 2, 2, 2, 2, 0, 1, 0, 0, 0, 1, 2, 1, 1, 0, 0, 0, 1, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 2, 1, 0, 2, 2, 2, 1, 1, 1, 0, 0, 2, 1, 1, 2, 2, 1, 2, 2, 2, 0, 0, 0, 1, 1, 0, 1, 0, 1, 2, 1, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 1, 2, 0, 0, 1, 0, 2, 0, 2, 2, 1, 0, 1, 1, 0, 2, 0, 0, 2, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 1, 2, 1, 1, 0, 2, 0, 2, 2, 1, 1, 1, 0, 1, 2, 1, 2, 2, 0, 1, 1, 0, 1, 0, 0, 2, 0, 0, 2, 0, 2, 2, 1, 0, 0, 0, 1, 2, 0, 1, 0, 2, 0, 0, 1, 0, 2, 2, 2, 0, 2, 1, 0, 0, 2, 2, 1, 2, 1, 1, 0, 1, 2, 1, 2, 1, 2, 2, 1, 1, 0, 1, 2, 0, 2, 1, 2, 0, 1, 1, 0, 0, 1, 2, 2, 0, 2, 2, 0, 2, 2, 1, 0, 1, 0, 0, 1, 2, 2, 1, 0, 0, 1, 0, 1, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 1, 2, 1, 0, 2, 1, 0, 1, 1, 1, 0, 1, 2, 0, 1, 0, 2, 1, 1, 2, 2, 0, 2, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 2, 2, 0, 1, 2, 2, 1, 2, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 0, 2, 2, 0, 1, 1, 2, 0, 1, 2, 0, 1, 0, 2, 0, 2, 0, 2, 2, 0, 1, 0, 1, 2, 2, 1, 2, 2, 2, 0, 2, 2, 1, 0, 2, 0, 1, 0, 2, 0, 0, 1, 2, 0, 1, 1, 1, 0, 1, 2, 0, 0, 2, 0, 1, 0, 0, 2, 2, 2, 0, 1, 2, 2, 0, 2, 2, 0, 1, 0, 2, 0, 0, 1, 1, 0, 0, 1, 2, 2, 2, 0, 2, 1, 2, 1, 0, 2, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 0, 0, 1, 0, 0, 2, 0, 2, 0, 0, 1, 0, 0, 0, 2, 2, 0, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 0, 0, 0, 2, 2]\n",
      "predicted = [0, 1, 0, 0, 1, 2, 2, 1, 2, 0, 2, 0, 0, 2, 0, 2, 0, 1, 0, 0, 2, 1, 0, 2, 0, 0, 1, 2, 2, 1, 0, 0, 2, 2, 2, 2, 0, 1, 0, 0, 0, 2, 0, 1, 1, 0, 1, 2, 1, 0, 0, 1, 0, 2, 2, 1, 0, 2, 0, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 2, 0, 1, 0, 1, 2, 2, 1, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 0, 2, 1, 1, 0, 2, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 1, 2, 2, 0, 1, 1, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 0, 2, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 2, 1, 1, 1, 2, 1, 0, 1, 0, 0, 0, 0, 2, 1, 1, 2, 1, 1, 2, 0, 1, 0, 0, 1, 2, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 2, 0, 0, 1, 1, 2, 0, 1, 0, 1, 0, 0, 1, 2, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 0, 0, 0, 2, 0, 0, 0, 1, 1, 2, 0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0, 1, 1, 2, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 2, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 2, 1, 1, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 2, 2, 2, 1, 0, 1, 0, 2, 2, 1, 2, 1, 2, 1, 2, 0, 1, 1, 1, 2, 1, 1, 2, 0, 2, 0, 1, 2, 1, 2, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 0, 0, 1, 2, 1, 1, 0, 0, 0, 2, 2, 1, 0, 0, 1, 1, 2, 1, 0, 1, 2, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 0, 2, 1, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 2, 2, 0, 0, 2, 0, 1, 2, 1, 2, 0, 0, 1, 2, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 2, 1, 0, 2, 0, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 2, 1, 2, 0, 1, 0, 2, 2, 0, 1, 1, 1, 1, 1, 0, 1, 2, 2, 1, 0, 0, 1, 0, 0, 2, 2, 1, 1, 0, 1, 0, 2, 2, 2, 1, 1, 1, 2, 2, 0, 1, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 2, 1, 0, 1, 1, 1, 2, 0, 0, 2, 1, 1, 0, 2, 0, 0, 2, 0, 2, 2, 0, 2, 0, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 2, 2, 1, 0, 1, 2, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 2, 0, 1, 0, 1, 0, 0, 0, 0, 1, 2, 0, 1, 1, 2, 0, 1, 1, 1, 0, 1, 1, 1, 0, 2, 2, 1, 1, 1, 0, 2, 0, 2, 2, 1, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 1, 0, 1, 2, 0, 0, 1, 2, 0, 2, 2, 0, 1, 2, 2, 0, 1, 2, 0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 0, 0, 1, 2, 2, 1, 1, 1, 1, 2, 0, 1, 1, 2, 2, 2, 2, 1, 0, 0, 2, 2, 1, 2, 2, 1, 0, 2, 0, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 2, 0, 2, 1, 1, 1, 2, 1, 0, 1, 0, 1, 0, 1, 1, 2, 0, 1, 2, 1, 0, 2, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0, 1, 0, 0, 1, 1, 2, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 2, 0, 0, 0, 2, 0, 1, 0, 0, 1, 1, 2, 1, 0, 2, 0, 1, 0, 0, 2, 2, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 2, 2, 2, 1, 2, 1, 0, 0, 0, 1, 2, 0, 0, 2, 1, 2, 1, 1, 0, 0, 0, 2, 1, 2, 1, 0, 0, 1, 2, 1, 0, 0, 1, 1, 2, 0, 2, 0, 2, 1, 0, 0, 0, 1, 0, 0, 1, 2, 2, 1, 2, 0, 1, 1, 2, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 1, 2, 0, 1, 0, 2, 2, 2, 0, 2, 0, 0, 1, 1, 0, 1, 1, 0, 2, 2, 0, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 2, 1, 0, 0, 2, 2, 1, 1, 0, 0, 0, 2, 1, 2, 1, 1, 1, 0, 0, 1, 2, 0, 2, 2, 0, 0, 1, 1, 1]\n",
      "Travel genre test accuracy is 0.42668024439918534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adc563/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "## data loaders\n",
    "BATCH_SIZE = mnli_val.shape[0]\n",
    "\n",
    "mnli_val_dataset_s1 = MNLI_Dataset(mnli_val_s1_travel_ix, mnli_val_travel_labels)\n",
    "\n",
    "mnli_val_loader_s1 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s1,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "mnli_val_dataset_s2 = MNLI_Dataset(mnli_val_s2_travel_ix, mnli_val_travel_labels)\n",
    "\n",
    "mnli_val_loader_s2 = torch.utils.data.DataLoader(dataset=mnli_val_dataset_s2,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=False)\n",
    "\n",
    "test_accuracy = test_model(mnli_val_loader_s1,\n",
    "                           mnli_val_loader_s2,\n",
    "                           model)\n",
    "\n",
    "print (\"Travel genre test accuracy is \"+ str(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'enclosed', 'but', 'airy', 'building', ',', 'you', \"'ll\", 'find', 'ladies', 'with', 'large', 'machetes', 'expertly', 'chopping', 'off', 'hunks', 'of', 'kingfish', ',', 'tuna', ',', 'or', 'shark', 'for', 'eager', 'buyers', '.', '\\x01']\n",
      "['You', \"'ll\", 'find', 'small', 'lepers', 'chopping', 'of', 'chunks', 'of', 'tuna', ',', 'its', 'the', 'only', 'place', 'they', 'can', 'work', '.', '\\x01']\n"
     ]
    }
   ],
   "source": [
    "print ([id2token_wiki[token] for token in np.array([*mnli_val_loader_s1][0][0][2])][:30])\n",
    "print ([id2token_wiki[token] for token in np.array([*mnli_val_loader_s2][0][0][2])][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
