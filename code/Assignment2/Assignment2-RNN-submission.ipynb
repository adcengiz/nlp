{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"tocheading\">Table of Contents</h2>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import random\n",
    "import spacy\n",
    "import csv\n",
    "import string\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Upload & Preprocessing\n",
    "The datasets provided are already tokenized. Thus, without running the data through a tokenizer, we use pretrained word embeddings (e.g. fast-Text) to embed the tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Vectors\n",
    "\n",
    "The web page for recommended word vector sets can be found here: https://fasttext.cc/docs/en/english-vectors.html wiki-news-300d-1M.vec from Mikolov et al (2018, Advances in Pre-Training Distributed Word Representations) 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens) is used in this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', \n",
    "                  newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        ## convert all maps to lists\n",
    "        data[tokens[0]] = [*map(float, tokens[1:])]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the wiki word vectors\n",
    "fname = \"wiki-news-300d-1M.vec\"\n",
    "word_vectors = load_vectors(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab_tokens = [*word_vectors.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique tokens in the wiki news English vectors is 999994\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of unique tokens in the wiki news English vectors is \" + str(len(all_vocab_tokens) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from Vocab Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_df = pd.DataFrame(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_df = word_vector_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_lookup = np.array(word_vector_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_vocab(table_df):\n",
    "    \n",
    "    token_array = np.array([*table_df.index])\n",
    "    num_index_array = np.array([*range(table_df.shape[0])])\n",
    "    \n",
    "    token2id = {}\n",
    "    id2token = {}\n",
    "    for i in [*range(len(token_array))]:\n",
    "        token2id[token_array[i]] = num_index_array[i]\n",
    "        id2token[num_index_array[i]] = token_array[i]\n",
    "\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id_wiki, id2token_wiki = index_vocab(word_vector_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check for table correctness!__\n",
    "\n",
    "Do token2id and id2token match each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93141"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id_wiki[\"Alberto\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alberto'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2token_wiki[93141]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the table fit the initial word vector vocab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(word_vectors[\"Alberto\"] == table_lookup[93141])==True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1: SNLI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\"entailment\":0,\n",
    "             \"neutral\":1,\n",
    "             \"contradiction\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train = pd.read_table(\"data/snli_train.tsv\")\n",
    "snli_val = pd.read_table(\"data/snli_val.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## get tokenized training data\n",
    "snli_train[\"sentence1\"] = snli_train[\"sentence1\"].apply(lambda x: x.split(\" \"))\n",
    "snli_train[\"sentence2\"] = snli_train[\"sentence2\"].apply(lambda x: x.split(\" \"))\n",
    "\n",
    "## get numeric labels\n",
    "snli_train[\"label_num\"] = snli_train[\"label\"].apply(lambda x: label_dict[x])\n",
    "snli_val[\"label_num\"] = snli_val[\"label\"].apply(lambda x: label_dict[x])\n",
    "\n",
    "## get tokenized validation data\n",
    "snli_val[\"sentence1\"] = snli_val[\"sentence1\"].apply(lambda x: x.split(\" \"))\n",
    "snli_val[\"sentence2\"] = snli_val[\"sentence2\"].apply(lambda x: x.split(\" \"))\n",
    "\n",
    "## write original indices\n",
    "snli_train[\"original_index\"] = snli_train.index.values\n",
    "snli_val[\"original_index\"] = snli_val.index.values\n",
    "\n",
    "## label arrays\n",
    "snli_train_labels = np.array(snli_train[\"label_num\"])\n",
    "snli_val_labels = np.array(snli_val[\"label_num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: MultiNLI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train = pd.read_table(\"data/mnli_train.tsv\")\n",
    "mnli_val = pd.read_table(\"data/mnli_val.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and now that was in fifty one that 's forty ye...</td>\n",
       "      <td>It was already a problem forty years ago but n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jon could smell baked bread on the air and his...</td>\n",
       "      <td>Jon smelt food in the air and was hungry .</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it will be like Italian basketball with the uh...</td>\n",
       "      <td>This type of Italian basketball is nothing lik...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  and now that was in fifty one that 's forty ye...   \n",
       "1  Jon could smell baked bread on the air and his...   \n",
       "2  it will be like Italian basketball with the uh...   \n",
       "\n",
       "                                           sentence2          label      genre  \n",
       "0  It was already a problem forty years ago but n...        neutral  telephone  \n",
       "1         Jon smelt food in the air and was hungry .        neutral    fiction  \n",
       "2  This type of Italian basketball is nothing lik...  contradiction  telephone  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get tokenized training data\n",
    "mnli_train[\"sentence1\"] = mnli_train[\"sentence1\"].apply(lambda x: x.split(\" \"))\n",
    "mnli_train[\"sentence2\"] = mnli_train[\"sentence2\"].apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get tokenized validation data\n",
    "mnli_val[\"sentence1\"] = mnli_val[\"sentence1\"].apply(lambda x: x.split(\" \"))\n",
    "mnli_val[\"sentence2\"] = mnli_val[\"sentence2\"].apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get labels\n",
    "mnli_train[\"label_num\"] = mnli_train[\"label\"].apply(lambda x: label_dict[x])\n",
    "mnli_val[\"label_num\"] = mnli_val[\"label\"].apply(lambda x: label_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get train and val datasets for each __MNLI genre__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train[\"original_index\"] = mnli_train.index.values\n",
    "mnli_val[\"original_index\"] = mnli_val.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## telephone\n",
    "mnli_train_telephone = mnli_train[mnli_train[\"genre\"]==\"telephone\"]\n",
    "mnli_val_telephone = mnli_val[mnli_val[\"genre\"]==\"telephone\"]\n",
    "## slate\n",
    "mnli_train_slate = mnli_train[mnli_train[\"genre\"]==\"slate\"]\n",
    "mnli_val_slate = mnli_val[mnli_val[\"genre\"]==\"slate\"]\n",
    "## travel\n",
    "mnli_train_travel = mnli_train[mnli_train[\"genre\"]==\"travel\"]\n",
    "mnli_val_travel = mnli_val[mnli_val[\"genre\"]==\"travel\"]\n",
    "## government\n",
    "mnli_train_government = mnli_train[mnli_train[\"genre\"]==\"government\"]\n",
    "mnli_val_government = mnli_val[mnli_val[\"genre\"]==\"government\"]\n",
    "## fiction\n",
    "mnli_train_fiction = mnli_train[mnli_train[\"genre\"]==\"fiction\"]\n",
    "mnli_val_fiction = mnli_val[mnli_val[\"genre\"]==\"fiction\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## idx = token2id_wiki\n",
    "\n",
    "def token2index_dataset(tokens_data,\n",
    "                        idx_dict=None):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        ## get index list for each sentence.\n",
    "        index_list = [idx_dict[token] if token in \\\n",
    "                      idx_dict else idx_dict[\"unk\"] for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embedding_weights(vectors, \n",
    "                           token2id, \n",
    "                           id2token, \n",
    "                           embedding_size):\n",
    "    \n",
    "    weights = np.zeros((len(token2id), \n",
    "                        embedding_size))\n",
    "\n",
    "    for idx in range(2, len(id2token)):\n",
    "        token = id2token[idx]\n",
    "        weights[idx] = np.array(token2id_wiki[token])\n",
    "        \n",
    "    weights[1] = np.random.randn(embedding_size)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(train_data, \n",
    "                     vocab_size, \n",
    "                     vocab_embeddings):\n",
    "    all_tokens = []\n",
    "    for row in (train_data['sentence1'] + train_data['sentence2']).iteritems():\n",
    "        all_tokens += row[1]\n",
    "        \n",
    "    vocabulary, count = zip(*Counter(all_tokens).most_common(vocab_size))\n",
    "    vectors = wiki_embed_table\n",
    "    vocabulary = [word for word in vocabulary if word in token2id_wiki.keys()]\n",
    "\n",
    "    id2token = list(vocabulary)\n",
    "    token2id = dict(zip(vocabulary, range(2, 2+len(vocabulary))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    \n",
    "    return token2id, id2token, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, dataset, vocab_size=50000):\n",
    "#     data = prepare_data(data)\n",
    "    if dataset == \"train\":\n",
    "        token2id, id2token, vectors = build_vocabulary(data, \n",
    "                                                       vocab_size,\n",
    "                                                       wiki_embed_table)\n",
    "        return data, token2id, id2token, vectors\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_embed_table = torch.tensor(table_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PAD_IDX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-504d070952d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m snli_train, token2id, id2token, vectors = preprocess_data(snli_train,\n\u001b[1;32m      2\u001b[0m                                                           \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                                           50000)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-8c9e29e47298>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(data, dataset, vocab_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m         token2id, id2token, vectors = build_vocabulary(data, \n\u001b[1;32m      5\u001b[0m                                                        \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                                        wiki_embed_table)\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-7c2e03edfc10>\u001b[0m in \u001b[0;36mbuild_vocabulary\u001b[0;34m(train_data, vocab_size, vocab_embeddings)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtoken2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mid2token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mid2token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPAD_IDX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNK_IDX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PAD_IDX' is not defined"
     ]
    }
   ],
   "source": [
    "snli_train, token2id, id2token, vectors = preprocess_data(snli_train,\n",
    "                                                          \"train\", \n",
    "                                                          50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ I am getting the indices for Sentence 1 and Sentence 2 separately (not concatenating them at first from the beginning) since, in hyperparameter search I want to try more than one ways of interacting the hidden representations of the two sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## SNLI\n",
    "snli_train_sentence1_indices = token2index_dataset([*snli_train[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "snli_train_sentence2_indices = token2index_dataset([*snli_train[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "snli_val_sentence1_indices = token2index_dataset([*snli_val[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "snli_val_sentence2_indices = token2index_dataset([*snli_val[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "\n",
    "## MNLI\n",
    "mnli_train_sentence1_indices = token2index_dataset([*mnli_train[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_sentence2_indices = token2index_dataset([*mnli_train[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_sentence1_indices = token2index_dataset([*mnli_val[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_sentence2_indices = token2index_dataset([*mnli_val[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENRES\n",
    "\n",
    "## telephone\n",
    "mnli_train_s1_tel_ix = token2index_dataset([*mnli_train_telephone[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_tel_ix = token2index_dataset([*mnli_train_telephone[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_tel_ix = token2index_dataset([*mnli_val_telephone[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_tel_ix = token2index_dataset([*mnli_val_telephone[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "## slate\n",
    "mnli_train_s1_slate_ix = token2index_dataset([*mnli_train_slate[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_slate_ix = token2index_dataset([*mnli_train_slate[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_slate_ix = token2index_dataset([*mnli_val_slate[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_slate_ix = token2index_dataset([*mnli_val_slate[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "## travel\n",
    "mnli_train_s1_travel_ix = token2index_dataset([*mnli_train_travel[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_travel_ix = token2index_dataset([*mnli_train_travel[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_travel_ix = token2index_dataset([*mnli_val_travel[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_travel_ix = token2index_dataset([*mnli_val_travel[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "## gov\n",
    "mnli_train_s1_gov_ix = token2index_dataset([*mnli_train_government[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_gov_ix = token2index_dataset([*mnli_train_government[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_gov_ix = token2index_dataset([*mnli_val_government[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_gov_ix = token2index_dataset([*mnli_val_government[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "## fiction\n",
    "mnli_train_s1_fiction_ix = token2index_dataset([*mnli_train_fiction[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_fiction_ix = token2index_dataset([*mnli_train_fiction[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_fiction_ix = token2index_dataset([*mnli_val_fiction[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_fiction_ix = token2index_dataset([*mnli_val_fiction[\"sentence2\"]],idx_dict=token2id_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting training and validation set __labels__ (targets) for both datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SNLI\n",
    "snli_train_labels = np.array(snli_train[\"label_num\"])\n",
    "snli_val_labels = np.array(snli_val[\"label_num\"])\n",
    "\n",
    "## MNLI\n",
    "mnli_train_labels = np.array(mnli_train[\"label_num\"])\n",
    "mnli_val_labels = np.array(mnli_val[\"label_num\"])\n",
    "\n",
    "## GENRES\n",
    "\n",
    "## telephone\n",
    "mnli_train_tel_labels = np.array(mnli_train_telephone[\"label_num\"])\n",
    "mnli_val_tel_labels = np.array(mnli_val_telephone[\"label_num\"])\n",
    "## slate\n",
    "mnli_train_slate_labels = np.array(mnli_train_slate[\"label_num\"])\n",
    "mnli_val_slate_labels = np.array(mnli_val_slate[\"label_num\"])\n",
    "## travel\n",
    "mnli_train_travel_labels = np.array(mnli_train_travel[\"label_num\"])\n",
    "mnli_val_travel_labels = np.array(mnli_val_travel[\"label_num\"])\n",
    "## gov\n",
    "mnli_train_gov_labels = np.array(mnli_train_government[\"label_num\"])\n",
    "mnli_val_gov_labels = np.array(mnli_val_government[\"label_num\"])\n",
    "## fiction\n",
    "mnli_train_fiction_labels = np.array(mnli_train_fiction[\"label_num\"])\n",
    "mnli_val_fiction_labels = np.array(mnli_val_fiction[\"label_num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get pretrained word embeddings from the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SNLI Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 100\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, token2id=token2id_wiki, max_sentence_length=MAX_SENTENCE_LENGTH):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.sentence1, self.sentence2, self.labels = data['sentence1'].values, \\\n",
    "                                                      data['sentence2'].values, data['label_num'].values\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.token2id = token2id_wiki\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, row):\n",
    "        label = self.labels[row]\n",
    "        sentence1_word_idx, sentence2_word_idx = [], []\n",
    "        sentence1_mask, sentence2_mask = [], []\n",
    "        \n",
    "        for word in self.sentence1[row][:MAX_SENTENCE_LENGTH]:\n",
    "            if word in self.token2id.keys():\n",
    "                sentence1_word_idx.append(self.token2id[word])\n",
    "                sentence1_mask.append(0)\n",
    "            else:\n",
    "                sentence1_word_idx.append(UNK_IDX)\n",
    "                sentence1_mask.append(1)\n",
    "                \n",
    "        for word in self.sentence2[row][:MAX_SENTENCE_LENGTH]:\n",
    "            if word in self.token2id.keys():\n",
    "                sentence2_word_idx.append(self.token2id[word])\n",
    "                sentence2_mask.append(0)\n",
    "            else:\n",
    "                sentence2_word_idx.append(UNK_IDX)\n",
    "                sentence2_mask.append(1)\n",
    "        \n",
    "        sentence1_list = [sentence1_word_idx, sentence1_mask, len(sentence1_word_idx)]\n",
    "        sentence2_list = [sentence2_word_idx, sentence2_mask, len(sentence2_word_idx)]\n",
    "        \n",
    "        return sentence1_list + sentence2_list + [label]\n",
    "\n",
    "\n",
    "    \n",
    "def snli_func(batch, max_sent_length):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sentence1_data, sentence2_data = [], []\n",
    "    sentence1_mask, sentence2_mask = [], []\n",
    "    s1_lengths, s2_lengths = [], []\n",
    "    labels = []\n",
    "\n",
    "    for datum in batch:\n",
    "        s1_lengths.append(datum[2])\n",
    "        s2_lengths.append(datum[5])\n",
    "        labels.append(datum[6])\n",
    "        \n",
    "        # PAD\n",
    "        sentence1_data_padded = np.pad(np.array(datum[0]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        sentence1_data.append(sentence1_data_padded)\n",
    "        \n",
    "        sentence1_mask_padded = np.pad(np.array(datum[1]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        sentence1_mask.append(sentence1_mask_padded)\n",
    "        \n",
    "        sentence2_data_padded = np.pad(np.array(datum[3]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        sentence2_data.append(sentence2_data_padded)\n",
    "        \n",
    "        sentence2_mask_padded = np.pad(np.array(datum[4]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])),\n",
    "                               mode=\"constant\", constant_values=0)\n",
    "        sentence2_mask.append(sentence2_mask_padded)\n",
    "        \n",
    "    ind_dec_order = np.argsort(s1_lengths)[::-1]\n",
    "    sentence1_data = np.array(sentence1_data)[ind_dec_order]\n",
    "    sentence2_data = np.array(sentence2_data)[ind_dec_order]\n",
    "    sentence1_mask = np.array(sentence1_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    sentence2_mask = np.array(sentence2_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    s1_lengths = np.array(s1_lengths)[ind_dec_order]\n",
    "    s2_lengths = np.array(s2_lengths)[ind_dec_order]\n",
    "    \n",
    "    labels = np.array(labels)[ind_dec_order]\n",
    "    \n",
    "    s1_list = [torch.from_numpy(sentence1_data), \n",
    "               torch.from_numpy(sentence1_mask).float(), s1_lengths]\n",
    "    s2_list = [torch.from_numpy(sentence2_data), \n",
    "               torch.from_numpy(sentence2_mask).float(), s2_lengths]\n",
    "        \n",
    "    return s1_list + s2_list + [torch.from_numpy(labels)]\n",
    "\n",
    "\n",
    "snli_train_dataset = SNLIDataset(snli_train, \n",
    "                                 max_sentence_length=MAX_SENTENCE_LENGTH)\n",
    "snli_train_loader = torch.utils.data.DataLoader(dataset=snli_train_dataset,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: snli_func(x, max_sentence_length),\n",
    "                               shuffle=False)\n",
    "\n",
    "snli_val_dataset = SNLIDataset(snli_val, \n",
    "                               max_sentence_length=MAX_SENTENCE_LENGTH)\n",
    "snli_val_loader = torch.utils.data.DataLoader(dataset=snli_val_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: snli_func(x, max_sentence_length),\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MNLI Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 100\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class MNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, token2id=token2id_wiki, max_sentence_length=MAX_SENTENCE_LENGTH):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.sentence1, self.sentence2, self.labels = data['sentence1'].values, \\\n",
    "                                                      data['sentence2'].values, data['label_num'].values\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.token2id = token2id_wiki\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, row):\n",
    "        label = self.labels[row]\n",
    "        sentence1_word_idx, sentence2_word_idx = [], []\n",
    "        sentence1_mask, sentence2_mask = [], []\n",
    "        \n",
    "        for word in self.sentence1[row][:MAX_SENTENCE_LENGTH]:\n",
    "            if word in self.token2id.keys():\n",
    "                sentence1_word_idx.append(self.token2id[word])\n",
    "                sentence1_mask.append(0)\n",
    "            else:\n",
    "                sentence1_word_idx.append(UNK_IDX)\n",
    "                sentence1_mask.append(1)\n",
    "                \n",
    "        for word in self.sentence2[row][:MAX_SENTENCE_LENGTH]:\n",
    "            if word in self.token2id.keys():\n",
    "                sentence2_word_idx.append(self.token2id[word])\n",
    "                sentence2_mask.append(0)\n",
    "            else:\n",
    "                sentence2_word_idx.append(UNK_IDX)\n",
    "                sentence2_mask.append(1)\n",
    "        \n",
    "        sentence1_list = [sentence1_word_idx, sentence1_mask, len(sentence1_word_idx)]\n",
    "        sentence2_list = [sentence2_word_idx, sentence2_mask, len(sentence2_word_idx)]\n",
    "        \n",
    "        return sentence1_list + sentence2_list + [label]\n",
    "\n",
    "\n",
    "    \n",
    "def mnli_func(batch, max_sent_length):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sentence1_data, sentence2_data = [], []\n",
    "    sentence1_mask, sentence2_mask = [], []\n",
    "    s1_lengths, s2_lengths = [], []\n",
    "    labels = []\n",
    "\n",
    "    for datum in batch:\n",
    "        s1_lengths.append(datum[2])\n",
    "        s2_lengths.append(datum[5])\n",
    "        labels.append(datum[6])\n",
    "        \n",
    "        # PAD\n",
    "        sentence1_data_padded = np.pad(np.array(datum[0]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        sentence1_data.append(sentence1_data_padded)\n",
    "        \n",
    "        sentence1_mask_padded = np.pad(np.array(datum[1]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        sentence1_mask.append(sentence1_mask_padded)\n",
    "        \n",
    "        sentence2_data_padded = np.pad(np.array(datum[3]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        sentence2_data.append(sentence2_data_padded)\n",
    "        \n",
    "        sentence2_mask_padded = np.pad(np.array(datum[4]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])),\n",
    "                               mode=\"constant\", constant_values=0)\n",
    "        sentence2_mask.append(sentence2_mask_padded)\n",
    "        \n",
    "    ind_dec_order = np.argsort(s1_lengths)[::-1]\n",
    "    sentence1_data = np.array(sentence1_data)[ind_dec_order]\n",
    "    sentence2_data = np.array(sentence2_data)[ind_dec_order]\n",
    "    sentence1_mask = np.array(sentence1_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    sentence2_mask = np.array(sentence2_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    s1_lengths = np.array(s1_lengths)[ind_dec_order]\n",
    "    s2_lengths = np.array(s2_lengths)[ind_dec_order]\n",
    "    \n",
    "    labels = np.array(labels)[ind_dec_order]\n",
    "    \n",
    "    s1_list = [torch.from_numpy(sentence1_data), \n",
    "               torch.from_numpy(sentence1_mask).float(), s1_lengths]\n",
    "    s2_list = [torch.from_numpy(sentence2_data), \n",
    "               torch.from_numpy(sentence2_mask).float(), s2_lengths]\n",
    "        \n",
    "    return s1_list + s2_list + [torch.from_numpy(labels)]\n",
    "\n",
    "\n",
    "mnli_train_dataset = MNLIDataset(mnli_train, \n",
    "                                 max_sentence_length=MAX_SENTENCE_LENGTH)\n",
    "mnli_train_loader = torch.utils.data.DataLoader(dataset=mnli_train_dataset,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: mnli_func(x, max_sentence_length),\n",
    "                               shuffle=False)\n",
    "\n",
    "mnli_val_dataset = SNLIDataset(snli_val, \n",
    "                               max_sentence_length=MAX_SENTENCE_LENGTH)\n",
    "mnli_val_loader = torch.utils.data.DataLoader(dataset=mnli_val_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: mnli_func(x, max_sentence_length),\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model\n",
    "\n",
    "The model is trained on SNLI training set. The best model is chosen using SNLI validation set, then the best model is evaluated on each genre in MultiNLI validation set. \n",
    "\n",
    "We will use an encoder (either a CNN or an RNN) to map each string of text (hypothesis and premise) to a fixed-dimension vector representation. \n",
    "\n",
    "- We will interact the two hidden representations and output a __3-class softmax__. \n",
    "\n",
    "- To keep things simple, we will simply __concatenate__ the two representations, and feed them through a network of __2 fully-connected layers__. \n",
    "\n",
    "- For the encoder, we want the following:\n",
    "\n",
    "### Part 2.1: Bidirectional_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 8\n",
    "# batch_size = 16\n",
    "# epochs = 20\n",
    "no_cuda = False\n",
    "# log_interval = 1\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "# cuda = False\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_embed_table = torch.tensor(table_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_train_length = max([snli_train_dataset_s1[x][1] for x in range(len(snli_train_dataset_s1))])\n",
    "\n",
    "class biGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 embedding_weights,\n",
    "                 percent_dropout,\n",
    "                 vocab_size=wiki_embed_table.size(0),\n",
    "                 interaction_type=\"concat\",\n",
    "                 num_layers=1,\n",
    "                 input_size=300):\n",
    "\n",
    "        super(biGRU, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        \n",
    "        ## use pretrained word embeddings\n",
    "        wiki_embed_table = torch.FloatTensor(table_lookup)\n",
    "        embedding = nn.Embedding.from_pretrained(wiki_embed_table)\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.interaction = interaction_type\n",
    "        self.dropout = percent_dropout\n",
    "        \n",
    "        self.GRU = nn.GRU(300, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.drop_out = nn.Dropout(self.dropout)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.randn(2*self.num_layers, ## 2 for bidirectional\n",
    "                             batch_size, self.hidden_size).to(device)\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, sentence, \n",
    "                mask, lengths):\n",
    "        sort_original = sorted(range(len(lengths)), \n",
    "                             key=lambda sentence: -lengths[sentence])\n",
    "        unsort_to_original = sorted(range(len(lengths)), \n",
    "                             key=lambda sentence: sort_original[sentence])\n",
    "        \n",
    "        sentence = sentence[sort_original]\n",
    "        _mask = mask[sort_original]\n",
    "        lengths = lengths[sort_original]\n",
    "        \n",
    "        batch_size, seq_len = sentence.size()\n",
    "        \n",
    "        # init hidden\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        embeds = self.embedding(sentence)\n",
    "        embeds = mask*embeds + (1-_mask)*embeds.clone().detach()\n",
    "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, lengths, \n",
    "                                                         batch_first=True)\n",
    "        \n",
    "        gru_out, self.hidden = self.GRU(embeds, self.hidden)\n",
    "        \n",
    "        # undo packing\n",
    "        gru_out, _ = torch.nn.utils.rnn.pad_packed_sequence(gru_out, \n",
    "                                                            batch_first=True)\n",
    "        \n",
    "        gru_out = gru_out.view(batch_size, -1, 2, self.hidden_size)\n",
    "        gru_out = torch.sum(gru_out, dim=1)\n",
    "        gru_out = torch.cat([gru_out[:,i,:] for i in range(2)], dim=1)\n",
    "        gru_out = gru_out[unsort_to_original] ## back to original indices\n",
    "        \n",
    "        return gru_out\n",
    "    \n",
    "    \n",
    "class Linear_Layers(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 hidden_size_2,\n",
    "                 percent_dropout,\n",
    "                 interaction_type=\"concat\",\n",
    "                 classes=3,\n",
    "                 input_size=300):\n",
    "        \n",
    "        super(Linear_Layers, self).__init__()\n",
    "\n",
    "        self.interaction = interaction_type\n",
    "        self.num_classes = classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "        self.percent_dropout = percent_dropout\n",
    "        self.num_classes = classes\n",
    "\n",
    "        if self.interaction == \"concat\":\n",
    "            ## 2 x 2 for bidirectional and concatenation\n",
    "            self.fc1 = nn.Linear(4*self.hidden_size, self.hidden_size_2)\n",
    "        else:\n",
    "            ## 2 for bidirectional for multiplication and subtraction\n",
    "            self.fc1 = nn.Linear(2*self.hidden_size, self.hidden_size_2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.hidden_size_2, self.num_classes)\n",
    "        self.ReLU = nn.ReLU(inplace=True)\n",
    "        self.drop_out = nn.Dropout(self.percent_dropout)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "                nn.init.uniform_(module.bias)\n",
    "\n",
    "    def forward(self, gru_out_1, gru_out_2):\n",
    "        \n",
    "        if self.interaction == \"concat\":\n",
    "            hidden = torch.cat([gru_out_1, gru_out_2], dim=1)\n",
    "        elif self.interaction == \"mul\":\n",
    "            ## elem-wise multiplication *\n",
    "            hidden = gru_out_1*gru_out_2\n",
    "        elif self.interaction == \"subtract\":\n",
    "            ## subtraction\n",
    "            hidden = gru_out_1-gru_out_2\n",
    "        \n",
    "        hidden = hidden.view(hidden.size(0),-1) \n",
    "        \n",
    "        hidden_out = self.fc1(hidden)\n",
    "        hidden_out = self.drop_out(self.ReLU(hidden_out))\n",
    "        out = self.fc2(hidden_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1562.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(snli_train)/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(RNN, \n",
    "          Linear_Classifier, \n",
    "          DataLoader, \n",
    "          criterion, \n",
    "          optimizer, \n",
    "          epoch):\n",
    "    \n",
    "    RNN.train()\n",
    "    Linear_Classifier.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (sentence1, s1_original, sentence1_lengths, \n",
    "                    sentence2, s2_original, sentence2_lengths, labels)\\\n",
    "    in enumerate(DataLoader):\n",
    "            \n",
    "        sentence1, s1_original = sentence1.to(device), s1_original.to(device),  \n",
    "        sentence2, s2_original = sentence2.to(device), s2_original.to(device),\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        RNN.train()\n",
    "        Linear_Classifier.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output_s1 = RNN(sentence1, \n",
    "                              s1_original, \n",
    "                              sentence1_lengths)\n",
    "        output_s2 = RNN(sentence2, \n",
    "                              s2_original, \n",
    "                              sentence2_lengths)\n",
    "        \n",
    "        out = Linear_Classifier(output_s1, output_s2)\n",
    "        \n",
    "        loss = criterion(out, labels)\n",
    "        loss.cuda().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(sentence1) / len(DataLoader.dataset)\n",
    "        \n",
    "        if (batch_idx+1) % (len(DataLoader.dataset)//(20*labels.shape[0])) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx+1) * labels.shape[0], len(DataLoader.dataset),\n",
    "                100. * (batch_idx+1) / len(DataLoader), loss.item()))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def test(RNN, \n",
    "         Linear_Classifier, \n",
    "         DataLoader, \n",
    "         criterion):\n",
    "\n",
    "    RNN.eval()\n",
    "    Linear_Classifier.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    label_list = []\n",
    "    output_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (sentence1, s1_original, sentence1_lengths, \n",
    "                    sentence2, s2_original, sentence2_lengths, labels)\\\n",
    "                    in enumerate(DataLoader):\n",
    "\n",
    "            sentence1, s1_original = sentence1.to(device), s1_original.to(device),  \n",
    "            sentence2, s2_original = sentence2.to(device), s2_original.to(device),\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            output_s1 = RNN(sentence1, \n",
    "                                  s1_original, \n",
    "                                  sentence1_lengths)\n",
    "            # Reverse\n",
    "            output_s2 = RNN(sentence2, \n",
    "                                  s2_original, \n",
    "                                  sentence2_lengths)\n",
    "            \n",
    "            out = Linear_Classifier(output_s1, output_s2)\n",
    "        \n",
    "            loss = criterion(out, labels)\n",
    "\n",
    "            test_loss += loss.item()/len(DataLoader.dataset)\n",
    "\n",
    "            output_list.append(out)\n",
    "            label_list.append(labels)\n",
    "            \n",
    "            print (\"outputs= \"+str(torch.cat(output_list, dim=0)))\n",
    "            print (\"labels= \"+str(torch.cat(label_list, dim=0)))\n",
    "            \n",
    "    return test_loss, torch.cat(output_list, dim=0), torch.cat(label_list, dim=0)\n",
    "\n",
    "def accuracy(RNN, \n",
    "             Linear_Classifier, \n",
    "             DataLoader, \n",
    "             criterion):\n",
    "    \n",
    "    _, predicted, true_labels = test(RNN = RNN,\n",
    "                              Linear_Classifier = Linear_Classifier,\n",
    "                              DataLoader = DataLoader,\n",
    "                              criterion = criterion)\n",
    "\n",
    "    predicted = predicted.max(1)[1]\n",
    "    return 100 * predicted.eq(true_labels.data.view_as(predicted)).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "num_classes = 3\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "gru_hidden_size = 256\n",
    "classifier_hidden_size = 512\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "lr = 3e-4\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Search\n",
    "\n",
    "The hyperparameters included in the hyperparameter search space are;\n",
    "\n",
    "- The size of the hidden dimension of the CNN,\n",
    "- The kernel size of the CNN,\n",
    "- Experiment with different ways of interacting the two encoded sentences (concatenation, element-wise multiplication, outer multiplication etc)\n",
    "- Regularization (e.g. weight decay, dropout).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "params = [[1e-3, 1e-2],    ## learning rate\n",
    "          [512], ## hidden_size\n",
    "          [64, 128],     ## hidden_size_2\n",
    "          [0.1],         ## dropout\n",
    "          [\"mul\",\"concat\",\"subtract\"]]  ## interaction type\n",
    "\n",
    "params = [*itertools.product(*params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.001, 512, 64, 0.1, 'mul'),\n",
       " (0.001, 512, 64, 0.1, 'concat'),\n",
       " (0.001, 512, 64, 0.1, 'subtract'),\n",
       " (0.001, 512, 128, 0.1, 'mul'),\n",
       " (0.001, 512, 128, 0.1, 'concat'),\n",
       " (0.001, 512, 128, 0.1, 'subtract'),\n",
       " (0.01, 512, 64, 0.1, 'mul'),\n",
       " (0.01, 512, 64, 0.1, 'concat'),\n",
       " (0.01, 512, 64, 0.1, 'subtract'),\n",
       " (0.01, 512, 128, 0.1, 'mul'),\n",
       " (0.01, 512, 128, 0.1, 'concat'),\n",
       " (0.01, 512, 128, 0.1, 'subtract')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999994, 300)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_lookup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Set: (0.001, 512, 64, 0.1, 'mul')\n",
      "epoch = 0\n",
      "Train Epoch: 0 [4992/100000 (5%)]\tLoss: 1.393512\n",
      "Train Epoch: 0 [9984/100000 (10%)]\tLoss: 1.124658\n",
      "Train Epoch: 0 [14976/100000 (15%)]\tLoss: 1.136117\n",
      "Train Epoch: 0 [19968/100000 (20%)]\tLoss: 1.085578\n",
      "Train Epoch: 0 [24960/100000 (25%)]\tLoss: 1.088257\n",
      "Train Epoch: 0 [29952/100000 (30%)]\tLoss: 1.136143\n",
      "Train Epoch: 0 [34944/100000 (35%)]\tLoss: 1.087647\n",
      "Train Epoch: 0 [39936/100000 (40%)]\tLoss: 1.087210\n",
      "Train Epoch: 0 [44928/100000 (45%)]\tLoss: 1.126026\n",
      "Train Epoch: 0 [49920/100000 (50%)]\tLoss: 1.089368\n",
      "Train Epoch: 0 [54912/100000 (55%)]\tLoss: 1.089418\n",
      "Train Epoch: 0 [59904/100000 (60%)]\tLoss: 1.099251\n",
      "Train Epoch: 0 [64896/100000 (65%)]\tLoss: 1.056493\n",
      "Train Epoch: 0 [69888/100000 (70%)]\tLoss: 1.083852\n",
      "Train Epoch: 0 [74880/100000 (75%)]\tLoss: 1.096810\n",
      "Train Epoch: 0 [79872/100000 (80%)]\tLoss: 1.082849\n",
      "Train Epoch: 0 [84864/100000 (85%)]\tLoss: 1.069817\n",
      "Train Epoch: 0 [89856/100000 (90%)]\tLoss: 1.068043\n",
      "Train Epoch: 0 [94848/100000 (95%)]\tLoss: 1.078911\n",
      "Train Epoch: 0 [99840/100000 (100%)]\tLoss: 1.059656\n",
      "epoch = 1\n",
      "Train Epoch: 1 [4992/100000 (5%)]\tLoss: 1.095101\n",
      "Train Epoch: 1 [9984/100000 (10%)]\tLoss: 1.062310\n",
      "Train Epoch: 1 [14976/100000 (15%)]\tLoss: 1.093668\n",
      "Train Epoch: 1 [19968/100000 (20%)]\tLoss: 1.043761\n",
      "Train Epoch: 1 [24960/100000 (25%)]\tLoss: 1.025347\n",
      "Train Epoch: 1 [29952/100000 (30%)]\tLoss: 1.057287\n",
      "Train Epoch: 1 [34944/100000 (35%)]\tLoss: 1.058774\n",
      "Train Epoch: 1 [39936/100000 (40%)]\tLoss: 1.058233\n",
      "Train Epoch: 1 [44928/100000 (45%)]\tLoss: 1.043946\n",
      "Train Epoch: 1 [49920/100000 (50%)]\tLoss: 1.035963\n",
      "Train Epoch: 1 [54912/100000 (55%)]\tLoss: 1.047727\n",
      "Train Epoch: 1 [59904/100000 (60%)]\tLoss: 1.092080\n",
      "Train Epoch: 1 [64896/100000 (65%)]\tLoss: 1.015884\n",
      "Train Epoch: 1 [69888/100000 (70%)]\tLoss: 1.086953\n",
      "Train Epoch: 1 [74880/100000 (75%)]\tLoss: 1.030135\n",
      "Train Epoch: 1 [79872/100000 (80%)]\tLoss: 1.042275\n",
      "Train Epoch: 1 [84864/100000 (85%)]\tLoss: 1.038527\n",
      "Train Epoch: 1 [89856/100000 (90%)]\tLoss: 0.996702\n",
      "Train Epoch: 1 [94848/100000 (95%)]\tLoss: 1.112915\n",
      "Train Epoch: 1 [99840/100000 (100%)]\tLoss: 1.044509\n",
      "epoch = 2\n",
      "Train Epoch: 2 [4992/100000 (5%)]\tLoss: 1.063042\n",
      "Train Epoch: 2 [9984/100000 (10%)]\tLoss: 0.995592\n",
      "Train Epoch: 2 [14976/100000 (15%)]\tLoss: 1.034722\n",
      "Train Epoch: 2 [19968/100000 (20%)]\tLoss: 1.065187\n",
      "Train Epoch: 2 [24960/100000 (25%)]\tLoss: 1.026707\n",
      "Train Epoch: 2 [29952/100000 (30%)]\tLoss: 0.990280\n",
      "Train Epoch: 2 [34944/100000 (35%)]\tLoss: 0.966372\n",
      "Train Epoch: 2 [39936/100000 (40%)]\tLoss: 1.018057\n",
      "Train Epoch: 2 [44928/100000 (45%)]\tLoss: 0.992820\n",
      "Train Epoch: 2 [49920/100000 (50%)]\tLoss: 0.961368\n",
      "Train Epoch: 2 [54912/100000 (55%)]\tLoss: 0.915655\n",
      "Train Epoch: 2 [59904/100000 (60%)]\tLoss: 1.107718\n",
      "Train Epoch: 2 [64896/100000 (65%)]\tLoss: 0.998035\n",
      "Train Epoch: 2 [69888/100000 (70%)]\tLoss: 1.071863\n",
      "Train Epoch: 2 [74880/100000 (75%)]\tLoss: 1.045190\n",
      "Train Epoch: 2 [79872/100000 (80%)]\tLoss: 1.029274\n",
      "Train Epoch: 2 [84864/100000 (85%)]\tLoss: 1.055662\n",
      "Train Epoch: 2 [89856/100000 (90%)]\tLoss: 1.022281\n",
      "Train Epoch: 2 [94848/100000 (95%)]\tLoss: 0.996890\n",
      "Train Epoch: 2 [99840/100000 (100%)]\tLoss: 1.039080\n",
      "epoch = 3\n",
      "Train Epoch: 3 [4992/100000 (5%)]\tLoss: 1.061338\n",
      "Train Epoch: 3 [9984/100000 (10%)]\tLoss: 0.872987\n",
      "Train Epoch: 3 [14976/100000 (15%)]\tLoss: 0.973530\n",
      "Train Epoch: 3 [19968/100000 (20%)]\tLoss: 1.038430\n",
      "Train Epoch: 3 [24960/100000 (25%)]\tLoss: 0.998199\n",
      "Train Epoch: 3 [29952/100000 (30%)]\tLoss: 0.901291\n",
      "Train Epoch: 3 [34944/100000 (35%)]\tLoss: 0.979041\n",
      "Train Epoch: 3 [39936/100000 (40%)]\tLoss: 0.903399\n",
      "Train Epoch: 3 [44928/100000 (45%)]\tLoss: 0.960606\n",
      "Train Epoch: 3 [49920/100000 (50%)]\tLoss: 0.932337\n",
      "Train Epoch: 3 [54912/100000 (55%)]\tLoss: 0.863356\n",
      "Train Epoch: 3 [59904/100000 (60%)]\tLoss: 1.056003\n",
      "Train Epoch: 3 [64896/100000 (65%)]\tLoss: 0.993923\n",
      "Train Epoch: 3 [69888/100000 (70%)]\tLoss: 1.005051\n",
      "Train Epoch: 3 [74880/100000 (75%)]\tLoss: 1.047075\n",
      "Train Epoch: 3 [79872/100000 (80%)]\tLoss: 0.880839\n",
      "Train Epoch: 3 [84864/100000 (85%)]\tLoss: 1.062726\n",
      "Train Epoch: 3 [89856/100000 (90%)]\tLoss: 0.926617\n",
      "Train Epoch: 3 [94848/100000 (95%)]\tLoss: 0.910710\n",
      "Train Epoch: 3 [99840/100000 (100%)]\tLoss: 1.015764\n",
      "epoch = 4\n",
      "Train Epoch: 4 [4992/100000 (5%)]\tLoss: 0.977727\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.870570\n",
      "Train Epoch: 4 [14976/100000 (15%)]\tLoss: 0.987958\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.917000\n",
      "Train Epoch: 4 [24960/100000 (25%)]\tLoss: 0.937077\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.909170\n",
      "Train Epoch: 4 [34944/100000 (35%)]\tLoss: 0.916217\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.818929\n",
      "Train Epoch: 4 [44928/100000 (45%)]\tLoss: 0.883515\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.914625\n",
      "Train Epoch: 4 [54912/100000 (55%)]\tLoss: 0.877037\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 1.003280\n",
      "Train Epoch: 4 [64896/100000 (65%)]\tLoss: 1.014842\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.938637\n",
      "Train Epoch: 4 [74880/100000 (75%)]\tLoss: 0.933059\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.888424\n",
      "Train Epoch: 4 [84864/100000 (85%)]\tLoss: 0.968212\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.818068\n",
      "Train Epoch: 4 [94848/100000 (95%)]\tLoss: 0.865230\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 1.025765\n",
      "Parameter Set: (0.001, 512, 64, 0.1, 'concat')\n",
      "epoch = 0\n",
      "Train Epoch: 0 [4992/100000 (5%)]\tLoss: 1.078156\n",
      "Train Epoch: 0 [9984/100000 (10%)]\tLoss: 1.116044\n",
      "Train Epoch: 0 [14976/100000 (15%)]\tLoss: 1.103170\n",
      "Train Epoch: 0 [19968/100000 (20%)]\tLoss: 1.096484\n",
      "Train Epoch: 0 [24960/100000 (25%)]\tLoss: 1.085200\n",
      "Train Epoch: 0 [29952/100000 (30%)]\tLoss: 1.055934\n",
      "Train Epoch: 0 [34944/100000 (35%)]\tLoss: 1.068815\n",
      "Train Epoch: 0 [39936/100000 (40%)]\tLoss: 1.045515\n",
      "Train Epoch: 0 [44928/100000 (45%)]\tLoss: 1.039914\n",
      "Train Epoch: 0 [49920/100000 (50%)]\tLoss: 1.057461\n",
      "Train Epoch: 0 [54912/100000 (55%)]\tLoss: 1.060472\n",
      "Train Epoch: 0 [59904/100000 (60%)]\tLoss: 1.066296\n",
      "Train Epoch: 0 [64896/100000 (65%)]\tLoss: 1.046406\n",
      "Train Epoch: 0 [69888/100000 (70%)]\tLoss: 1.057694\n",
      "Train Epoch: 0 [74880/100000 (75%)]\tLoss: 0.997879\n",
      "Train Epoch: 0 [79872/100000 (80%)]\tLoss: 1.073806\n",
      "Train Epoch: 0 [84864/100000 (85%)]\tLoss: 0.990675\n",
      "Train Epoch: 0 [89856/100000 (90%)]\tLoss: 0.961949\n",
      "Train Epoch: 0 [94848/100000 (95%)]\tLoss: 1.038982\n",
      "Train Epoch: 0 [99840/100000 (100%)]\tLoss: 1.008719\n",
      "epoch = 1\n",
      "Train Epoch: 1 [4992/100000 (5%)]\tLoss: 1.063742\n",
      "Train Epoch: 1 [9984/100000 (10%)]\tLoss: 1.068781\n",
      "Train Epoch: 1 [14976/100000 (15%)]\tLoss: 0.992098\n",
      "Train Epoch: 1 [19968/100000 (20%)]\tLoss: 1.011442\n",
      "Train Epoch: 1 [24960/100000 (25%)]\tLoss: 0.958112\n",
      "Train Epoch: 1 [29952/100000 (30%)]\tLoss: 0.923600\n",
      "Train Epoch: 1 [34944/100000 (35%)]\tLoss: 0.905705\n",
      "Train Epoch: 1 [39936/100000 (40%)]\tLoss: 0.908737\n",
      "Train Epoch: 1 [44928/100000 (45%)]\tLoss: 0.919316\n",
      "Train Epoch: 1 [49920/100000 (50%)]\tLoss: 0.984198\n",
      "Train Epoch: 1 [54912/100000 (55%)]\tLoss: 0.916102\n",
      "Train Epoch: 1 [59904/100000 (60%)]\tLoss: 1.032625\n",
      "Train Epoch: 1 [64896/100000 (65%)]\tLoss: 0.970779\n",
      "Train Epoch: 1 [69888/100000 (70%)]\tLoss: 0.957253\n",
      "Train Epoch: 1 [74880/100000 (75%)]\tLoss: 0.887464\n",
      "Train Epoch: 1 [79872/100000 (80%)]\tLoss: 0.904460\n",
      "Train Epoch: 1 [84864/100000 (85%)]\tLoss: 0.964920\n",
      "Train Epoch: 1 [89856/100000 (90%)]\tLoss: 0.847183\n",
      "Train Epoch: 1 [94848/100000 (95%)]\tLoss: 0.958151\n",
      "Train Epoch: 1 [99840/100000 (100%)]\tLoss: 0.978847\n",
      "epoch = 2\n",
      "Train Epoch: 2 [4992/100000 (5%)]\tLoss: 0.942506\n",
      "Train Epoch: 2 [9984/100000 (10%)]\tLoss: 0.843193\n",
      "Train Epoch: 2 [14976/100000 (15%)]\tLoss: 0.892655\n",
      "Train Epoch: 2 [19968/100000 (20%)]\tLoss: 0.869110\n",
      "Train Epoch: 2 [24960/100000 (25%)]\tLoss: 0.852750\n",
      "Train Epoch: 2 [29952/100000 (30%)]\tLoss: 0.828651\n",
      "Train Epoch: 2 [34944/100000 (35%)]\tLoss: 0.879633\n",
      "Train Epoch: 2 [39936/100000 (40%)]\tLoss: 0.904427\n",
      "Train Epoch: 2 [44928/100000 (45%)]\tLoss: 0.920424\n",
      "Train Epoch: 2 [49920/100000 (50%)]\tLoss: 0.930173\n",
      "Train Epoch: 2 [54912/100000 (55%)]\tLoss: 0.848572\n",
      "Train Epoch: 2 [59904/100000 (60%)]\tLoss: 1.031651\n",
      "Train Epoch: 2 [64896/100000 (65%)]\tLoss: 0.912229\n",
      "Train Epoch: 2 [69888/100000 (70%)]\tLoss: 0.975994\n",
      "Train Epoch: 2 [74880/100000 (75%)]\tLoss: 0.859731\n",
      "Train Epoch: 2 [79872/100000 (80%)]\tLoss: 0.901256\n",
      "Train Epoch: 2 [84864/100000 (85%)]\tLoss: 0.934705\n",
      "Train Epoch: 2 [89856/100000 (90%)]\tLoss: 0.779646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [94848/100000 (95%)]\tLoss: 0.825459\n",
      "Train Epoch: 2 [99840/100000 (100%)]\tLoss: 0.923804\n",
      "epoch = 3\n",
      "Train Epoch: 3 [4992/100000 (5%)]\tLoss: 0.864688\n",
      "Train Epoch: 3 [9984/100000 (10%)]\tLoss: 0.899063\n",
      "Train Epoch: 3 [14976/100000 (15%)]\tLoss: 0.845985\n",
      "Train Epoch: 3 [19968/100000 (20%)]\tLoss: 0.859547\n",
      "Train Epoch: 3 [24960/100000 (25%)]\tLoss: 0.896304\n",
      "Train Epoch: 3 [29952/100000 (30%)]\tLoss: 0.760724\n",
      "Train Epoch: 3 [34944/100000 (35%)]\tLoss: 0.847752\n",
      "Train Epoch: 3 [39936/100000 (40%)]\tLoss: 0.823596\n",
      "Train Epoch: 3 [44928/100000 (45%)]\tLoss: 0.842493\n",
      "Train Epoch: 3 [49920/100000 (50%)]\tLoss: 0.819322\n",
      "Train Epoch: 3 [54912/100000 (55%)]\tLoss: 0.807645\n",
      "Train Epoch: 3 [59904/100000 (60%)]\tLoss: 1.064914\n",
      "Train Epoch: 3 [64896/100000 (65%)]\tLoss: 0.889742\n",
      "Train Epoch: 3 [69888/100000 (70%)]\tLoss: 0.856749\n",
      "Train Epoch: 3 [74880/100000 (75%)]\tLoss: 0.790646\n",
      "Train Epoch: 3 [79872/100000 (80%)]\tLoss: 0.843226\n",
      "Train Epoch: 3 [84864/100000 (85%)]\tLoss: 0.866599\n",
      "Train Epoch: 3 [89856/100000 (90%)]\tLoss: 0.722024\n",
      "Train Epoch: 3 [94848/100000 (95%)]\tLoss: 0.874860\n",
      "Train Epoch: 3 [99840/100000 (100%)]\tLoss: 0.919556\n",
      "epoch = 4\n",
      "Train Epoch: 4 [4992/100000 (5%)]\tLoss: 0.847701\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.792178\n",
      "Train Epoch: 4 [14976/100000 (15%)]\tLoss: 0.811230\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.868035\n",
      "Train Epoch: 4 [24960/100000 (25%)]\tLoss: 0.876233\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.745444\n",
      "Train Epoch: 4 [34944/100000 (35%)]\tLoss: 0.857687\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.826791\n",
      "Train Epoch: 4 [44928/100000 (45%)]\tLoss: 0.866958\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.831090\n",
      "Train Epoch: 4 [54912/100000 (55%)]\tLoss: 0.777373\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 0.989570\n",
      "Train Epoch: 4 [64896/100000 (65%)]\tLoss: 0.876285\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.837454\n",
      "Train Epoch: 4 [74880/100000 (75%)]\tLoss: 0.814313\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.815502\n",
      "Train Epoch: 4 [84864/100000 (85%)]\tLoss: 0.918378\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.724087\n",
      "Train Epoch: 4 [94848/100000 (95%)]\tLoss: 0.829578\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.869493\n",
      "Parameter Set: (0.001, 512, 64, 0.1, 'subtract')\n",
      "epoch = 0\n",
      "Train Epoch: 0 [4992/100000 (5%)]\tLoss: 1.435754\n",
      "Train Epoch: 0 [9984/100000 (10%)]\tLoss: 1.504250\n",
      "Train Epoch: 0 [14976/100000 (15%)]\tLoss: 1.310710\n",
      "Train Epoch: 0 [19968/100000 (20%)]\tLoss: 1.623941\n",
      "Train Epoch: 0 [24960/100000 (25%)]\tLoss: 1.175309\n",
      "Train Epoch: 0 [29952/100000 (30%)]\tLoss: 1.464125\n",
      "Train Epoch: 0 [34944/100000 (35%)]\tLoss: 1.109660\n",
      "Train Epoch: 0 [39936/100000 (40%)]\tLoss: 1.194763\n",
      "Train Epoch: 0 [44928/100000 (45%)]\tLoss: 1.320854\n",
      "Train Epoch: 0 [49920/100000 (50%)]\tLoss: 1.132209\n",
      "Train Epoch: 0 [54912/100000 (55%)]\tLoss: 1.124283\n",
      "Train Epoch: 0 [59904/100000 (60%)]\tLoss: 1.181703\n",
      "Train Epoch: 0 [64896/100000 (65%)]\tLoss: 1.201929\n",
      "Train Epoch: 0 [69888/100000 (70%)]\tLoss: 1.152446\n",
      "Train Epoch: 0 [74880/100000 (75%)]\tLoss: 1.174470\n",
      "Train Epoch: 0 [79872/100000 (80%)]\tLoss: 1.219477\n",
      "Train Epoch: 0 [84864/100000 (85%)]\tLoss: 1.069550\n",
      "Train Epoch: 0 [89856/100000 (90%)]\tLoss: 1.014396\n",
      "Train Epoch: 0 [94848/100000 (95%)]\tLoss: 0.985277\n",
      "Train Epoch: 0 [99840/100000 (100%)]\tLoss: 1.070764\n",
      "epoch = 1\n",
      "Train Epoch: 1 [4992/100000 (5%)]\tLoss: 1.078691\n",
      "Train Epoch: 1 [9984/100000 (10%)]\tLoss: 1.058872\n",
      "Train Epoch: 1 [14976/100000 (15%)]\tLoss: 0.999392\n",
      "Train Epoch: 1 [19968/100000 (20%)]\tLoss: 1.132095\n",
      "Train Epoch: 1 [24960/100000 (25%)]\tLoss: 1.049823\n",
      "Train Epoch: 1 [29952/100000 (30%)]\tLoss: 1.115521\n",
      "Train Epoch: 1 [34944/100000 (35%)]\tLoss: 1.065806\n",
      "Train Epoch: 1 [39936/100000 (40%)]\tLoss: 1.029775\n",
      "Train Epoch: 1 [44928/100000 (45%)]\tLoss: 1.108972\n",
      "Train Epoch: 1 [49920/100000 (50%)]\tLoss: 1.056620\n",
      "Train Epoch: 1 [54912/100000 (55%)]\tLoss: 1.033293\n",
      "Train Epoch: 1 [59904/100000 (60%)]\tLoss: 1.075899\n",
      "Train Epoch: 1 [64896/100000 (65%)]\tLoss: 1.025232\n",
      "Train Epoch: 1 [69888/100000 (70%)]\tLoss: 1.004088\n",
      "Train Epoch: 1 [74880/100000 (75%)]\tLoss: 1.029728\n",
      "Train Epoch: 1 [79872/100000 (80%)]\tLoss: 1.060482\n",
      "Train Epoch: 1 [84864/100000 (85%)]\tLoss: 1.028414\n",
      "Train Epoch: 1 [89856/100000 (90%)]\tLoss: 0.880143\n",
      "Train Epoch: 1 [94848/100000 (95%)]\tLoss: 0.998714\n",
      "Train Epoch: 1 [99840/100000 (100%)]\tLoss: 1.089706\n",
      "epoch = 2\n",
      "Train Epoch: 2 [4992/100000 (5%)]\tLoss: 1.009289\n",
      "Train Epoch: 2 [9984/100000 (10%)]\tLoss: 1.007205\n",
      "Train Epoch: 2 [14976/100000 (15%)]\tLoss: 0.867959\n",
      "Train Epoch: 2 [19968/100000 (20%)]\tLoss: 1.013289\n",
      "Train Epoch: 2 [24960/100000 (25%)]\tLoss: 0.969989\n",
      "Train Epoch: 2 [29952/100000 (30%)]\tLoss: 0.950964\n",
      "Train Epoch: 2 [34944/100000 (35%)]\tLoss: 0.985213\n",
      "Train Epoch: 2 [39936/100000 (40%)]\tLoss: 0.946795\n",
      "Train Epoch: 2 [44928/100000 (45%)]\tLoss: 0.969599\n",
      "Train Epoch: 2 [49920/100000 (50%)]\tLoss: 0.954022\n",
      "Train Epoch: 2 [54912/100000 (55%)]\tLoss: 0.878425\n",
      "Train Epoch: 2 [59904/100000 (60%)]\tLoss: 1.004192\n",
      "Train Epoch: 2 [64896/100000 (65%)]\tLoss: 0.951731\n",
      "Train Epoch: 2 [69888/100000 (70%)]\tLoss: 1.042796\n",
      "Train Epoch: 2 [74880/100000 (75%)]\tLoss: 0.966370\n",
      "Train Epoch: 2 [79872/100000 (80%)]\tLoss: 1.007420\n",
      "Train Epoch: 2 [84864/100000 (85%)]\tLoss: 0.998967\n",
      "Train Epoch: 2 [89856/100000 (90%)]\tLoss: 0.807189\n",
      "Train Epoch: 2 [94848/100000 (95%)]\tLoss: 0.927971\n",
      "Train Epoch: 2 [99840/100000 (100%)]\tLoss: 0.894320\n",
      "epoch = 3\n",
      "Train Epoch: 3 [4992/100000 (5%)]\tLoss: 0.997329\n",
      "Train Epoch: 3 [9984/100000 (10%)]\tLoss: 1.000868\n",
      "Train Epoch: 3 [14976/100000 (15%)]\tLoss: 0.843172\n",
      "Train Epoch: 3 [19968/100000 (20%)]\tLoss: 0.972644\n",
      "Train Epoch: 3 [24960/100000 (25%)]\tLoss: 0.851664\n",
      "Train Epoch: 3 [29952/100000 (30%)]\tLoss: 0.958371\n",
      "Train Epoch: 3 [34944/100000 (35%)]\tLoss: 0.862521\n",
      "Train Epoch: 3 [39936/100000 (40%)]\tLoss: 0.936737\n",
      "Train Epoch: 3 [44928/100000 (45%)]\tLoss: 0.945006\n",
      "Train Epoch: 3 [49920/100000 (50%)]\tLoss: 0.979419\n",
      "Train Epoch: 3 [54912/100000 (55%)]\tLoss: 0.859796\n",
      "Train Epoch: 3 [59904/100000 (60%)]\tLoss: 1.022436\n",
      "Train Epoch: 3 [64896/100000 (65%)]\tLoss: 0.937175\n",
      "Train Epoch: 3 [69888/100000 (70%)]\tLoss: 0.901771\n",
      "Train Epoch: 3 [74880/100000 (75%)]\tLoss: 0.909438\n",
      "Train Epoch: 3 [79872/100000 (80%)]\tLoss: 0.896268\n",
      "Train Epoch: 3 [84864/100000 (85%)]\tLoss: 0.892033\n",
      "Train Epoch: 3 [89856/100000 (90%)]\tLoss: 0.792117\n",
      "Train Epoch: 3 [94848/100000 (95%)]\tLoss: 0.841327\n",
      "Train Epoch: 3 [99840/100000 (100%)]\tLoss: 0.814950\n",
      "epoch = 4\n",
      "Train Epoch: 4 [4992/100000 (5%)]\tLoss: 1.005531\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.936255\n",
      "Train Epoch: 4 [14976/100000 (15%)]\tLoss: 0.834961\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.931055\n",
      "Train Epoch: 4 [24960/100000 (25%)]\tLoss: 0.790537\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.902986\n",
      "Train Epoch: 4 [34944/100000 (35%)]\tLoss: 0.865287\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.901129\n",
      "Train Epoch: 4 [44928/100000 (45%)]\tLoss: 0.919665\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.915886\n",
      "Train Epoch: 4 [54912/100000 (55%)]\tLoss: 0.830380\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 1.024832\n",
      "Train Epoch: 4 [64896/100000 (65%)]\tLoss: 0.948170\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.938957\n",
      "Train Epoch: 4 [74880/100000 (75%)]\tLoss: 0.785689\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.879881\n",
      "Train Epoch: 4 [84864/100000 (85%)]\tLoss: 0.900797\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.766320\n",
      "Train Epoch: 4 [94848/100000 (95%)]\tLoss: 0.843672\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.835864\n",
      "Parameter Set: (0.001, 512, 128, 0.1, 'mul')\n",
      "epoch = 0\n",
      "Train Epoch: 0 [4992/100000 (5%)]\tLoss: 1.914400\n",
      "Train Epoch: 0 [9984/100000 (10%)]\tLoss: 1.283414\n",
      "Train Epoch: 0 [14976/100000 (15%)]\tLoss: 1.198134\n",
      "Train Epoch: 0 [19968/100000 (20%)]\tLoss: 1.134460\n",
      "Train Epoch: 0 [24960/100000 (25%)]\tLoss: 1.180923\n",
      "Train Epoch: 0 [29952/100000 (30%)]\tLoss: 1.121498\n",
      "Train Epoch: 0 [34944/100000 (35%)]\tLoss: 1.068521\n",
      "Train Epoch: 0 [39936/100000 (40%)]\tLoss: 1.143734\n",
      "Train Epoch: 0 [44928/100000 (45%)]\tLoss: 1.066949\n",
      "Train Epoch: 0 [49920/100000 (50%)]\tLoss: 1.080556\n",
      "Train Epoch: 0 [54912/100000 (55%)]\tLoss: 1.086793\n",
      "Train Epoch: 0 [59904/100000 (60%)]\tLoss: 1.073382\n",
      "Train Epoch: 0 [64896/100000 (65%)]\tLoss: 1.066664\n",
      "Train Epoch: 0 [69888/100000 (70%)]\tLoss: 1.093385\n",
      "Train Epoch: 0 [74880/100000 (75%)]\tLoss: 1.069727\n",
      "Train Epoch: 0 [79872/100000 (80%)]\tLoss: 1.081253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [84864/100000 (85%)]\tLoss: 1.058260\n",
      "Train Epoch: 0 [89856/100000 (90%)]\tLoss: 1.018105\n",
      "Train Epoch: 0 [94848/100000 (95%)]\tLoss: 1.093275\n",
      "Train Epoch: 0 [99840/100000 (100%)]\tLoss: 1.092472\n",
      "epoch = 1\n",
      "Train Epoch: 1 [4992/100000 (5%)]\tLoss: 1.154116\n",
      "Train Epoch: 1 [9984/100000 (10%)]\tLoss: 1.079200\n",
      "Train Epoch: 1 [14976/100000 (15%)]\tLoss: 1.099629\n",
      "Train Epoch: 1 [19968/100000 (20%)]\tLoss: 1.079534\n",
      "Train Epoch: 1 [24960/100000 (25%)]\tLoss: 1.090568\n",
      "Train Epoch: 1 [29952/100000 (30%)]\tLoss: 1.066542\n",
      "Train Epoch: 1 [34944/100000 (35%)]\tLoss: 1.075300\n",
      "Train Epoch: 1 [39936/100000 (40%)]\tLoss: 1.033507\n",
      "Train Epoch: 1 [44928/100000 (45%)]\tLoss: 1.032608\n",
      "Train Epoch: 1 [49920/100000 (50%)]\tLoss: 1.035005\n",
      "Train Epoch: 1 [54912/100000 (55%)]\tLoss: 1.017093\n",
      "Train Epoch: 1 [59904/100000 (60%)]\tLoss: 1.070532\n",
      "Train Epoch: 1 [64896/100000 (65%)]\tLoss: 1.035247\n",
      "Train Epoch: 1 [69888/100000 (70%)]\tLoss: 1.072553\n",
      "Train Epoch: 1 [74880/100000 (75%)]\tLoss: 1.073163\n",
      "Train Epoch: 1 [79872/100000 (80%)]\tLoss: 1.096780\n",
      "Train Epoch: 1 [84864/100000 (85%)]\tLoss: 1.031099\n",
      "Train Epoch: 1 [89856/100000 (90%)]\tLoss: 1.027529\n",
      "Train Epoch: 1 [94848/100000 (95%)]\tLoss: 1.064007\n",
      "Train Epoch: 1 [99840/100000 (100%)]\tLoss: 1.075050\n",
      "epoch = 2\n",
      "Train Epoch: 2 [4992/100000 (5%)]\tLoss: 1.093491\n",
      "Train Epoch: 2 [9984/100000 (10%)]\tLoss: 0.995047\n",
      "Train Epoch: 2 [14976/100000 (15%)]\tLoss: 1.047818\n",
      "Train Epoch: 2 [19968/100000 (20%)]\tLoss: 1.052048\n",
      "Train Epoch: 2 [24960/100000 (25%)]\tLoss: 0.995366\n",
      "Train Epoch: 2 [29952/100000 (30%)]\tLoss: 0.962439\n",
      "Train Epoch: 2 [34944/100000 (35%)]\tLoss: 1.003665\n",
      "Train Epoch: 2 [39936/100000 (40%)]\tLoss: 0.936238\n",
      "Train Epoch: 2 [44928/100000 (45%)]\tLoss: 1.015079\n",
      "Train Epoch: 2 [49920/100000 (50%)]\tLoss: 0.946166\n",
      "Train Epoch: 2 [54912/100000 (55%)]\tLoss: 0.981635\n",
      "Train Epoch: 2 [59904/100000 (60%)]\tLoss: 1.059985\n",
      "Train Epoch: 2 [64896/100000 (65%)]\tLoss: 1.004311\n",
      "Train Epoch: 2 [69888/100000 (70%)]\tLoss: 1.037724\n",
      "Train Epoch: 2 [74880/100000 (75%)]\tLoss: 0.993434\n",
      "Train Epoch: 2 [79872/100000 (80%)]\tLoss: 1.061113\n",
      "Train Epoch: 2 [84864/100000 (85%)]\tLoss: 0.974982\n",
      "Train Epoch: 2 [89856/100000 (90%)]\tLoss: 0.941311\n",
      "Train Epoch: 2 [94848/100000 (95%)]\tLoss: 0.915003\n",
      "Train Epoch: 2 [99840/100000 (100%)]\tLoss: 1.056032\n",
      "epoch = 3\n",
      "Train Epoch: 3 [4992/100000 (5%)]\tLoss: 0.977555\n",
      "Train Epoch: 3 [9984/100000 (10%)]\tLoss: 0.861543\n",
      "Train Epoch: 3 [14976/100000 (15%)]\tLoss: 1.010455\n",
      "Train Epoch: 3 [19968/100000 (20%)]\tLoss: 0.964489\n",
      "Train Epoch: 3 [24960/100000 (25%)]\tLoss: 0.909156\n",
      "Train Epoch: 3 [29952/100000 (30%)]\tLoss: 0.891474\n",
      "Train Epoch: 3 [34944/100000 (35%)]\tLoss: 0.971983\n",
      "Train Epoch: 3 [39936/100000 (40%)]\tLoss: 0.887842\n",
      "Train Epoch: 3 [44928/100000 (45%)]\tLoss: 0.948012\n",
      "Train Epoch: 3 [49920/100000 (50%)]\tLoss: 0.880028\n",
      "Train Epoch: 3 [54912/100000 (55%)]\tLoss: 0.881673\n",
      "Train Epoch: 3 [59904/100000 (60%)]\tLoss: 0.983535\n",
      "Train Epoch: 3 [64896/100000 (65%)]\tLoss: 0.971343\n",
      "Train Epoch: 3 [69888/100000 (70%)]\tLoss: 0.911019\n",
      "Train Epoch: 3 [74880/100000 (75%)]\tLoss: 0.929094\n",
      "Train Epoch: 3 [79872/100000 (80%)]\tLoss: 1.006960\n",
      "Train Epoch: 3 [84864/100000 (85%)]\tLoss: 1.001108\n",
      "Train Epoch: 3 [89856/100000 (90%)]\tLoss: 0.891538\n",
      "Train Epoch: 3 [94848/100000 (95%)]\tLoss: 0.955675\n",
      "Train Epoch: 3 [99840/100000 (100%)]\tLoss: 0.983116\n",
      "epoch = 4\n",
      "Train Epoch: 4 [4992/100000 (5%)]\tLoss: 0.942095\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.820943\n",
      "Train Epoch: 4 [14976/100000 (15%)]\tLoss: 0.927526\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.936682\n",
      "Train Epoch: 4 [24960/100000 (25%)]\tLoss: 0.902324\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.818905\n",
      "Train Epoch: 4 [34944/100000 (35%)]\tLoss: 0.945672\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.845548\n",
      "Train Epoch: 4 [44928/100000 (45%)]\tLoss: 0.836037\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.882559\n",
      "Train Epoch: 4 [54912/100000 (55%)]\tLoss: 0.810521\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 1.014987\n",
      "Train Epoch: 4 [64896/100000 (65%)]\tLoss: 1.024020\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.905193\n",
      "Train Epoch: 4 [74880/100000 (75%)]\tLoss: 0.880806\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.903866\n",
      "Train Epoch: 4 [84864/100000 (85%)]\tLoss: 0.978141\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.786311\n",
      "Train Epoch: 4 [94848/100000 (95%)]\tLoss: 0.880675\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.913009\n",
      "Parameter Set: (0.001, 512, 128, 0.1, 'concat')\n",
      "epoch = 0\n",
      "Train Epoch: 0 [4992/100000 (5%)]\tLoss: 1.110537\n",
      "Train Epoch: 0 [9984/100000 (10%)]\tLoss: 1.118883\n",
      "Train Epoch: 0 [14976/100000 (15%)]\tLoss: 1.077866\n",
      "Train Epoch: 0 [19968/100000 (20%)]\tLoss: 1.093657\n",
      "Train Epoch: 0 [24960/100000 (25%)]\tLoss: 1.043104\n",
      "Train Epoch: 0 [29952/100000 (30%)]\tLoss: 1.099771\n",
      "Train Epoch: 0 [34944/100000 (35%)]\tLoss: 1.070263\n",
      "Train Epoch: 0 [39936/100000 (40%)]\tLoss: 1.065706\n",
      "Train Epoch: 0 [44928/100000 (45%)]\tLoss: 1.068887\n",
      "Train Epoch: 0 [49920/100000 (50%)]\tLoss: 1.052715\n",
      "Train Epoch: 0 [54912/100000 (55%)]\tLoss: 1.045558\n",
      "Train Epoch: 0 [59904/100000 (60%)]\tLoss: 1.038843\n",
      "Train Epoch: 0 [64896/100000 (65%)]\tLoss: 1.004504\n",
      "Train Epoch: 0 [69888/100000 (70%)]\tLoss: 1.057470\n",
      "Train Epoch: 0 [74880/100000 (75%)]\tLoss: 1.046250\n",
      "Train Epoch: 0 [79872/100000 (80%)]\tLoss: 1.101406\n",
      "Train Epoch: 0 [84864/100000 (85%)]\tLoss: 1.013492\n",
      "Train Epoch: 0 [89856/100000 (90%)]\tLoss: 0.918979\n",
      "Train Epoch: 0 [94848/100000 (95%)]\tLoss: 1.003325\n",
      "Train Epoch: 0 [99840/100000 (100%)]\tLoss: 1.009435\n",
      "epoch = 1\n",
      "Train Epoch: 1 [4992/100000 (5%)]\tLoss: 1.045844\n",
      "Train Epoch: 1 [9984/100000 (10%)]\tLoss: 1.048194\n",
      "Train Epoch: 1 [14976/100000 (15%)]\tLoss: 1.051857\n",
      "Train Epoch: 1 [19968/100000 (20%)]\tLoss: 1.007139\n",
      "Train Epoch: 1 [24960/100000 (25%)]\tLoss: 0.947248\n",
      "Train Epoch: 1 [29952/100000 (30%)]\tLoss: 0.913270\n",
      "Train Epoch: 1 [34944/100000 (35%)]\tLoss: 0.990848\n",
      "Train Epoch: 1 [39936/100000 (40%)]\tLoss: 0.876077\n",
      "Train Epoch: 1 [44928/100000 (45%)]\tLoss: 0.959371\n",
      "Train Epoch: 1 [49920/100000 (50%)]\tLoss: 0.948844\n",
      "Train Epoch: 1 [54912/100000 (55%)]\tLoss: 0.923814\n",
      "Train Epoch: 1 [59904/100000 (60%)]\tLoss: 0.963396\n",
      "Train Epoch: 1 [64896/100000 (65%)]\tLoss: 0.964289\n",
      "Train Epoch: 1 [69888/100000 (70%)]\tLoss: 0.896090\n",
      "Train Epoch: 1 [74880/100000 (75%)]\tLoss: 0.887402\n",
      "Train Epoch: 1 [79872/100000 (80%)]\tLoss: 0.945812\n",
      "Train Epoch: 1 [84864/100000 (85%)]\tLoss: 0.990511\n",
      "Train Epoch: 1 [89856/100000 (90%)]\tLoss: 0.797015\n",
      "Train Epoch: 1 [94848/100000 (95%)]\tLoss: 0.868035\n",
      "Train Epoch: 1 [99840/100000 (100%)]\tLoss: 0.954406\n",
      "epoch = 2\n",
      "Train Epoch: 2 [4992/100000 (5%)]\tLoss: 0.986999\n",
      "Train Epoch: 2 [9984/100000 (10%)]\tLoss: 0.922102\n",
      "Train Epoch: 2 [14976/100000 (15%)]\tLoss: 0.898119\n",
      "Train Epoch: 2 [19968/100000 (20%)]\tLoss: 0.927468\n",
      "Train Epoch: 2 [24960/100000 (25%)]\tLoss: 0.908109\n",
      "Train Epoch: 2 [29952/100000 (30%)]\tLoss: 0.890623\n",
      "Train Epoch: 2 [34944/100000 (35%)]\tLoss: 0.941330\n",
      "Train Epoch: 2 [39936/100000 (40%)]\tLoss: 0.846661\n",
      "Train Epoch: 2 [44928/100000 (45%)]\tLoss: 0.926678\n",
      "Train Epoch: 2 [49920/100000 (50%)]\tLoss: 0.892170\n",
      "Train Epoch: 2 [54912/100000 (55%)]\tLoss: 0.831291\n",
      "Train Epoch: 2 [59904/100000 (60%)]\tLoss: 0.993241\n",
      "Train Epoch: 2 [64896/100000 (65%)]\tLoss: 0.959380\n",
      "Train Epoch: 2 [69888/100000 (70%)]\tLoss: 0.858722\n",
      "Train Epoch: 2 [74880/100000 (75%)]\tLoss: 0.845994\n",
      "Train Epoch: 2 [79872/100000 (80%)]\tLoss: 0.950442\n",
      "Train Epoch: 2 [84864/100000 (85%)]\tLoss: 0.911200\n",
      "Train Epoch: 2 [89856/100000 (90%)]\tLoss: 0.773481\n",
      "Train Epoch: 2 [94848/100000 (95%)]\tLoss: 0.843099\n",
      "Train Epoch: 2 [99840/100000 (100%)]\tLoss: 0.940822\n",
      "epoch = 3\n",
      "Train Epoch: 3 [4992/100000 (5%)]\tLoss: 0.921066\n",
      "Train Epoch: 3 [9984/100000 (10%)]\tLoss: 0.859256\n",
      "Train Epoch: 3 [14976/100000 (15%)]\tLoss: 0.823271\n",
      "Train Epoch: 3 [19968/100000 (20%)]\tLoss: 0.880278\n",
      "Train Epoch: 3 [24960/100000 (25%)]\tLoss: 0.887825\n",
      "Train Epoch: 3 [29952/100000 (30%)]\tLoss: 0.733489\n",
      "Train Epoch: 3 [34944/100000 (35%)]\tLoss: 0.871712\n",
      "Train Epoch: 3 [39936/100000 (40%)]\tLoss: 0.840425\n",
      "Train Epoch: 3 [44928/100000 (45%)]\tLoss: 0.875774\n",
      "Train Epoch: 3 [49920/100000 (50%)]\tLoss: 0.883345\n",
      "Train Epoch: 3 [54912/100000 (55%)]\tLoss: 0.820473\n",
      "Train Epoch: 3 [59904/100000 (60%)]\tLoss: 1.009340\n",
      "Train Epoch: 3 [64896/100000 (65%)]\tLoss: 0.924480\n",
      "Train Epoch: 3 [69888/100000 (70%)]\tLoss: 0.884007\n",
      "Train Epoch: 3 [74880/100000 (75%)]\tLoss: 0.819525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [79872/100000 (80%)]\tLoss: 0.907933\n",
      "Train Epoch: 3 [84864/100000 (85%)]\tLoss: 0.909393\n",
      "Train Epoch: 3 [89856/100000 (90%)]\tLoss: 0.778947\n",
      "Train Epoch: 3 [94848/100000 (95%)]\tLoss: 0.815705\n",
      "Train Epoch: 3 [99840/100000 (100%)]\tLoss: 0.884360\n",
      "epoch = 4\n",
      "Train Epoch: 4 [4992/100000 (5%)]\tLoss: 0.869929\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.844599\n",
      "Train Epoch: 4 [14976/100000 (15%)]\tLoss: 0.810252\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.873065\n",
      "Train Epoch: 4 [24960/100000 (25%)]\tLoss: 0.888149\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.809627\n",
      "Train Epoch: 4 [34944/100000 (35%)]\tLoss: 0.857806\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.853804\n",
      "Train Epoch: 4 [44928/100000 (45%)]\tLoss: 0.836085\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.769454\n",
      "Train Epoch: 4 [54912/100000 (55%)]\tLoss: 0.804534\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 0.997016\n",
      "Train Epoch: 4 [64896/100000 (65%)]\tLoss: 0.875825\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.778595\n",
      "Train Epoch: 4 [74880/100000 (75%)]\tLoss: 0.757513\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.882055\n",
      "Train Epoch: 4 [84864/100000 (85%)]\tLoss: 0.930077\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.717730\n",
      "Train Epoch: 4 [94848/100000 (95%)]\tLoss: 0.844431\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.850468\n",
      "Parameter Set: (0.001, 512, 128, 0.1, 'subtract')\n",
      "epoch = 0\n",
      "Train Epoch: 0 [4992/100000 (5%)]\tLoss: 1.306993\n",
      "Train Epoch: 0 [9984/100000 (10%)]\tLoss: 1.606850\n",
      "Train Epoch: 0 [14976/100000 (15%)]\tLoss: 1.346258\n",
      "Train Epoch: 0 [19968/100000 (20%)]\tLoss: 1.318401\n",
      "Train Epoch: 0 [24960/100000 (25%)]\tLoss: 1.148120\n",
      "Train Epoch: 0 [29952/100000 (30%)]\tLoss: 1.201139\n",
      "Train Epoch: 0 [34944/100000 (35%)]\tLoss: 1.070752\n",
      "Train Epoch: 0 [39936/100000 (40%)]\tLoss: 1.141946\n",
      "Train Epoch: 0 [44928/100000 (45%)]\tLoss: 1.042660\n",
      "Train Epoch: 0 [49920/100000 (50%)]\tLoss: 1.218051\n",
      "Train Epoch: 0 [54912/100000 (55%)]\tLoss: 1.155031\n",
      "Train Epoch: 0 [59904/100000 (60%)]\tLoss: 1.146642\n",
      "Train Epoch: 0 [64896/100000 (65%)]\tLoss: 1.107320\n",
      "Train Epoch: 0 [69888/100000 (70%)]\tLoss: 1.088546\n",
      "Train Epoch: 0 [74880/100000 (75%)]\tLoss: 1.045081\n",
      "Train Epoch: 0 [79872/100000 (80%)]\tLoss: 1.089288\n",
      "Train Epoch: 0 [84864/100000 (85%)]\tLoss: 1.148104\n",
      "Train Epoch: 0 [89856/100000 (90%)]\tLoss: 1.099807\n",
      "Train Epoch: 0 [94848/100000 (95%)]\tLoss: 1.036427\n",
      "Train Epoch: 0 [99840/100000 (100%)]\tLoss: 1.065238\n",
      "epoch = 1\n",
      "Train Epoch: 1 [4992/100000 (5%)]\tLoss: 1.064594\n",
      "Train Epoch: 1 [9984/100000 (10%)]\tLoss: 1.080084\n",
      "Train Epoch: 1 [14976/100000 (15%)]\tLoss: 1.002839\n",
      "Train Epoch: 1 [19968/100000 (20%)]\tLoss: 1.102768\n",
      "Train Epoch: 1 [24960/100000 (25%)]\tLoss: 0.999399\n",
      "Train Epoch: 1 [29952/100000 (30%)]\tLoss: 1.032953\n",
      "Train Epoch: 1 [34944/100000 (35%)]\tLoss: 1.006212\n",
      "Train Epoch: 1 [39936/100000 (40%)]\tLoss: 1.046843\n",
      "Train Epoch: 1 [44928/100000 (45%)]\tLoss: 1.021039\n",
      "Train Epoch: 1 [49920/100000 (50%)]\tLoss: 1.063584\n",
      "Train Epoch: 1 [54912/100000 (55%)]\tLoss: 1.030986\n",
      "Train Epoch: 1 [59904/100000 (60%)]\tLoss: 1.087851\n",
      "Train Epoch: 1 [64896/100000 (65%)]\tLoss: 1.021623\n",
      "Train Epoch: 1 [69888/100000 (70%)]\tLoss: 0.949499\n",
      "Train Epoch: 1 [74880/100000 (75%)]\tLoss: 0.960982\n",
      "Train Epoch: 1 [79872/100000 (80%)]\tLoss: 1.055876\n",
      "Train Epoch: 1 [84864/100000 (85%)]\tLoss: 1.080305\n",
      "Train Epoch: 1 [89856/100000 (90%)]\tLoss: 0.840020\n",
      "Train Epoch: 1 [94848/100000 (95%)]\tLoss: 1.013969\n",
      "Train Epoch: 1 [99840/100000 (100%)]\tLoss: 1.024069\n",
      "epoch = 2\n",
      "Train Epoch: 2 [4992/100000 (5%)]\tLoss: 0.966340\n",
      "Train Epoch: 2 [9984/100000 (10%)]\tLoss: 1.010391\n",
      "Train Epoch: 2 [14976/100000 (15%)]\tLoss: 0.865120\n",
      "Train Epoch: 2 [19968/100000 (20%)]\tLoss: 0.949891\n",
      "Train Epoch: 2 [24960/100000 (25%)]\tLoss: 0.937704\n",
      "Train Epoch: 2 [29952/100000 (30%)]\tLoss: 0.935794\n",
      "Train Epoch: 2 [34944/100000 (35%)]\tLoss: 0.989971\n",
      "Train Epoch: 2 [39936/100000 (40%)]\tLoss: 0.996912\n",
      "Train Epoch: 2 [44928/100000 (45%)]\tLoss: 0.893174\n",
      "Train Epoch: 2 [49920/100000 (50%)]\tLoss: 0.964512\n",
      "Train Epoch: 2 [54912/100000 (55%)]\tLoss: 0.924437\n",
      "Train Epoch: 2 [59904/100000 (60%)]\tLoss: 1.031103\n",
      "Train Epoch: 2 [64896/100000 (65%)]\tLoss: 0.958660\n",
      "Train Epoch: 2 [69888/100000 (70%)]\tLoss: 0.915494\n",
      "Train Epoch: 2 [74880/100000 (75%)]\tLoss: 0.922016\n",
      "Train Epoch: 2 [79872/100000 (80%)]\tLoss: 1.014622\n",
      "Train Epoch: 2 [84864/100000 (85%)]\tLoss: 0.961097\n",
      "Train Epoch: 2 [89856/100000 (90%)]\tLoss: 0.816102\n",
      "Train Epoch: 2 [94848/100000 (95%)]\tLoss: 0.979864\n",
      "Train Epoch: 2 [99840/100000 (100%)]\tLoss: 0.902458\n",
      "epoch = 3\n",
      "Train Epoch: 3 [4992/100000 (5%)]\tLoss: 0.969300\n",
      "Train Epoch: 3 [9984/100000 (10%)]\tLoss: 0.991534\n",
      "Train Epoch: 3 [14976/100000 (15%)]\tLoss: 0.828474\n",
      "Train Epoch: 3 [19968/100000 (20%)]\tLoss: 0.943299\n",
      "Train Epoch: 3 [24960/100000 (25%)]\tLoss: 0.875385\n",
      "Train Epoch: 3 [29952/100000 (30%)]\tLoss: 0.936581\n",
      "Train Epoch: 3 [34944/100000 (35%)]\tLoss: 0.881158\n",
      "Train Epoch: 3 [39936/100000 (40%)]\tLoss: 0.954227\n",
      "Train Epoch: 3 [44928/100000 (45%)]\tLoss: 0.959838\n",
      "Train Epoch: 3 [49920/100000 (50%)]\tLoss: 0.949960\n",
      "Train Epoch: 3 [54912/100000 (55%)]\tLoss: 0.876499\n",
      "Train Epoch: 3 [59904/100000 (60%)]\tLoss: 0.940766\n",
      "Train Epoch: 3 [64896/100000 (65%)]\tLoss: 0.941727\n",
      "Train Epoch: 3 [69888/100000 (70%)]\tLoss: 0.877443\n",
      "Train Epoch: 3 [74880/100000 (75%)]\tLoss: 0.876608\n",
      "Train Epoch: 3 [79872/100000 (80%)]\tLoss: 0.949498\n",
      "Train Epoch: 3 [84864/100000 (85%)]\tLoss: 0.818996\n",
      "Train Epoch: 3 [89856/100000 (90%)]\tLoss: 0.810490\n",
      "Train Epoch: 3 [94848/100000 (95%)]\tLoss: 0.900823\n",
      "Train Epoch: 3 [99840/100000 (100%)]\tLoss: 0.872914\n",
      "epoch = 4\n",
      "Train Epoch: 4 [4992/100000 (5%)]\tLoss: 0.920686\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.930988\n",
      "Train Epoch: 4 [14976/100000 (15%)]\tLoss: 0.812473\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.909216\n",
      "Train Epoch: 4 [24960/100000 (25%)]\tLoss: 0.812481\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.872856\n",
      "Train Epoch: 4 [34944/100000 (35%)]\tLoss: 0.805315\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.865142\n",
      "Train Epoch: 4 [44928/100000 (45%)]\tLoss: 0.888738\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.890657\n",
      "Train Epoch: 4 [54912/100000 (55%)]\tLoss: 0.789244\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 0.970940\n",
      "Train Epoch: 4 [64896/100000 (65%)]\tLoss: 0.936852\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.864017\n",
      "Train Epoch: 4 [74880/100000 (75%)]\tLoss: 0.840646\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.843525\n",
      "Train Epoch: 4 [84864/100000 (85%)]\tLoss: 0.828494\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.750059\n",
      "Train Epoch: 4 [94848/100000 (95%)]\tLoss: 0.815147\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.848774\n",
      "Parameter Set: (0.01, 512, 64, 0.1, 'mul')\n",
      "epoch = 0\n",
      "Train Epoch: 0 [4992/100000 (5%)]\tLoss: 1.601747\n",
      "Train Epoch: 0 [9984/100000 (10%)]\tLoss: 1.173255\n",
      "Train Epoch: 0 [14976/100000 (15%)]\tLoss: 1.140267\n",
      "Train Epoch: 0 [19968/100000 (20%)]\tLoss: 1.203057\n",
      "Train Epoch: 0 [24960/100000 (25%)]\tLoss: 1.109937\n",
      "Train Epoch: 0 [29952/100000 (30%)]\tLoss: 1.120242\n",
      "Train Epoch: 0 [34944/100000 (35%)]\tLoss: 1.118373\n",
      "Train Epoch: 0 [39936/100000 (40%)]\tLoss: 1.116954\n",
      "Train Epoch: 0 [44928/100000 (45%)]\tLoss: 1.112511\n",
      "Train Epoch: 0 [49920/100000 (50%)]\tLoss: 1.089309\n",
      "Train Epoch: 0 [54912/100000 (55%)]\tLoss: 1.091136\n",
      "Train Epoch: 0 [59904/100000 (60%)]\tLoss: 1.081418\n",
      "Train Epoch: 0 [64896/100000 (65%)]\tLoss: 1.087365\n",
      "Train Epoch: 0 [69888/100000 (70%)]\tLoss: 1.091053\n",
      "Train Epoch: 0 [74880/100000 (75%)]\tLoss: 1.065952\n",
      "Train Epoch: 0 [79872/100000 (80%)]\tLoss: 1.094704\n",
      "Train Epoch: 0 [84864/100000 (85%)]\tLoss: 1.084136\n",
      "Train Epoch: 0 [89856/100000 (90%)]\tLoss: 1.087433\n",
      "Train Epoch: 0 [94848/100000 (95%)]\tLoss: 1.101509\n",
      "Train Epoch: 0 [99840/100000 (100%)]\tLoss: 1.097961\n",
      "epoch = 1\n",
      "Train Epoch: 1 [4992/100000 (5%)]\tLoss: 1.083971\n",
      "Train Epoch: 1 [9984/100000 (10%)]\tLoss: 1.124172\n",
      "Train Epoch: 1 [14976/100000 (15%)]\tLoss: 1.093744\n",
      "Train Epoch: 1 [19968/100000 (20%)]\tLoss: 1.063954\n",
      "Train Epoch: 1 [24960/100000 (25%)]\tLoss: 1.132950\n",
      "Train Epoch: 1 [29952/100000 (30%)]\tLoss: 1.075320\n",
      "Train Epoch: 1 [34944/100000 (35%)]\tLoss: 1.055322\n",
      "Train Epoch: 1 [39936/100000 (40%)]\tLoss: 1.054166\n",
      "Train Epoch: 1 [44928/100000 (45%)]\tLoss: 1.049952\n",
      "Train Epoch: 1 [49920/100000 (50%)]\tLoss: 1.025605\n",
      "Train Epoch: 1 [54912/100000 (55%)]\tLoss: 1.070002\n",
      "Train Epoch: 1 [59904/100000 (60%)]\tLoss: 1.072529\n",
      "Train Epoch: 1 [64896/100000 (65%)]\tLoss: 1.038910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [69888/100000 (70%)]\tLoss: 1.050431\n",
      "Train Epoch: 1 [74880/100000 (75%)]\tLoss: 1.054444\n",
      "Train Epoch: 1 [79872/100000 (80%)]\tLoss: 1.088788\n",
      "Train Epoch: 1 [84864/100000 (85%)]\tLoss: 1.050695\n",
      "Train Epoch: 1 [89856/100000 (90%)]\tLoss: 0.998435\n",
      "Train Epoch: 1 [94848/100000 (95%)]\tLoss: 1.085672\n",
      "Train Epoch: 1 [99840/100000 (100%)]\tLoss: 1.102667\n",
      "epoch = 2\n",
      "Train Epoch: 2 [4992/100000 (5%)]\tLoss: 1.112649\n",
      "Train Epoch: 2 [9984/100000 (10%)]\tLoss: 0.987496\n",
      "Train Epoch: 2 [14976/100000 (15%)]\tLoss: 1.061691\n",
      "Train Epoch: 2 [19968/100000 (20%)]\tLoss: 1.066441\n",
      "Train Epoch: 2 [24960/100000 (25%)]\tLoss: 1.080441\n",
      "Train Epoch: 2 [29952/100000 (30%)]\tLoss: 1.026772\n",
      "Train Epoch: 2 [34944/100000 (35%)]\tLoss: 1.036933\n",
      "Train Epoch: 2 [39936/100000 (40%)]\tLoss: 0.967230\n",
      "Train Epoch: 2 [44928/100000 (45%)]\tLoss: 1.058061\n",
      "Train Epoch: 2 [49920/100000 (50%)]\tLoss: 0.982778\n",
      "Train Epoch: 2 [54912/100000 (55%)]\tLoss: 0.978976\n",
      "Train Epoch: 2 [59904/100000 (60%)]\tLoss: 1.086357\n",
      "Train Epoch: 2 [64896/100000 (65%)]\tLoss: 0.989012\n",
      "Train Epoch: 2 [69888/100000 (70%)]\tLoss: 1.011935\n",
      "Train Epoch: 2 [74880/100000 (75%)]\tLoss: 0.975377\n",
      "Train Epoch: 2 [79872/100000 (80%)]\tLoss: 0.998922\n",
      "Train Epoch: 2 [84864/100000 (85%)]\tLoss: 1.035296\n",
      "Train Epoch: 2 [89856/100000 (90%)]\tLoss: 1.049987\n",
      "Train Epoch: 2 [94848/100000 (95%)]\tLoss: 1.019641\n",
      "Train Epoch: 2 [99840/100000 (100%)]\tLoss: 1.063578\n",
      "epoch = 3\n",
      "Train Epoch: 3 [4992/100000 (5%)]\tLoss: 1.015132\n",
      "Train Epoch: 3 [9984/100000 (10%)]\tLoss: 0.971706\n",
      "Train Epoch: 3 [14976/100000 (15%)]\tLoss: 1.023456\n",
      "Train Epoch: 3 [19968/100000 (20%)]\tLoss: 1.017113\n",
      "Train Epoch: 3 [24960/100000 (25%)]\tLoss: 0.988187\n",
      "Train Epoch: 3 [29952/100000 (30%)]\tLoss: 0.964734\n",
      "Train Epoch: 3 [34944/100000 (35%)]\tLoss: 0.961944\n",
      "Train Epoch: 3 [39936/100000 (40%)]\tLoss: 0.892366\n",
      "Train Epoch: 3 [44928/100000 (45%)]\tLoss: 0.988374\n",
      "Train Epoch: 3 [49920/100000 (50%)]\tLoss: 0.911521\n",
      "Train Epoch: 3 [54912/100000 (55%)]\tLoss: 0.943952\n",
      "Train Epoch: 3 [59904/100000 (60%)]\tLoss: 1.056530\n",
      "Train Epoch: 3 [64896/100000 (65%)]\tLoss: 1.021909\n",
      "Train Epoch: 3 [69888/100000 (70%)]\tLoss: 0.957225\n",
      "Train Epoch: 3 [74880/100000 (75%)]\tLoss: 0.942388\n",
      "Train Epoch: 3 [79872/100000 (80%)]\tLoss: 0.932603\n",
      "Train Epoch: 3 [84864/100000 (85%)]\tLoss: 0.990955\n",
      "Train Epoch: 3 [89856/100000 (90%)]\tLoss: 0.975268\n",
      "Train Epoch: 3 [94848/100000 (95%)]\tLoss: 1.078805\n",
      "Train Epoch: 3 [99840/100000 (100%)]\tLoss: 1.043359\n",
      "epoch = 4\n",
      "Train Epoch: 4 [4992/100000 (5%)]\tLoss: 1.045708\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.917539\n",
      "Train Epoch: 4 [14976/100000 (15%)]\tLoss: 0.994277\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.996506\n",
      "Train Epoch: 4 [24960/100000 (25%)]\tLoss: 1.012844\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.876729\n",
      "Train Epoch: 4 [34944/100000 (35%)]\tLoss: 0.927388\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.869437\n",
      "Train Epoch: 4 [44928/100000 (45%)]\tLoss: 0.958049\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.871718\n",
      "Train Epoch: 4 [54912/100000 (55%)]\tLoss: 0.879181\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 1.014232\n",
      "Train Epoch: 4 [64896/100000 (65%)]\tLoss: 0.968957\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.957930\n",
      "Train Epoch: 4 [74880/100000 (75%)]\tLoss: 0.923861\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 1.014566\n",
      "Train Epoch: 4 [84864/100000 (85%)]\tLoss: 1.009668\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.790527\n",
      "Train Epoch: 4 [94848/100000 (95%)]\tLoss: 0.909424\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.976386\n",
      "Parameter Set: (0.01, 512, 64, 0.1, 'concat')\n",
      "epoch = 0\n",
      "Train Epoch: 0 [4992/100000 (5%)]\tLoss: 1.075312\n",
      "Train Epoch: 0 [9984/100000 (10%)]\tLoss: 1.116284\n",
      "Train Epoch: 0 [14976/100000 (15%)]\tLoss: 1.113830\n",
      "Train Epoch: 0 [19968/100000 (20%)]\tLoss: 1.150410\n",
      "Train Epoch: 0 [24960/100000 (25%)]\tLoss: 1.065630\n",
      "Train Epoch: 0 [29952/100000 (30%)]\tLoss: 1.103169\n",
      "Train Epoch: 0 [34944/100000 (35%)]\tLoss: 1.062247\n",
      "Train Epoch: 0 [39936/100000 (40%)]\tLoss: 1.050331\n",
      "Train Epoch: 0 [44928/100000 (45%)]\tLoss: 1.081058\n",
      "Train Epoch: 0 [49920/100000 (50%)]\tLoss: 1.045596\n",
      "Train Epoch: 0 [54912/100000 (55%)]\tLoss: 1.105300\n",
      "Train Epoch: 0 [59904/100000 (60%)]\tLoss: 1.074088\n",
      "Train Epoch: 0 [64896/100000 (65%)]\tLoss: 1.000749\n",
      "Train Epoch: 0 [69888/100000 (70%)]\tLoss: 0.990170\n",
      "Train Epoch: 0 [74880/100000 (75%)]\tLoss: 1.026596\n",
      "Train Epoch: 0 [79872/100000 (80%)]\tLoss: 1.049210\n",
      "Train Epoch: 0 [84864/100000 (85%)]\tLoss: 1.004826\n",
      "Train Epoch: 0 [89856/100000 (90%)]\tLoss: 0.924315\n",
      "Train Epoch: 0 [94848/100000 (95%)]\tLoss: 0.979980\n",
      "Train Epoch: 0 [99840/100000 (100%)]\tLoss: 1.009672\n",
      "epoch = 1\n",
      "Train Epoch: 1 [4992/100000 (5%)]\tLoss: 1.027640\n",
      "Train Epoch: 1 [9984/100000 (10%)]\tLoss: 1.007073\n",
      "Train Epoch: 1 [14976/100000 (15%)]\tLoss: 1.000444\n",
      "Train Epoch: 1 [19968/100000 (20%)]\tLoss: 0.986006\n",
      "Train Epoch: 1 [24960/100000 (25%)]\tLoss: 0.963946\n",
      "Train Epoch: 1 [29952/100000 (30%)]\tLoss: 0.903629\n",
      "Train Epoch: 1 [34944/100000 (35%)]\tLoss: 0.977637\n",
      "Train Epoch: 1 [39936/100000 (40%)]\tLoss: 0.869857\n",
      "Train Epoch: 1 [44928/100000 (45%)]\tLoss: 0.939982\n",
      "Train Epoch: 1 [49920/100000 (50%)]\tLoss: 0.954591\n",
      "Train Epoch: 1 [54912/100000 (55%)]\tLoss: 0.892866\n",
      "Train Epoch: 1 [59904/100000 (60%)]\tLoss: 1.010407\n",
      "Train Epoch: 1 [64896/100000 (65%)]\tLoss: 0.928063\n",
      "Train Epoch: 1 [69888/100000 (70%)]\tLoss: 0.914943\n",
      "Train Epoch: 1 [74880/100000 (75%)]\tLoss: 0.847494\n",
      "Train Epoch: 1 [79872/100000 (80%)]\tLoss: 0.950311\n",
      "Train Epoch: 1 [84864/100000 (85%)]\tLoss: 1.022218\n",
      "Train Epoch: 1 [89856/100000 (90%)]\tLoss: 0.813968\n",
      "Train Epoch: 1 [94848/100000 (95%)]\tLoss: 0.897781\n",
      "Train Epoch: 1 [99840/100000 (100%)]\tLoss: 0.924158\n",
      "epoch = 2\n",
      "Train Epoch: 2 [4992/100000 (5%)]\tLoss: 1.010796\n",
      "Train Epoch: 2 [9984/100000 (10%)]\tLoss: 0.832362\n",
      "Train Epoch: 2 [14976/100000 (15%)]\tLoss: 0.927689\n",
      "Train Epoch: 2 [19968/100000 (20%)]\tLoss: 0.934315\n",
      "Train Epoch: 2 [24960/100000 (25%)]\tLoss: 0.905049\n",
      "Train Epoch: 2 [29952/100000 (30%)]\tLoss: 0.836993\n",
      "Train Epoch: 2 [34944/100000 (35%)]\tLoss: 0.923859\n",
      "Train Epoch: 2 [39936/100000 (40%)]\tLoss: 0.858856\n",
      "Train Epoch: 2 [44928/100000 (45%)]\tLoss: 0.873904\n",
      "Train Epoch: 2 [49920/100000 (50%)]\tLoss: 0.901415\n",
      "Train Epoch: 2 [54912/100000 (55%)]\tLoss: 0.850480\n",
      "Train Epoch: 2 [59904/100000 (60%)]\tLoss: 1.000123\n",
      "Train Epoch: 2 [64896/100000 (65%)]\tLoss: 0.924427\n",
      "Train Epoch: 2 [69888/100000 (70%)]\tLoss: 0.918599\n",
      "Train Epoch: 2 [74880/100000 (75%)]\tLoss: 0.865142\n",
      "Train Epoch: 2 [79872/100000 (80%)]\tLoss: 0.893622\n",
      "Train Epoch: 2 [84864/100000 (85%)]\tLoss: 0.900878\n",
      "Train Epoch: 2 [89856/100000 (90%)]\tLoss: 0.788465\n",
      "Train Epoch: 2 [94848/100000 (95%)]\tLoss: 0.852272\n",
      "Train Epoch: 2 [99840/100000 (100%)]\tLoss: 0.944458\n",
      "epoch = 3\n",
      "Train Epoch: 3 [4992/100000 (5%)]\tLoss: 0.875218\n",
      "Train Epoch: 3 [9984/100000 (10%)]\tLoss: 0.906986\n",
      "Train Epoch: 3 [14976/100000 (15%)]\tLoss: 0.839563\n",
      "Train Epoch: 3 [19968/100000 (20%)]\tLoss: 0.949720\n",
      "Train Epoch: 3 [24960/100000 (25%)]\tLoss: 0.845031\n",
      "Train Epoch: 3 [29952/100000 (30%)]\tLoss: 0.717166\n",
      "Train Epoch: 3 [34944/100000 (35%)]\tLoss: 0.877062\n",
      "Train Epoch: 3 [39936/100000 (40%)]\tLoss: 0.827903\n",
      "Train Epoch: 3 [44928/100000 (45%)]\tLoss: 0.825787\n",
      "Train Epoch: 3 [49920/100000 (50%)]\tLoss: 0.844391\n",
      "Train Epoch: 3 [54912/100000 (55%)]\tLoss: 0.767640\n",
      "Train Epoch: 3 [59904/100000 (60%)]\tLoss: 1.035679\n",
      "Train Epoch: 3 [64896/100000 (65%)]\tLoss: 0.881526\n",
      "Train Epoch: 3 [69888/100000 (70%)]\tLoss: 0.867527\n",
      "Train Epoch: 3 [74880/100000 (75%)]\tLoss: 0.816496\n",
      "Train Epoch: 3 [79872/100000 (80%)]\tLoss: 0.874179\n",
      "Train Epoch: 3 [84864/100000 (85%)]\tLoss: 0.921580\n",
      "Train Epoch: 3 [89856/100000 (90%)]\tLoss: 0.750880\n",
      "Train Epoch: 3 [94848/100000 (95%)]\tLoss: 0.788585\n",
      "Train Epoch: 3 [99840/100000 (100%)]\tLoss: 0.875321\n",
      "epoch = 4\n",
      "Train Epoch: 4 [4992/100000 (5%)]\tLoss: 0.885018\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.852788\n",
      "Train Epoch: 4 [14976/100000 (15%)]\tLoss: 0.848469\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.886887\n",
      "Train Epoch: 4 [24960/100000 (25%)]\tLoss: 0.842074\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.723519\n",
      "Train Epoch: 4 [34944/100000 (35%)]\tLoss: 0.888442\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.847067\n",
      "Train Epoch: 4 [44928/100000 (45%)]\tLoss: 0.867775\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.793643\n",
      "Train Epoch: 4 [54912/100000 (55%)]\tLoss: 0.768389\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 1.022614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [64896/100000 (65%)]\tLoss: 0.805981\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.806756\n",
      "Train Epoch: 4 [74880/100000 (75%)]\tLoss: 0.727149\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.856333\n",
      "Train Epoch: 4 [84864/100000 (85%)]\tLoss: 0.948179\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.748077\n",
      "Train Epoch: 4 [94848/100000 (95%)]\tLoss: 0.787526\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.899222\n",
      "Parameter Set: (0.01, 512, 64, 0.1, 'subtract')\n",
      "epoch = 0\n",
      "Train Epoch: 0 [4992/100000 (5%)]\tLoss: 1.558538\n",
      "Train Epoch: 0 [9984/100000 (10%)]\tLoss: 1.274714\n",
      "Train Epoch: 0 [14976/100000 (15%)]\tLoss: 1.228295\n",
      "Train Epoch: 0 [19968/100000 (20%)]\tLoss: 1.326538\n",
      "Train Epoch: 0 [24960/100000 (25%)]\tLoss: 1.103672\n",
      "Train Epoch: 0 [29952/100000 (30%)]\tLoss: 1.302859\n",
      "Train Epoch: 0 [34944/100000 (35%)]\tLoss: 1.082945\n",
      "Train Epoch: 0 [39936/100000 (40%)]\tLoss: 1.156664\n",
      "Train Epoch: 0 [44928/100000 (45%)]\tLoss: 1.169660\n",
      "Train Epoch: 0 [49920/100000 (50%)]\tLoss: 1.291843\n",
      "Train Epoch: 0 [54912/100000 (55%)]\tLoss: 1.250618\n",
      "Train Epoch: 0 [59904/100000 (60%)]\tLoss: 1.106305\n",
      "Train Epoch: 0 [64896/100000 (65%)]\tLoss: 1.008358\n",
      "Train Epoch: 0 [69888/100000 (70%)]\tLoss: 1.172917\n",
      "Train Epoch: 0 [74880/100000 (75%)]\tLoss: 1.150221\n",
      "Train Epoch: 0 [79872/100000 (80%)]\tLoss: 1.129344\n",
      "Train Epoch: 0 [84864/100000 (85%)]\tLoss: 1.121538\n",
      "Train Epoch: 0 [89856/100000 (90%)]\tLoss: 1.076330\n",
      "Train Epoch: 0 [94848/100000 (95%)]\tLoss: 1.049443\n",
      "Train Epoch: 0 [99840/100000 (100%)]\tLoss: 1.172951\n",
      "epoch = 1\n",
      "Train Epoch: 1 [4992/100000 (5%)]\tLoss: 1.064383\n",
      "Train Epoch: 1 [9984/100000 (10%)]\tLoss: 1.186553\n",
      "Train Epoch: 1 [14976/100000 (15%)]\tLoss: 0.981544\n",
      "Train Epoch: 1 [19968/100000 (20%)]\tLoss: 0.980251\n",
      "Train Epoch: 1 [24960/100000 (25%)]\tLoss: 1.030167\n",
      "Train Epoch: 1 [29952/100000 (30%)]\tLoss: 1.094915\n",
      "Train Epoch: 1 [34944/100000 (35%)]\tLoss: 1.053088\n",
      "Train Epoch: 1 [39936/100000 (40%)]\tLoss: 1.050979\n",
      "Train Epoch: 1 [44928/100000 (45%)]\tLoss: 1.066052\n",
      "Train Epoch: 1 [49920/100000 (50%)]\tLoss: 1.077773\n",
      "Train Epoch: 1 [54912/100000 (55%)]\tLoss: 1.019277\n",
      "Train Epoch: 1 [59904/100000 (60%)]\tLoss: 1.106389\n",
      "Train Epoch: 1 [64896/100000 (65%)]\tLoss: 1.056689\n",
      "Train Epoch: 1 [69888/100000 (70%)]\tLoss: 0.914121\n",
      "Train Epoch: 1 [74880/100000 (75%)]\tLoss: 1.013077\n",
      "Train Epoch: 1 [79872/100000 (80%)]\tLoss: 1.060544\n",
      "Train Epoch: 1 [84864/100000 (85%)]\tLoss: 1.054164\n",
      "Train Epoch: 1 [89856/100000 (90%)]\tLoss: 0.921889\n",
      "Train Epoch: 1 [94848/100000 (95%)]\tLoss: 0.958260\n",
      "Train Epoch: 1 [99840/100000 (100%)]\tLoss: 1.034152\n",
      "epoch = 2\n",
      "Train Epoch: 2 [4992/100000 (5%)]\tLoss: 1.029039\n",
      "Train Epoch: 2 [9984/100000 (10%)]\tLoss: 1.059157\n",
      "Train Epoch: 2 [14976/100000 (15%)]\tLoss: 0.838006\n",
      "Train Epoch: 2 [19968/100000 (20%)]\tLoss: 1.010856\n",
      "Train Epoch: 2 [24960/100000 (25%)]\tLoss: 0.928736\n",
      "Train Epoch: 2 [29952/100000 (30%)]\tLoss: 1.000312\n",
      "Train Epoch: 2 [34944/100000 (35%)]\tLoss: 0.972124\n",
      "Train Epoch: 2 [39936/100000 (40%)]\tLoss: 0.974810\n",
      "Train Epoch: 2 [44928/100000 (45%)]\tLoss: 0.949903\n",
      "Train Epoch: 2 [49920/100000 (50%)]\tLoss: 1.016128\n",
      "Train Epoch: 2 [54912/100000 (55%)]\tLoss: 0.932848\n",
      "Train Epoch: 2 [59904/100000 (60%)]\tLoss: 1.047927\n",
      "Train Epoch: 2 [64896/100000 (65%)]\tLoss: 0.973389\n",
      "Train Epoch: 2 [69888/100000 (70%)]\tLoss: 0.898966\n",
      "Train Epoch: 2 [74880/100000 (75%)]\tLoss: 1.001468\n",
      "Train Epoch: 2 [79872/100000 (80%)]\tLoss: 1.013849\n",
      "Train Epoch: 2 [84864/100000 (85%)]\tLoss: 0.995030\n",
      "Train Epoch: 2 [89856/100000 (90%)]\tLoss: 0.766350\n",
      "Train Epoch: 2 [94848/100000 (95%)]\tLoss: 0.928375\n",
      "Train Epoch: 2 [99840/100000 (100%)]\tLoss: 0.924801\n",
      "epoch = 3\n",
      "Train Epoch: 3 [4992/100000 (5%)]\tLoss: 0.954475\n",
      "Train Epoch: 3 [9984/100000 (10%)]\tLoss: 0.945779\n",
      "Train Epoch: 3 [14976/100000 (15%)]\tLoss: 0.773404\n",
      "Train Epoch: 3 [19968/100000 (20%)]\tLoss: 0.935205\n",
      "Train Epoch: 3 [24960/100000 (25%)]\tLoss: 0.879408\n",
      "Train Epoch: 3 [29952/100000 (30%)]\tLoss: 0.924219\n",
      "Train Epoch: 3 [34944/100000 (35%)]\tLoss: 0.900154\n",
      "Train Epoch: 3 [39936/100000 (40%)]\tLoss: 0.990081\n",
      "Train Epoch: 3 [44928/100000 (45%)]\tLoss: 0.980083\n",
      "Train Epoch: 3 [49920/100000 (50%)]\tLoss: 0.990834\n",
      "Train Epoch: 3 [54912/100000 (55%)]\tLoss: 0.849814\n",
      "Train Epoch: 3 [59904/100000 (60%)]\tLoss: 0.941781\n",
      "Train Epoch: 3 [64896/100000 (65%)]\tLoss: 0.946587\n",
      "Train Epoch: 3 [69888/100000 (70%)]\tLoss: 0.907376\n",
      "Train Epoch: 3 [74880/100000 (75%)]\tLoss: 0.924687\n",
      "Train Epoch: 3 [79872/100000 (80%)]\tLoss: 0.942487\n",
      "Train Epoch: 3 [84864/100000 (85%)]\tLoss: 0.927496\n",
      "Train Epoch: 3 [89856/100000 (90%)]\tLoss: 0.743235\n",
      "Train Epoch: 3 [94848/100000 (95%)]\tLoss: 0.848018\n",
      "Train Epoch: 3 [99840/100000 (100%)]\tLoss: 0.923782\n",
      "epoch = 4\n",
      "Train Epoch: 4 [4992/100000 (5%)]\tLoss: 0.965717\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.912632\n",
      "Train Epoch: 4 [14976/100000 (15%)]\tLoss: 0.706611\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.837558\n",
      "Train Epoch: 4 [24960/100000 (25%)]\tLoss: 0.857667\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.903168\n",
      "Train Epoch: 4 [34944/100000 (35%)]\tLoss: 0.796989\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.901061\n",
      "Train Epoch: 4 [44928/100000 (45%)]\tLoss: 0.974309\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.863557\n",
      "Train Epoch: 4 [54912/100000 (55%)]\tLoss: 0.825319\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 1.004278\n",
      "Train Epoch: 4 [64896/100000 (65%)]\tLoss: 0.873778\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.851310\n",
      "Train Epoch: 4 [74880/100000 (75%)]\tLoss: 0.911166\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.823895\n",
      "Train Epoch: 4 [84864/100000 (85%)]\tLoss: 0.811021\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.703182\n",
      "Train Epoch: 4 [94848/100000 (95%)]\tLoss: 0.796855\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.874776\n",
      "Parameter Set: (0.01, 512, 128, 0.1, 'mul')\n",
      "epoch = 0\n",
      "Train Epoch: 0 [4992/100000 (5%)]\tLoss: 1.461143\n",
      "Train Epoch: 0 [9984/100000 (10%)]\tLoss: 1.318948\n",
      "Train Epoch: 0 [14976/100000 (15%)]\tLoss: 1.110089\n",
      "Train Epoch: 0 [19968/100000 (20%)]\tLoss: 1.184825\n",
      "Train Epoch: 0 [24960/100000 (25%)]\tLoss: 1.239814\n",
      "Train Epoch: 0 [29952/100000 (30%)]\tLoss: 1.124876\n",
      "Train Epoch: 0 [34944/100000 (35%)]\tLoss: 1.106469\n",
      "Train Epoch: 0 [39936/100000 (40%)]\tLoss: 1.100516\n",
      "Train Epoch: 0 [44928/100000 (45%)]\tLoss: 1.106248\n",
      "Train Epoch: 0 [49920/100000 (50%)]\tLoss: 1.077511\n",
      "Train Epoch: 0 [54912/100000 (55%)]\tLoss: 1.075385\n",
      "Train Epoch: 0 [59904/100000 (60%)]\tLoss: 1.051884\n",
      "Train Epoch: 0 [64896/100000 (65%)]\tLoss: 1.088277\n",
      "Train Epoch: 0 [69888/100000 (70%)]\tLoss: 1.116383\n",
      "Train Epoch: 0 [74880/100000 (75%)]\tLoss: 1.089620\n",
      "Train Epoch: 0 [79872/100000 (80%)]\tLoss: 1.112690\n",
      "Train Epoch: 0 [84864/100000 (85%)]\tLoss: 1.072906\n",
      "Train Epoch: 0 [89856/100000 (90%)]\tLoss: 1.052761\n",
      "Train Epoch: 0 [94848/100000 (95%)]\tLoss: 1.069058\n",
      "Train Epoch: 0 [99840/100000 (100%)]\tLoss: 1.079772\n",
      "epoch = 1\n",
      "Train Epoch: 1 [4992/100000 (5%)]\tLoss: 1.151267\n",
      "Train Epoch: 1 [9984/100000 (10%)]\tLoss: 1.032860\n",
      "Train Epoch: 1 [14976/100000 (15%)]\tLoss: 1.140453\n",
      "Train Epoch: 1 [19968/100000 (20%)]\tLoss: 1.076691\n",
      "Train Epoch: 1 [24960/100000 (25%)]\tLoss: 1.106179\n",
      "Train Epoch: 1 [29952/100000 (30%)]\tLoss: 1.051280\n",
      "Train Epoch: 1 [34944/100000 (35%)]\tLoss: 1.076361\n",
      "Train Epoch: 1 [39936/100000 (40%)]\tLoss: 0.995372\n",
      "Train Epoch: 1 [44928/100000 (45%)]\tLoss: 1.020822\n",
      "Train Epoch: 1 [49920/100000 (50%)]\tLoss: 1.005405\n",
      "Train Epoch: 1 [54912/100000 (55%)]\tLoss: 1.031771\n",
      "Train Epoch: 1 [59904/100000 (60%)]\tLoss: 1.055371\n",
      "Train Epoch: 1 [64896/100000 (65%)]\tLoss: 1.049479\n",
      "Train Epoch: 1 [69888/100000 (70%)]\tLoss: 1.079608\n",
      "Train Epoch: 1 [74880/100000 (75%)]\tLoss: 1.031519\n",
      "Train Epoch: 1 [79872/100000 (80%)]\tLoss: 1.055929\n",
      "Train Epoch: 1 [84864/100000 (85%)]\tLoss: 1.026425\n",
      "Train Epoch: 1 [89856/100000 (90%)]\tLoss: 1.013896\n",
      "Train Epoch: 1 [94848/100000 (95%)]\tLoss: 1.021722\n",
      "Train Epoch: 1 [99840/100000 (100%)]\tLoss: 1.062975\n",
      "epoch = 2\n",
      "Train Epoch: 2 [4992/100000 (5%)]\tLoss: 1.139196\n",
      "Train Epoch: 2 [9984/100000 (10%)]\tLoss: 1.003998\n",
      "Train Epoch: 2 [14976/100000 (15%)]\tLoss: 1.073850\n",
      "Train Epoch: 2 [19968/100000 (20%)]\tLoss: 0.990668\n",
      "Train Epoch: 2 [24960/100000 (25%)]\tLoss: 1.047034\n",
      "Train Epoch: 2 [29952/100000 (30%)]\tLoss: 0.971240\n",
      "Train Epoch: 2 [34944/100000 (35%)]\tLoss: 1.006752\n",
      "Train Epoch: 2 [39936/100000 (40%)]\tLoss: 0.935017\n",
      "Train Epoch: 2 [44928/100000 (45%)]\tLoss: 1.027027\n",
      "Train Epoch: 2 [49920/100000 (50%)]\tLoss: 1.019825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [54912/100000 (55%)]\tLoss: 0.991579\n",
      "Train Epoch: 2 [59904/100000 (60%)]\tLoss: 1.013509\n",
      "Train Epoch: 2 [64896/100000 (65%)]\tLoss: 1.021979\n",
      "Train Epoch: 2 [69888/100000 (70%)]\tLoss: 0.997882\n",
      "Train Epoch: 2 [74880/100000 (75%)]\tLoss: 1.011353\n",
      "Train Epoch: 2 [79872/100000 (80%)]\tLoss: 1.114938\n",
      "Train Epoch: 2 [84864/100000 (85%)]\tLoss: 1.008046\n",
      "Train Epoch: 2 [89856/100000 (90%)]\tLoss: 0.923051\n",
      "Train Epoch: 2 [94848/100000 (95%)]\tLoss: 0.928559\n",
      "Train Epoch: 2 [99840/100000 (100%)]\tLoss: 1.065084\n",
      "epoch = 3\n",
      "Train Epoch: 3 [4992/100000 (5%)]\tLoss: 1.027219\n",
      "Train Epoch: 3 [9984/100000 (10%)]\tLoss: 0.942804\n",
      "Train Epoch: 3 [14976/100000 (15%)]\tLoss: 0.976149\n",
      "Train Epoch: 3 [19968/100000 (20%)]\tLoss: 0.968343\n",
      "Train Epoch: 3 [24960/100000 (25%)]\tLoss: 0.999608\n",
      "Train Epoch: 3 [29952/100000 (30%)]\tLoss: 0.919451\n",
      "Train Epoch: 3 [34944/100000 (35%)]\tLoss: 0.967969\n",
      "Train Epoch: 3 [39936/100000 (40%)]\tLoss: 0.870451\n",
      "Train Epoch: 3 [44928/100000 (45%)]\tLoss: 0.973734\n",
      "Train Epoch: 3 [49920/100000 (50%)]\tLoss: 0.941505\n",
      "Train Epoch: 3 [54912/100000 (55%)]\tLoss: 0.864322\n",
      "Train Epoch: 3 [59904/100000 (60%)]\tLoss: 1.034926\n",
      "Train Epoch: 3 [64896/100000 (65%)]\tLoss: 1.039632\n",
      "Train Epoch: 3 [69888/100000 (70%)]\tLoss: 0.927218\n",
      "Train Epoch: 3 [74880/100000 (75%)]\tLoss: 0.909931\n",
      "Train Epoch: 3 [79872/100000 (80%)]\tLoss: 0.999437\n",
      "Train Epoch: 3 [84864/100000 (85%)]\tLoss: 0.933021\n",
      "Train Epoch: 3 [89856/100000 (90%)]\tLoss: 0.821704\n",
      "Train Epoch: 3 [94848/100000 (95%)]\tLoss: 0.914854\n",
      "Train Epoch: 3 [99840/100000 (100%)]\tLoss: 0.993341\n",
      "epoch = 4\n",
      "Train Epoch: 4 [4992/100000 (5%)]\tLoss: 0.930160\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.834578\n",
      "Train Epoch: 4 [14976/100000 (15%)]\tLoss: 0.943207\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.914603\n",
      "Train Epoch: 4 [24960/100000 (25%)]\tLoss: 0.922937\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.847704\n",
      "Train Epoch: 4 [34944/100000 (35%)]\tLoss: 0.939282\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.839893\n",
      "Train Epoch: 4 [44928/100000 (45%)]\tLoss: 0.913677\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.865077\n",
      "Train Epoch: 4 [54912/100000 (55%)]\tLoss: 0.888551\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 0.979480\n",
      "Train Epoch: 4 [64896/100000 (65%)]\tLoss: 0.990490\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.870073\n",
      "Train Epoch: 4 [74880/100000 (75%)]\tLoss: 0.926111\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.923954\n",
      "Train Epoch: 4 [84864/100000 (85%)]\tLoss: 0.944662\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.909329\n",
      "Train Epoch: 4 [94848/100000 (95%)]\tLoss: 0.805883\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.977299\n",
      "Parameter Set: (0.01, 512, 128, 0.1, 'concat')\n",
      "epoch = 0\n",
      "Train Epoch: 0 [4992/100000 (5%)]\tLoss: 1.163076\n",
      "Train Epoch: 0 [9984/100000 (10%)]\tLoss: 1.161178\n",
      "Train Epoch: 0 [14976/100000 (15%)]\tLoss: 1.067668\n",
      "Train Epoch: 0 [19968/100000 (20%)]\tLoss: 1.084270\n",
      "Train Epoch: 0 [24960/100000 (25%)]\tLoss: 1.030558\n",
      "Train Epoch: 0 [29952/100000 (30%)]\tLoss: 1.064187\n",
      "Train Epoch: 0 [34944/100000 (35%)]\tLoss: 1.053923\n",
      "Train Epoch: 0 [39936/100000 (40%)]\tLoss: 1.011263\n",
      "Train Epoch: 0 [44928/100000 (45%)]\tLoss: 1.062203\n",
      "Train Epoch: 0 [49920/100000 (50%)]\tLoss: 1.096074\n",
      "Train Epoch: 0 [54912/100000 (55%)]\tLoss: 1.090995\n",
      "Train Epoch: 0 [59904/100000 (60%)]\tLoss: 1.073288\n",
      "Train Epoch: 0 [64896/100000 (65%)]\tLoss: 0.982885\n",
      "Train Epoch: 0 [69888/100000 (70%)]\tLoss: 1.077869\n",
      "Train Epoch: 0 [74880/100000 (75%)]\tLoss: 1.001217\n",
      "Train Epoch: 0 [79872/100000 (80%)]\tLoss: 1.034033\n",
      "Train Epoch: 0 [84864/100000 (85%)]\tLoss: 1.050514\n",
      "Train Epoch: 0 [89856/100000 (90%)]\tLoss: 0.983559\n",
      "Train Epoch: 0 [94848/100000 (95%)]\tLoss: 1.007867\n",
      "Train Epoch: 0 [99840/100000 (100%)]\tLoss: 0.953888\n",
      "epoch = 1\n",
      "Train Epoch: 1 [4992/100000 (5%)]\tLoss: 1.043288\n",
      "Train Epoch: 1 [9984/100000 (10%)]\tLoss: 1.012261\n",
      "Train Epoch: 1 [14976/100000 (15%)]\tLoss: 0.976368\n",
      "Train Epoch: 1 [19968/100000 (20%)]\tLoss: 0.988312\n",
      "Train Epoch: 1 [24960/100000 (25%)]\tLoss: 1.012140\n",
      "Train Epoch: 1 [29952/100000 (30%)]\tLoss: 0.964943\n",
      "Train Epoch: 1 [34944/100000 (35%)]\tLoss: 0.972632\n",
      "Train Epoch: 1 [39936/100000 (40%)]\tLoss: 0.915864\n",
      "Train Epoch: 1 [44928/100000 (45%)]\tLoss: 0.936058\n",
      "Train Epoch: 1 [49920/100000 (50%)]\tLoss: 0.962606\n",
      "Train Epoch: 1 [54912/100000 (55%)]\tLoss: 0.898282\n",
      "Train Epoch: 1 [59904/100000 (60%)]\tLoss: 0.997335\n",
      "Train Epoch: 1 [64896/100000 (65%)]\tLoss: 0.934141\n",
      "Train Epoch: 1 [69888/100000 (70%)]\tLoss: 0.931676\n",
      "Train Epoch: 1 [74880/100000 (75%)]\tLoss: 0.921621\n",
      "Train Epoch: 1 [79872/100000 (80%)]\tLoss: 0.986066\n",
      "Train Epoch: 1 [84864/100000 (85%)]\tLoss: 0.981290\n",
      "Train Epoch: 1 [89856/100000 (90%)]\tLoss: 0.747080\n",
      "Train Epoch: 1 [94848/100000 (95%)]\tLoss: 0.912668\n",
      "Train Epoch: 1 [99840/100000 (100%)]\tLoss: 0.915526\n",
      "epoch = 2\n",
      "Train Epoch: 2 [4992/100000 (5%)]\tLoss: 0.981367\n",
      "Train Epoch: 2 [9984/100000 (10%)]\tLoss: 0.942570\n",
      "Train Epoch: 2 [14976/100000 (15%)]\tLoss: 0.942207\n",
      "Train Epoch: 2 [19968/100000 (20%)]\tLoss: 0.968675\n",
      "Train Epoch: 2 [24960/100000 (25%)]\tLoss: 0.895950\n",
      "Train Epoch: 2 [29952/100000 (30%)]\tLoss: 0.834629\n",
      "Train Epoch: 2 [34944/100000 (35%)]\tLoss: 0.904722\n",
      "Train Epoch: 2 [39936/100000 (40%)]\tLoss: 0.845476\n",
      "Train Epoch: 2 [44928/100000 (45%)]\tLoss: 0.875642\n",
      "Train Epoch: 2 [49920/100000 (50%)]\tLoss: 0.936366\n",
      "Train Epoch: 2 [54912/100000 (55%)]\tLoss: 0.827309\n",
      "Train Epoch: 2 [59904/100000 (60%)]\tLoss: 1.049429\n",
      "Train Epoch: 2 [64896/100000 (65%)]\tLoss: 0.892281\n",
      "Train Epoch: 2 [69888/100000 (70%)]\tLoss: 0.855572\n",
      "Train Epoch: 2 [74880/100000 (75%)]\tLoss: 0.862920\n",
      "Train Epoch: 2 [79872/100000 (80%)]\tLoss: 0.955127\n",
      "Train Epoch: 2 [84864/100000 (85%)]\tLoss: 0.959163\n",
      "Train Epoch: 2 [89856/100000 (90%)]\tLoss: 0.765363\n",
      "Train Epoch: 2 [94848/100000 (95%)]\tLoss: 0.886090\n",
      "Train Epoch: 2 [99840/100000 (100%)]\tLoss: 0.864195\n",
      "epoch = 3\n",
      "Train Epoch: 3 [4992/100000 (5%)]\tLoss: 0.955932\n",
      "Train Epoch: 3 [9984/100000 (10%)]\tLoss: 0.917721\n",
      "Train Epoch: 3 [14976/100000 (15%)]\tLoss: 0.882417\n",
      "Train Epoch: 3 [19968/100000 (20%)]\tLoss: 0.872083\n",
      "Train Epoch: 3 [24960/100000 (25%)]\tLoss: 0.899528\n",
      "Train Epoch: 3 [29952/100000 (30%)]\tLoss: 0.836255\n",
      "Train Epoch: 3 [34944/100000 (35%)]\tLoss: 0.891508\n",
      "Train Epoch: 3 [39936/100000 (40%)]\tLoss: 0.829097\n",
      "Train Epoch: 3 [44928/100000 (45%)]\tLoss: 0.885963\n",
      "Train Epoch: 3 [49920/100000 (50%)]\tLoss: 0.864781\n",
      "Train Epoch: 3 [54912/100000 (55%)]\tLoss: 0.777208\n",
      "Train Epoch: 3 [59904/100000 (60%)]\tLoss: 0.998677\n",
      "Train Epoch: 3 [64896/100000 (65%)]\tLoss: 0.906474\n",
      "Train Epoch: 3 [69888/100000 (70%)]\tLoss: 0.836520\n",
      "Train Epoch: 3 [74880/100000 (75%)]\tLoss: 0.772478\n",
      "Train Epoch: 3 [79872/100000 (80%)]\tLoss: 0.857775\n",
      "Train Epoch: 3 [84864/100000 (85%)]\tLoss: 0.967490\n",
      "Train Epoch: 3 [89856/100000 (90%)]\tLoss: 0.789944\n",
      "Train Epoch: 3 [94848/100000 (95%)]\tLoss: 0.844014\n",
      "Train Epoch: 3 [99840/100000 (100%)]\tLoss: 0.893043\n",
      "epoch = 4\n",
      "Train Epoch: 4 [4992/100000 (5%)]\tLoss: 0.842800\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.874389\n",
      "Train Epoch: 4 [14976/100000 (15%)]\tLoss: 0.833619\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.918894\n",
      "Train Epoch: 4 [24960/100000 (25%)]\tLoss: 0.792790\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.776128\n",
      "Train Epoch: 4 [34944/100000 (35%)]\tLoss: 0.841500\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.828415\n",
      "Train Epoch: 4 [44928/100000 (45%)]\tLoss: 0.838538\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.791281\n",
      "Train Epoch: 4 [54912/100000 (55%)]\tLoss: 0.813180\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 0.981190\n",
      "Train Epoch: 4 [64896/100000 (65%)]\tLoss: 0.843987\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.756418\n",
      "Train Epoch: 4 [74880/100000 (75%)]\tLoss: 0.739248\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.855544\n",
      "Train Epoch: 4 [84864/100000 (85%)]\tLoss: 0.930101\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.713589\n",
      "Train Epoch: 4 [94848/100000 (95%)]\tLoss: 0.849401\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.847799\n",
      "Parameter Set: (0.01, 512, 128, 0.1, 'subtract')\n",
      "epoch = 0\n",
      "Train Epoch: 0 [4992/100000 (5%)]\tLoss: 1.920878\n",
      "Train Epoch: 0 [9984/100000 (10%)]\tLoss: 1.388093\n",
      "Train Epoch: 0 [14976/100000 (15%)]\tLoss: 1.160740\n",
      "Train Epoch: 0 [19968/100000 (20%)]\tLoss: 1.724467\n",
      "Train Epoch: 0 [24960/100000 (25%)]\tLoss: 1.222505\n",
      "Train Epoch: 0 [29952/100000 (30%)]\tLoss: 1.422199\n",
      "Train Epoch: 0 [34944/100000 (35%)]\tLoss: 1.208845\n",
      "Train Epoch: 0 [39936/100000 (40%)]\tLoss: 1.214604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [44928/100000 (45%)]\tLoss: 1.139003\n",
      "Train Epoch: 0 [49920/100000 (50%)]\tLoss: 1.304288\n",
      "Train Epoch: 0 [54912/100000 (55%)]\tLoss: 1.279260\n",
      "Train Epoch: 0 [59904/100000 (60%)]\tLoss: 1.185284\n",
      "Train Epoch: 0 [64896/100000 (65%)]\tLoss: 1.163512\n",
      "Train Epoch: 0 [69888/100000 (70%)]\tLoss: 1.172989\n",
      "Train Epoch: 0 [74880/100000 (75%)]\tLoss: 0.986062\n",
      "Train Epoch: 0 [79872/100000 (80%)]\tLoss: 1.266138\n",
      "Train Epoch: 0 [84864/100000 (85%)]\tLoss: 1.184435\n",
      "Train Epoch: 0 [89856/100000 (90%)]\tLoss: 1.127719\n",
      "Train Epoch: 0 [94848/100000 (95%)]\tLoss: 0.989567\n",
      "Train Epoch: 0 [99840/100000 (100%)]\tLoss: 1.227998\n",
      "epoch = 1\n",
      "Train Epoch: 1 [4992/100000 (5%)]\tLoss: 1.026783\n",
      "Train Epoch: 1 [9984/100000 (10%)]\tLoss: 1.229349\n",
      "Train Epoch: 1 [14976/100000 (15%)]\tLoss: 1.056878\n",
      "Train Epoch: 1 [19968/100000 (20%)]\tLoss: 1.130006\n",
      "Train Epoch: 1 [24960/100000 (25%)]\tLoss: 1.042497\n",
      "Train Epoch: 1 [29952/100000 (30%)]\tLoss: 1.077495\n",
      "Train Epoch: 1 [34944/100000 (35%)]\tLoss: 1.078381\n",
      "Train Epoch: 1 [39936/100000 (40%)]\tLoss: 1.061881\n",
      "Train Epoch: 1 [44928/100000 (45%)]\tLoss: 0.966487\n",
      "Train Epoch: 1 [49920/100000 (50%)]\tLoss: 1.121631\n",
      "Train Epoch: 1 [54912/100000 (55%)]\tLoss: 1.037349\n",
      "Train Epoch: 1 [59904/100000 (60%)]\tLoss: 1.210566\n",
      "Train Epoch: 1 [64896/100000 (65%)]\tLoss: 1.014927\n",
      "Train Epoch: 1 [69888/100000 (70%)]\tLoss: 0.995961\n",
      "Train Epoch: 1 [74880/100000 (75%)]\tLoss: 1.016923\n",
      "Train Epoch: 1 [79872/100000 (80%)]\tLoss: 1.036987\n",
      "Train Epoch: 1 [84864/100000 (85%)]\tLoss: 1.114891\n",
      "Train Epoch: 1 [89856/100000 (90%)]\tLoss: 0.899743\n",
      "Train Epoch: 1 [94848/100000 (95%)]\tLoss: 0.953409\n",
      "Train Epoch: 1 [99840/100000 (100%)]\tLoss: 0.985277\n",
      "epoch = 2\n",
      "Train Epoch: 2 [4992/100000 (5%)]\tLoss: 1.091069\n",
      "Train Epoch: 2 [9984/100000 (10%)]\tLoss: 0.955623\n",
      "Train Epoch: 2 [14976/100000 (15%)]\tLoss: 0.925183\n",
      "Train Epoch: 2 [19968/100000 (20%)]\tLoss: 1.028681\n",
      "Train Epoch: 2 [24960/100000 (25%)]\tLoss: 0.979258\n",
      "Train Epoch: 2 [29952/100000 (30%)]\tLoss: 1.058922\n",
      "Train Epoch: 2 [34944/100000 (35%)]\tLoss: 0.976732\n",
      "Train Epoch: 2 [39936/100000 (40%)]\tLoss: 1.041842\n",
      "Train Epoch: 2 [44928/100000 (45%)]\tLoss: 1.029051\n",
      "Train Epoch: 2 [49920/100000 (50%)]\tLoss: 1.057309\n",
      "Train Epoch: 2 [54912/100000 (55%)]\tLoss: 0.962530\n",
      "Train Epoch: 2 [59904/100000 (60%)]\tLoss: 1.101552\n",
      "Train Epoch: 2 [64896/100000 (65%)]\tLoss: 0.960835\n",
      "Train Epoch: 2 [69888/100000 (70%)]\tLoss: 0.869420\n",
      "Train Epoch: 2 [74880/100000 (75%)]\tLoss: 0.942700\n",
      "Train Epoch: 2 [79872/100000 (80%)]\tLoss: 1.061828\n",
      "Train Epoch: 2 [84864/100000 (85%)]\tLoss: 0.939442\n",
      "Train Epoch: 2 [89856/100000 (90%)]\tLoss: 0.802045\n",
      "Train Epoch: 2 [94848/100000 (95%)]\tLoss: 0.955284\n",
      "Train Epoch: 2 [99840/100000 (100%)]\tLoss: 0.953442\n",
      "epoch = 3\n",
      "Train Epoch: 3 [4992/100000 (5%)]\tLoss: 1.030465\n",
      "Train Epoch: 3 [9984/100000 (10%)]\tLoss: 0.946983\n",
      "Train Epoch: 3 [14976/100000 (15%)]\tLoss: 0.804248\n",
      "Train Epoch: 3 [19968/100000 (20%)]\tLoss: 0.934790\n",
      "Train Epoch: 3 [24960/100000 (25%)]\tLoss: 0.860115\n",
      "Train Epoch: 3 [29952/100000 (30%)]\tLoss: 0.955797\n",
      "Train Epoch: 3 [34944/100000 (35%)]\tLoss: 0.906773\n",
      "Train Epoch: 3 [39936/100000 (40%)]\tLoss: 0.907205\n",
      "Train Epoch: 3 [44928/100000 (45%)]\tLoss: 1.000435\n",
      "Train Epoch: 3 [49920/100000 (50%)]\tLoss: 0.999944\n",
      "Train Epoch: 3 [54912/100000 (55%)]\tLoss: 0.888035\n",
      "Train Epoch: 3 [59904/100000 (60%)]\tLoss: 1.011921\n",
      "Train Epoch: 3 [64896/100000 (65%)]\tLoss: 0.948395\n",
      "Train Epoch: 3 [69888/100000 (70%)]\tLoss: 0.840767\n",
      "Train Epoch: 3 [74880/100000 (75%)]\tLoss: 0.873969\n",
      "Train Epoch: 3 [79872/100000 (80%)]\tLoss: 0.880657\n",
      "Train Epoch: 3 [84864/100000 (85%)]\tLoss: 0.907573\n",
      "Train Epoch: 3 [89856/100000 (90%)]\tLoss: 0.780340\n",
      "Train Epoch: 3 [94848/100000 (95%)]\tLoss: 0.904554\n",
      "Train Epoch: 3 [99840/100000 (100%)]\tLoss: 0.877160\n",
      "epoch = 4\n",
      "Train Epoch: 4 [4992/100000 (5%)]\tLoss: 0.932531\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.884834\n",
      "Train Epoch: 4 [14976/100000 (15%)]\tLoss: 0.841874\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.904605\n",
      "Train Epoch: 4 [24960/100000 (25%)]\tLoss: 0.865749\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.930917\n",
      "Train Epoch: 4 [34944/100000 (35%)]\tLoss: 0.835516\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.936712\n",
      "Train Epoch: 4 [44928/100000 (45%)]\tLoss: 0.888652\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.894631\n",
      "Train Epoch: 4 [54912/100000 (55%)]\tLoss: 0.856955\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 0.969117\n",
      "Train Epoch: 4 [64896/100000 (65%)]\tLoss: 0.886144\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.844990\n",
      "Train Epoch: 4 [74880/100000 (75%)]\tLoss: 0.816337\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.845620\n",
      "Train Epoch: 4 [84864/100000 (85%)]\tLoss: 0.803724\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.728236\n",
      "Train Epoch: 4 [94848/100000 (95%)]\tLoss: 0.840718\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.864139\n"
     ]
    }
   ],
   "source": [
    "param_losses = {}\n",
    "\n",
    "for param_set in params:\n",
    "    print (\"Parameter Set: \"+str(param_set))\n",
    "    ## INITIALIZE VALIDATION ACCURACY LIST\n",
    "    param_losses[param_set] = []\n",
    "    vectors = table_lookup\n",
    "    weights_init = init_embedding_weights(vectors, \n",
    "                                     token2id_wiki, \n",
    "                                     id2token_wiki,\n",
    "                                     embedding_size = 300)\n",
    "\n",
    "    RNN = biGRU(hidden_size=param_set[1],\n",
    "                num_layers=1,\n",
    "              percent_dropout = param_set[3],\n",
    "              embedding_weights = weights_init,\n",
    "              vocab_size=wiki_embed_table.size(0),\n",
    "              interaction_type=param_set[4],\n",
    "              input_size=300).to(device)\n",
    "    \n",
    "    linear_model = Linear_Layers(hidden_size = param_set[1],\n",
    "                 hidden_size_2 = param_set[2],\n",
    "                 percent_dropout = param_set[3],\n",
    "                 interaction_type=param_set[4],\n",
    "                 classes=3,\n",
    "                 input_size=300).to(device)\n",
    "\n",
    "    training_accuracy = []\n",
    "    validation_accuracy = []\n",
    "    num_epochs = 5\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print (\"epoch = \"+str(epoch))\n",
    "\n",
    "        loss_train = train(RNN, \n",
    "                           linear_model,\n",
    "                           DataLoader = snli_train_loader,\n",
    "                           criterion = nn.CrossEntropyLoss(),\n",
    "                           optimizer = torch.optim.Adam(list(RNN.parameters()) + \\\n",
    "                                                       list(linear_model.parameters()), \n",
    "                                                       lr=lr), \n",
    "                          epoch = epoch)\n",
    "\n",
    "        loss_val, val_preds, val_true = test(\n",
    "            RNN, \n",
    "            linear_model,\n",
    "            DataLoader = snli_val_loader,\n",
    "            criterion = nn.CrossEntropyLoss(reduction='sum'))\n",
    "\n",
    "        train_acc = accuracy(RNN, linear_model, snli_train_loader, \n",
    "                              nn.CrossEntropyLoss(reduction='sum'))\n",
    "        val_acc = accuracy(RNN, linear_model, \n",
    "                            snli_val_loader, nn.CrossEntropyLoss(reduction='sum'))\n",
    "\n",
    "        training_accuracy.append(train_acc)\n",
    "        validation_accuracy.append(val_acc)\n",
    "\n",
    "    # saving to pickle\n",
    "    param_losses[param_set] = validation_accuracy\n",
    "    pd.DataFrame(param_losses).to_csv(\"GRU_val_acc_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the best performing model from validation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "validation_gru = pd.DataFrame(pd.read_csv(\"GRU_val_acc_2.csv\", header=None)).drop(0,1)\n",
    "    \n",
    "validation_gru = validation_gru.T\n",
    "\n",
    "validation_gru.columns=[\"lr\",\"hidden\",\"hidden_2\",\n",
    "                                     \"dropout\",\"interaction\"]+[*range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>hidden</th>\n",
       "      <th>hidden_2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>interaction</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>concat</td>\n",
       "      <td>48.900002241134644</td>\n",
       "      <td>57.40000009536743</td>\n",
       "      <td>58.900004625320435</td>\n",
       "      <td>61.900001764297485</td>\n",
       "      <td>66.40000343322754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>mul</td>\n",
       "      <td>40.300002694129944</td>\n",
       "      <td>44.200003147125244</td>\n",
       "      <td>51.100003719329834</td>\n",
       "      <td>54.40000295639038</td>\n",
       "      <td>55.80000281333923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>subtract</td>\n",
       "      <td>44.40000355243683</td>\n",
       "      <td>51.30000114440918</td>\n",
       "      <td>57.50000476837158</td>\n",
       "      <td>59.800004959106445</td>\n",
       "      <td>64.20000195503235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>concat</td>\n",
       "      <td>49.2000013589859</td>\n",
       "      <td>58.40000510215759</td>\n",
       "      <td>60.100001096725464</td>\n",
       "      <td>62.5</td>\n",
       "      <td>63.600003719329834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>mul</td>\n",
       "      <td>38.30000162124634</td>\n",
       "      <td>44.10000145435333</td>\n",
       "      <td>52.000004053115845</td>\n",
       "      <td>54.90000247955322</td>\n",
       "      <td>58.60000252723694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>subtract</td>\n",
       "      <td>46.50000333786011</td>\n",
       "      <td>51.80000066757202</td>\n",
       "      <td>58.799999952316284</td>\n",
       "      <td>61.000001430511475</td>\n",
       "      <td>64.10000324249268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>concat</td>\n",
       "      <td>50.40000081062317</td>\n",
       "      <td>56.40000104904175</td>\n",
       "      <td>61.10000014305115</td>\n",
       "      <td>60.80000400543213</td>\n",
       "      <td>64.30000066757202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>mul</td>\n",
       "      <td>38.200002908706665</td>\n",
       "      <td>41.700002551078796</td>\n",
       "      <td>50.49999952316284</td>\n",
       "      <td>50.49999952316284</td>\n",
       "      <td>54.30000424385071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>subtract</td>\n",
       "      <td>45.6000030040741</td>\n",
       "      <td>52.900004386901855</td>\n",
       "      <td>56.49999976158142</td>\n",
       "      <td>60.00000238418579</td>\n",
       "      <td>64.10000324249268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>concat</td>\n",
       "      <td>48.500001430511475</td>\n",
       "      <td>56.2000036239624</td>\n",
       "      <td>58.799999952316284</td>\n",
       "      <td>62.10000514984131</td>\n",
       "      <td>64.20000195503235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.01</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>mul</td>\n",
       "      <td>38.600000739097595</td>\n",
       "      <td>48.100003600120544</td>\n",
       "      <td>51.70000195503235</td>\n",
       "      <td>55.20000457763672</td>\n",
       "      <td>58.500003814697266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.01</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>subtract</td>\n",
       "      <td>42.70000159740448</td>\n",
       "      <td>52.799999713897705</td>\n",
       "      <td>57.50000476837158</td>\n",
       "      <td>61.2000048160553</td>\n",
       "      <td>65.00000357627869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lr hidden hidden_2 dropout interaction                   1  \\\n",
       "1   0.001    512       64     0.1      concat  48.900002241134644   \n",
       "2   0.001    512       64     0.1         mul  40.300002694129944   \n",
       "3   0.001    512       64     0.1    subtract   44.40000355243683   \n",
       "4   0.001    512      128     0.1      concat    49.2000013589859   \n",
       "5   0.001    512      128     0.1         mul   38.30000162124634   \n",
       "6   0.001    512      128     0.1    subtract   46.50000333786011   \n",
       "7    0.01    512       64     0.1      concat   50.40000081062317   \n",
       "8    0.01    512       64     0.1         mul  38.200002908706665   \n",
       "9    0.01    512       64     0.1    subtract    45.6000030040741   \n",
       "10   0.01    512      128     0.1      concat  48.500001430511475   \n",
       "11   0.01    512      128     0.1         mul  38.600000739097595   \n",
       "12   0.01    512      128     0.1    subtract   42.70000159740448   \n",
       "\n",
       "                     2                   3                   4  \\\n",
       "1    57.40000009536743  58.900004625320435  61.900001764297485   \n",
       "2   44.200003147125244  51.100003719329834   54.40000295639038   \n",
       "3    51.30000114440918   57.50000476837158  59.800004959106445   \n",
       "4    58.40000510215759  60.100001096725464                62.5   \n",
       "5    44.10000145435333  52.000004053115845   54.90000247955322   \n",
       "6    51.80000066757202  58.799999952316284  61.000001430511475   \n",
       "7    56.40000104904175   61.10000014305115   60.80000400543213   \n",
       "8   41.700002551078796   50.49999952316284   50.49999952316284   \n",
       "9   52.900004386901855   56.49999976158142   60.00000238418579   \n",
       "10    56.2000036239624  58.799999952316284   62.10000514984131   \n",
       "11  48.100003600120544   51.70000195503235   55.20000457763672   \n",
       "12  52.799999713897705   57.50000476837158    61.2000048160553   \n",
       "\n",
       "                     5  \n",
       "1    66.40000343322754  \n",
       "2    55.80000281333923  \n",
       "3    64.20000195503235  \n",
       "4   63.600003719329834  \n",
       "5    58.60000252723694  \n",
       "6    64.10000324249268  \n",
       "7    64.30000066757202  \n",
       "8    54.30000424385071  \n",
       "9    64.10000324249268  \n",
       "10   64.20000195503235  \n",
       "11  58.500003814697266  \n",
       "12   65.00000357627869  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_gru[\"max_val_acc\"] = [max(validation_gru[[*range(1,6)]].\\\n",
    "                                     iloc[i]) for i in range(len(validation_gru))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]),)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array(validation_gru[\"max_val_acc\"])==max(validation_gru[\"max_val_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr                          0.001\n",
       "hidden                        512\n",
       "hidden_2                       64\n",
       "dropout                       0.1\n",
       "interaction                concat\n",
       "1              48.900002241134644\n",
       "2               57.40000009536743\n",
       "3              58.900004625320435\n",
       "4              61.900001764297485\n",
       "5               66.40000343322754\n",
       "max_val_acc     66.40000343322754\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_gru.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_series = validation_gru.iloc[0]\n",
    "\n",
    "hid = int(best_series[\"hidden\"])\n",
    "hid2 = int(best_series[\"hidden_2\"])\n",
    "drop = float(best_series[\"dropout\"])\n",
    "inter = best_series[\"interaction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = table_lookup\n",
    "weights_init = init_embedding_weights(vectors, \n",
    "                                 token2id_wiki, \n",
    "                                 id2token_wiki,\n",
    "                                 embedding_size = 300)\n",
    "\n",
    "RNN = biGRU(hidden_size=hid,\n",
    "            num_layers=1,\n",
    "          percent_dropout = drop,\n",
    "          embedding_weights = weights_init,\n",
    "          vocab_size=wiki_embed_table.size(0),\n",
    "          interaction_type=inter,\n",
    "          input_size=300).to(device)\n",
    "\n",
    "linear_model = Linear_Layers(hidden_size = hid,\n",
    "             hidden_size_2 = hid2,\n",
    "             percent_dropout = drop,\n",
    "             interaction_type=inter,\n",
    "             classes=3,\n",
    "             input_size=300).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Government"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [4992/100000 (5%)]\tLoss: 0.862368\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.849433\n",
      "Train Epoch: 4 [14976/100000 (15%)]\tLoss: 0.795001\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.825773\n",
      "Train Epoch: 4 [24960/100000 (25%)]\tLoss: 0.815622\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.702768\n",
      "Train Epoch: 4 [34944/100000 (35%)]\tLoss: 0.780505\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.845856\n",
      "Train Epoch: 4 [44928/100000 (45%)]\tLoss: 0.863767\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.827014\n",
      "Train Epoch: 4 [54912/100000 (55%)]\tLoss: 0.756672\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 0.994307\n",
      "Train Epoch: 4 [64896/100000 (65%)]\tLoss: 0.826789\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.778466\n",
      "Train Epoch: 4 [74880/100000 (75%)]\tLoss: 0.744746\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.836529\n",
      "Train Epoch: 4 [84864/100000 (85%)]\tLoss: 0.927523\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.657352\n",
      "Train Epoch: 4 [94848/100000 (95%)]\tLoss: 0.834709\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.827976\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-b6aec4796e99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mlinear_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mDataLoader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnli_val_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             criterion = nn.CrossEntropyLoss(reduction='sum'))\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"test_preds = \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-ce43943353dc>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(RNN, Linear_Classifier, DataLoader, criterion)\u001b[0m\n\u001b[1;32m     61\u001b[0m         for batch_idx, (sentence1, s1_original, sentence1_lengths, \n\u001b[1;32m     62\u001b[0m                     sentence2, s2_original, sentence2_lengths, labels)\\\n\u001b[0;32m---> 63\u001b[0;31m                     \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1_original\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-fe1a65dd42ab>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_sentence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtoken2id_wiki\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mid_s2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken2id_wiki\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0moriginal_s2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=32\n",
    "\n",
    "mnli_val_dataset = MNLIDataset(mnli_val_government, \n",
    "                               max_sentence_length=MAX_SENTENCE_LENGTH)\n",
    "mnli_val_loader = torch.utils.data.DataLoader(dataset=mnli_val_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: mnli_func(x, max_sentence_length),\n",
    "                             shuffle=False)\n",
    "\n",
    "loss_train = train(RNN,linear_model,\n",
    "                   DataLoader = snli_train_loader, ### train on SNLI\n",
    "                   criterion = nn.CrossEntropyLoss(),\n",
    "                   optimizer = torch.optim.Adam(list(RNN.parameters()) + \\\n",
    "                                                       list(linear_model.parameters()), \n",
    "                                                       lr=lr), \n",
    "                          epoch = epoch)\n",
    "\n",
    "loss_val, test_preds, test_true = test(\n",
    "            RNN, \n",
    "            linear_model,\n",
    "            DataLoader = mnli_val_loader,\n",
    "            criterion = nn.CrossEntropyLoss(reduction='sum'))\n",
    "\n",
    "print (\"test_preds = \"+str(test_preds))\n",
    "print (\"test_true = \"+str(test_true))\n",
    "\n",
    "test_acc = accuracy(RNN, linear_model, \n",
    "                            mnli_val_loader, nn.CrossEntropyLoss(reduction='sum'))\n",
    "\n",
    "print (\"Test accuracy = \"+ str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Telephone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [4992/100000 (5%)]\tLoss: 0.869442\n",
      "Train Epoch: 4 [9984/100000 (10%)]\tLoss: 0.830257\n",
      "Train Epoch: 4 [14976/100000 (15%)]\tLoss: 0.787680\n",
      "Train Epoch: 4 [19968/100000 (20%)]\tLoss: 0.786367\n",
      "Train Epoch: 4 [24960/100000 (25%)]\tLoss: 0.805326\n",
      "Train Epoch: 4 [29952/100000 (30%)]\tLoss: 0.714760\n",
      "Train Epoch: 4 [34944/100000 (35%)]\tLoss: 0.754466\n",
      "Train Epoch: 4 [39936/100000 (40%)]\tLoss: 0.826290\n",
      "Train Epoch: 4 [44928/100000 (45%)]\tLoss: 0.811950\n",
      "Train Epoch: 4 [49920/100000 (50%)]\tLoss: 0.801244\n",
      "Train Epoch: 4 [54912/100000 (55%)]\tLoss: 0.697598\n",
      "Train Epoch: 4 [59904/100000 (60%)]\tLoss: 0.947630\n",
      "Train Epoch: 4 [64896/100000 (65%)]\tLoss: 0.814013\n",
      "Train Epoch: 4 [69888/100000 (70%)]\tLoss: 0.758719\n",
      "Train Epoch: 4 [74880/100000 (75%)]\tLoss: 0.769580\n",
      "Train Epoch: 4 [79872/100000 (80%)]\tLoss: 0.752309\n",
      "Train Epoch: 4 [84864/100000 (85%)]\tLoss: 0.873225\n",
      "Train Epoch: 4 [89856/100000 (90%)]\tLoss: 0.670663\n",
      "Train Epoch: 4 [94848/100000 (95%)]\tLoss: 0.814725\n",
      "Train Epoch: 4 [99840/100000 (100%)]\tLoss: 0.796467\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=len(mnli_val_telephone)\n",
    "\n",
    "mnli_val_dataset = MNLIDataset(mnli_val_telephone, \n",
    "                               max_sentence_length=MAX_SENTENCE_LENGTH)\n",
    "mnli_val_loader = torch.utils.data.DataLoader(dataset=mnli_val_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: mnli_func(x, max_sentence_length),\n",
    "                             shuffle=False)\n",
    "\n",
    "loss_train = train(RNN,linear_model,\n",
    "                   DataLoader = snli_train_loader, ### train on SNLI\n",
    "                   criterion = nn.CrossEntropyLoss(),\n",
    "                   optimizer = torch.optim.Adam(list(RNN.parameters()) + \\\n",
    "                                                       list(linear_model.parameters()), \n",
    "                                                       lr=lr), \n",
    "                          epoch = epoch)\n",
    "\n",
    "loss_val, test_preds, test_true = test(\n",
    "            RNN, \n",
    "            linear_model,\n",
    "            DataLoader = mnli_val_loader,\n",
    "            criterion = nn.CrossEntropyLoss(reduction='sum'))\n",
    "\n",
    "print (\"test_preds = \"+str(test_preds))\n",
    "print (\"test_true = \"+str(test_true))\n",
    "\n",
    "test_acc = accuracy(RNN, linear_model, \n",
    "                            mnli_val_loader, nn.CrossEntropyLoss(reduction='sum'))\n",
    "\n",
    "print (\"Test accuracy = \"+ str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=len(mnli_val_slate)\n",
    "\n",
    "mnli_val_dataset = MNLIDataset(mnli_val_slate, \n",
    "                               max_sentence_length=MAX_SENTENCE_LENGTH)\n",
    "mnli_val_loader = torch.utils.data.DataLoader(dataset=mnli_val_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: mnli_func(x, max_sentence_length),\n",
    "                             shuffle=False)\n",
    "\n",
    "loss_train = train(RNN,linear_model,\n",
    "                   DataLoader = snli_train_loader, ### train on SNLI\n",
    "                   criterion = nn.CrossEntropyLoss(),\n",
    "                   optimizer = torch.optim.Adam(list(RNN.parameters()) + \\\n",
    "                                                       list(linear_model.parameters()), \n",
    "                                                       lr=lr), \n",
    "                          epoch = epoch)\n",
    "\n",
    "loss_val, test_preds, test_true = test(\n",
    "            RNN, \n",
    "            linear_model,\n",
    "            DataLoader = mnli_val_loader,\n",
    "            criterion = nn.CrossEntropyLoss(reduction='sum'))\n",
    "\n",
    "print (\"test_preds = \"+str(test_preds))\n",
    "print (\"test_true = \"+str(test_true))\n",
    "\n",
    "test_acc = accuracy(RNN, linear_model, \n",
    "                            mnli_val_loader, nn.CrossEntropyLoss(reduction='sum'))\n",
    "\n",
    "print (\"Test accuracy = \"+ str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=len(mnli_val_fiction)\n",
    "\n",
    "mnli_val_dataset = MNLIDataset(mnli_val_fiction, \n",
    "                               max_sentence_length=MAX_SENTENCE_LENGTH)\n",
    "mnli_val_loader = torch.utils.data.DataLoader(dataset=mnli_val_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: mnli_func(x, max_sentence_length),\n",
    "                             shuffle=False)\n",
    "\n",
    "loss_train = train(RNN,linear_model,\n",
    "                   DataLoader = snli_train_loader, ### train on SNLI\n",
    "                   criterion = nn.CrossEntropyLoss(),\n",
    "                   optimizer = torch.optim.Adam(list(RNN.parameters()) + \\\n",
    "                                                       list(linear_model.parameters()), \n",
    "                                                       lr=lr), \n",
    "                          epoch = epoch)\n",
    "\n",
    "loss_val, test_preds, test_true = test(\n",
    "            RNN, \n",
    "            linear_model,\n",
    "            DataLoader = mnli_val_loader,\n",
    "            criterion = nn.CrossEntropyLoss(reduction='sum'))\n",
    "\n",
    "print (\"test_preds = \"+str(test_preds))\n",
    "print (\"test_true = \"+str(test_true))\n",
    "\n",
    "test_acc = accuracy(RNN, linear_model, \n",
    "                            mnli_val_loader, nn.CrossEntropyLoss(reduction='sum'))\n",
    "\n",
    "print (\"Test accuracy = \"+ str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=len(mnli_val_travel)\n",
    "\n",
    "mnli_val_dataset = MNLIDataset(mnli_val_travel, \n",
    "                               max_sentence_length=MAX_SENTENCE_LENGTH)\n",
    "mnli_val_loader = torch.utils.data.DataLoader(dataset=mnli_val_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: mnli_func(x, max_sentence_length),\n",
    "                             shuffle=False)\n",
    "\n",
    "loss_train = train(RNN,linear_model,\n",
    "                   DataLoader = snli_train_loader, ### train on SNLI\n",
    "                   criterion = nn.CrossEntropyLoss(),\n",
    "                   optimizer = torch.optim.Adam(list(RNN.parameters()) + \\\n",
    "                                                       list(linear_model.parameters()), \n",
    "                                                       lr=lr), \n",
    "                          epoch = epoch)\n",
    "\n",
    "loss_val, test_preds, test_true = test(\n",
    "            RNN, \n",
    "            linear_model,\n",
    "            DataLoader = mnli_val_loader,\n",
    "            criterion = nn.CrossEntropyLoss(reduction='sum'))\n",
    "\n",
    "print (\"test_preds = \"+str(test_preds))\n",
    "print (\"test_true = \"+str(test_true))\n",
    "\n",
    "test_acc = accuracy(RNN, linear_model, \n",
    "                            mnli_val_loader, nn.CrossEntropyLoss(reduction='sum'))\n",
    "\n",
    "print (\"Test accuracy = \"+ str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
