{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"tocheading\">Table of Contents</h2>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import random\n",
    "import spacy\n",
    "import csv\n",
    "import string\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Upload & Preprocessing\n",
    "The datasets provided are already tokenized. Thus, without running the data through a tokenizer, we use pretrained word embeddings (e.g. fast-Text) to embed the tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Vectors\n",
    "\n",
    "The web page for recommended word vector sets can be found here: https://fasttext.cc/docs/en/english-vectors.html wiki-news-300d-1M.vec from Mikolov et al (2018, Advances in Pre-Training Distributed Word Representations) 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens) is used in this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', \n",
    "                  newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        ## convert all maps to lists\n",
    "        data[tokens[0]] = [*map(float, tokens[1:])]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the wiki word vectors\n",
    "fname = \"wiki-news-300d-1M.vec\"\n",
    "word_vectors = load_vectors(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab_tokens = [*word_vectors.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique tokens in the wiki news English vectors is 999994\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of unique tokens in the wiki news English vectors is \" + str(len(all_vocab_tokens) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from Vocab Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_df = pd.DataFrame(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_df = word_vector_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_lookup = np.array(word_vector_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_vocab(table_df):\n",
    "    \n",
    "    token_array = np.array([*table_df.index])\n",
    "    num_index_array = np.array([*range(table_df.shape[0])])\n",
    "    \n",
    "    token2id = {}\n",
    "    id2token = {}\n",
    "    for i in [*range(len(token_array))]:\n",
    "        token2id[token_array[i]] = num_index_array[i]\n",
    "        id2token[num_index_array[i]] = token_array[i]\n",
    "\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id_wiki, id2token_wiki = index_vocab(word_vector_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check for table correctness!__\n",
    "\n",
    "Do token2id and id2token match each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93141"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id_wiki[\"Alberto\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alberto'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2token_wiki[93141]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the table fit the initial word vector vocab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(word_vectors[\"Alberto\"] == table_lookup[93141])==True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1: SNLI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\"entailment\":1,\n",
    "             \"neutral\":0,\n",
    "             \"contradiction\":-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train = pd.read_table(\"data/snli_train.tsv\")\n",
    "snli_val = pd.read_table(\"data/snli_val.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get tokenized training data\n",
    "snli_train[\"sentence1\"] = snli_train[\"sentence1\"].apply(lambda x: x.split(\" \"))\n",
    "snli_train[\"sentence2\"] = snli_train[\"sentence2\"].apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get labels\n",
    "snli_train[\"label_num\"] = snli_train[\"label\"].apply(lambda x: label_dict[x])\n",
    "snli_val[\"label_num\"] = snli_val[\"label\"].apply(lambda x: label_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get tokenized validation data\n",
    "snli_val[\"sentence1\"] = snli_val[\"sentence1\"].apply(lambda x: x.split(\" \"))\n",
    "snli_val[\"sentence2\"] = snli_val[\"sentence2\"].apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get label arrays\n",
    "snli_train_labels = np.array(snli_train[\"label_num\"])\n",
    "snli_val_labels = np.array(snli_val[\"label_num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Three, women, on, a, stage, ,, one, wearing, ...</td>\n",
       "      <td>[There, are, two, women, standing, on, the, st...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Four, people, sit, on, a, subway, two, read, ...</td>\n",
       "      <td>[Multiple, people, are, on, a, subway, togethe...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bicycles, stationed, while, a, group, of, peo...</td>\n",
       "      <td>[People, get, together, near, a, stand, of, bi...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  [Three, women, on, a, stage, ,, one, wearing, ...   \n",
       "1  [Four, people, sit, on, a, subway, two, read, ...   \n",
       "2  [bicycles, stationed, while, a, group, of, peo...   \n",
       "\n",
       "                                           sentence2          label  label_num  \n",
       "0  [There, are, two, women, standing, on, the, st...  contradiction         -1  \n",
       "1  [Multiple, people, are, on, a, subway, togethe...     entailment          1  \n",
       "2  [People, get, together, near, a, stand, of, bi...     entailment          1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_val.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: MultiNLI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train = pd.read_table(\"data/mnli_train.tsv\")\n",
    "mnli_val = pd.read_table(\"data/mnli_val.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and now that was in fifty one that 's forty ye...</td>\n",
       "      <td>It was already a problem forty years ago but n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jon could smell baked bread on the air and his...</td>\n",
       "      <td>Jon smelt food in the air and was hungry .</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it will be like Italian basketball with the uh...</td>\n",
       "      <td>This type of Italian basketball is nothing lik...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  and now that was in fifty one that 's forty ye...   \n",
       "1  Jon could smell baked bread on the air and his...   \n",
       "2  it will be like Italian basketball with the uh...   \n",
       "\n",
       "                                           sentence2          label      genre  \n",
       "0  It was already a problem forty years ago but n...        neutral  telephone  \n",
       "1         Jon smelt food in the air and was hungry .        neutral    fiction  \n",
       "2  This type of Italian basketball is nothing lik...  contradiction  telephone  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "telephone     4270\n",
       "slate         4026\n",
       "travel        3985\n",
       "government    3883\n",
       "fiction       3836\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get tokenized training data\n",
    "mnli_train[\"sentence1\"] = mnli_train[\"sentence1\"].apply(lambda x: x.split(\" \"))\n",
    "mnli_train[\"sentence2\"] = mnli_train[\"sentence2\"].apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get tokenized validation data\n",
    "mnli_val[\"sentence1\"] = mnli_val[\"sentence1\"].apply(lambda x: x.split(\" \"))\n",
    "mnli_val[\"sentence2\"] = mnli_val[\"sentence2\"].apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get labels\n",
    "mnli_train[\"label_num\"] = mnli_train[\"label\"].apply(lambda x: label_dict[x])\n",
    "mnli_val[\"label_num\"] = mnli_val[\"label\"].apply(lambda x: label_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get train and val datasets for each __MNLI genre__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## telephone\n",
    "mnli_train_telephone = mnli_train[mnli_train[\"genre\"]==\"telephone\"]\n",
    "mnli_val_telephone = mnli_val[mnli_val[\"genre\"]==\"telephone\"]\n",
    "## slate\n",
    "mnli_train_slate = mnli_train[mnli_train[\"genre\"]==\"slate\"]\n",
    "mnli_val_slate = mnli_val[mnli_val[\"genre\"]==\"slate\"]\n",
    "## travel\n",
    "mnli_train_travel = mnli_train[mnli_train[\"genre\"]==\"travel\"]\n",
    "mnli_val_travel = mnli_val[mnli_val[\"genre\"]==\"travel\"]\n",
    "## government\n",
    "mnli_train_government = mnli_train[mnli_train[\"genre\"]==\"government\"]\n",
    "mnli_val_government = mnli_val[mnli_val[\"genre\"]==\"government\"]\n",
    "## fiction\n",
    "mnli_train_fiction = mnli_train[mnli_train[\"genre\"]==\"fiction\"]\n",
    "mnli_val_fiction = mnli_val[mnli_val[\"genre\"]==\"fiction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get label arrays for each train and val dataset\n",
    "\n",
    "## whole MNLI dataset\n",
    "mnli_train_labels = np.array(mnli_train[\"label_num\"])\n",
    "mnli_val_labels = np.array(mnli_val[\"label_num\"])\n",
    "## telephone\n",
    "mnli_train_tel_labels = np.array(mnli_train_telephone[\"label_num\"])\n",
    "mnli_val_tel_labels = np.array(mnli_val_telephone[\"label_num\"])\n",
    "## slate\n",
    "mnli_train_slate_labels = np.array(mnli_train_slate[\"label_num\"])\n",
    "mnli_val_slate_labels = np.array(mnli_val_slate[\"label_num\"])\n",
    "## travel\n",
    "mnli_train_travel_labels = np.array(mnli_train_travel[\"label_num\"])\n",
    "mnli_val_travel_labels = np.array(mnli_val_travel[\"label_num\"])\n",
    "## gov\n",
    "mnli_train_gov_labels = np.array(mnli_train_government[\"label_num\"])\n",
    "mnli_val_gov_labels = np.array(mnli_val_government[\"label_num\"])\n",
    "## fiction\n",
    "mnli_train_fiction_labels = np.array(mnli_train_fiction[\"label_num\"])\n",
    "mnli_val_fiction_labels = np.array(mnli_val_fiction[\"label_num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## idx = token2id_wiki\n",
    "\n",
    "def token2index_dataset(tokens_data,idx_dict=None):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        ## get index list for each sentence.\n",
    "        index_list = [idx_dict[token] if token in \\\n",
    "                      idx_dict else idx_dict[\"unk\"] for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ I am getting the indices for Sentence 1 and Sentence 2 separately (not concatenating them at first from the beginning) since, in hyperparameter search I want to try more than one ways of interacting the hidden representations of the two sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get train and val indices for both datasets\n",
    "\n",
    "## SNLI\n",
    "snli_train_sentence1_indices = token2index_dataset([*snli_train[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "snli_train_sentence2_indices = token2index_dataset([*snli_train[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "snli_val_sentence1_indices = token2index_dataset([*snli_val[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "snli_val_sentence2_indices = token2index_dataset([*snli_val[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "\n",
    "## MNLI\n",
    "mnli_train_sentence1_indices = token2index_dataset([*mnli_train[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_sentence2_indices = token2index_dataset([*mnli_train[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_sentence1_indices = token2index_dataset([*mnli_val[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_sentence2_indices = token2index_dataset([*mnli_train[\"sentence2\"]],idx_dict=token2id_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENRES\n",
    "\n",
    "## telephone\n",
    "mnli_train_s1_tel_ix = token2index_dataset([*mnli_train_telephone[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_tel_ix = token2index_dataset([*mnli_train_telephone[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_tel_ix = token2index_dataset([*mnli_val_telephone[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_tel_ix = token2index_dataset([*mnli_val_telephone[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "## slate\n",
    "mnli_train_s1_slate_ix = token2index_dataset([*mnli_train_slate[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_slate_ix = token2index_dataset([*mnli_train_slate[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_slate_ix = token2index_dataset([*mnli_val_slate[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_slate_ix = token2index_dataset([*mnli_val_slate[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "## travel\n",
    "mnli_train_s1_travel_ix = token2index_dataset([*mnli_train_travel[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_travel_ix = token2index_dataset([*mnli_train_travel[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_travel_ix = token2index_dataset([*mnli_val_travel[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_travel_ix = token2index_dataset([*mnli_val_travel[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "## gov\n",
    "mnli_train_s1_gov_ix = token2index_dataset([*mnli_train_government[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_gov_ix = token2index_dataset([*mnli_train_government[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_gov_ix = token2index_dataset([*mnli_val_government[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_gov_ix = token2index_dataset([*mnli_val_government[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "## fiction\n",
    "mnli_train_s1_fiction_ix = token2index_dataset([*mnli_train_fiction[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_train_s2_fiction_ix = token2index_dataset([*mnli_train_fiction[\"sentence2\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s1_fiction_ix = token2index_dataset([*mnli_val_fiction[\"sentence1\"]],idx_dict=token2id_wiki)\n",
    "mnli_val_s2_fiction_ix = token2index_dataset([*mnli_val_fiction[\"sentence2\"]],idx_dict=token2id_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get concatenated tokens\n",
    "snli_train_concat_indices = [snli_train_sentence1_indices[i]+snli_train_sentence2_indices[i] for i in [*range(len(snli_train_sentence1_indices))]]\n",
    "snli_val_concat_indices = [snli_val_sentence1_indices[i]+snli_val_sentence2_indices[i] for i in [*range(len(snli_val_sentence1_indices))]]\n",
    "\n",
    "mnli_train_concat_indices = [mnli_train_sentence1_indices[i]+mnli_train_sentence2_indices[i] for i in [*range(len(mnli_train_sentence1_indices))]]\n",
    "mnli_val_concat_indices = [mnli_val_sentence1_indices[i]+mnli_val_sentence2_indices[i] for i in [*range(len(mnli_val_sentence1_indices))]]\n",
    "\n",
    "## GENRES\n",
    "\n",
    "## telephone\n",
    "mnli_train_tel_concat_indices = [mnli_train_s1_tel_ix[i]+mnli_train_s2_tel_ix[i] for i in [*range(len(mnli_train_s1_tel_ix))]]\n",
    "mnli_val_tel_concat_indices = [mnli_val_s1_tel_ix[i]+mnli_val_s2_tel_ix[i] for i in [*range(len(mnli_val_s1_tel_ix))]]\n",
    "## slate\n",
    "mnli_train_slate_concat_indices = [mnli_train_s1_slate_ix[i]+mnli_train_s2_slate_ix[i] for i in [*range(len(mnli_train_s2_slate_ix))]]\n",
    "mnli_val_slate_concat_indices = [mnli_val_s1_slate_ix[i]+mnli_val_s2_slate_ix[i] for i in [*range(len(mnli_val_s2_slate_ix))]]\n",
    "## travel\n",
    "mnli_train_travel_concat_indices = [mnli_train_s1_travel_ix[i]+mnli_train_s2_travel_ix[i] for i in [*range(len(mnli_train_s2_travel_ix))]]\n",
    "mnli_val_travel_concat_indices = [mnli_val_s1_travel_ix[i]+mnli_val_s2_travel_ix[i] for i in [*range(len(mnli_val_s2_travel_ix))]]\n",
    "## gov\n",
    "mnli_train_gov_concat_indices = [mnli_train_s1_gov_ix[i]+mnli_train_s2_gov_ix[i] for i in [*range(len(mnli_train_s2_gov_ix))]]\n",
    "mnli_val_gov_concat_indices = [mnli_val_s1_gov_ix[i]+mnli_val_s2_gov_ix[i] for i in [*range(len(mnli_val_s2_gov_ix))]]\n",
    "## fiction\n",
    "mnli_train_fiction_concat_indices = [mnli_train_s1_fiction_ix[i]+mnli_train_s2_fiction_ix[i] for i in [*range(len(mnli_train_s2_fiction_ix))]]\n",
    "mnli_val_fiction_concat_indices = [mnli_val_s1_fiction_ix[i]+mnli_val_s2_fiction_ix[i] for i in [*range(len(mnli_val_s2_fiction_ix))]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting training and validation set __labels__ (targets) for both datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SNLI\n",
    "snli_train_labels = np.array(snli_train[\"label_num\"])\n",
    "snli_val_labels = np.array(snli_val[\"label_num\"])\n",
    "\n",
    "## MNLI\n",
    "mnli_train_labels = np.array(mnli_train[\"label_num\"])\n",
    "mnli_val_labels = np.array(mnli_val[\"label_num\"])\n",
    "\n",
    "## GENRES\n",
    "\n",
    "## telephone\n",
    "mnli_train_tel_labels = np.array(mnli_train_telephone[\"label_num\"])\n",
    "mnli_val_tel_labels = np.array(mnli_val_telephone[\"label_num\"])\n",
    "## slate\n",
    "mnli_train_slate_labels = np.array(mnli_train_slate[\"label_num\"])\n",
    "mnli_val_slate_labels = np.array(mnli_val_slate[\"label_num\"])\n",
    "## travel\n",
    "mnli_train_travel_labels = np.array(mnli_train_travel[\"label_num\"])\n",
    "mnli_val_travel_labels = np.array(mnli_val_travel[\"label_num\"])\n",
    "## gov\n",
    "mnli_train_gov_labels = np.array(mnli_train_government[\"label_num\"])\n",
    "mnli_val_gov_labels = np.array(mnli_val_government[\"label_num\"])\n",
    "## fiction\n",
    "mnli_train_fiction_labels = np.array(mnli_train_fiction[\"label_num\"])\n",
    "mnli_val_fiction_labels = np.array(mnli_val_fiction[\"label_num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get pretrained word embeddings from the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embed(dataset_indices=None,\n",
    "                            method=\"concat\"):\n",
    "    \n",
    "    \"\"\"Takes as input dataset indices and uses the pretrained \n",
    "    word embedding matrix to look up the vector representation for each word,\n",
    "    then interacts the two sentences according to the specified method.\n",
    "    For ex: dataset_indices = snli_train_concat_indices\n",
    "            method = 'concat' \n",
    "            \n",
    "    Returns a numpy array of embeddings\n",
    "            \n",
    "    To be used instead of nn.Embedding in the model class\"\"\"\n",
    "    \n",
    "    sentence_matrixrep = []\n",
    "    \n",
    "    for i in [*range(len(dataset_indices))]:\n",
    "        sentence_rep = [table_lookup[x] for x in dataset_indices[i]]\n",
    "        sentence_matrixrep.append(sentence_rep)\n",
    "            \n",
    "    embeddings = np.array([np.array(n) for n in sentence_matrixrep])\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code taken from lab3\n",
    "\n",
    "## SNLI\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "snli_train_targets = snli_train_labels\n",
    "snli_val_targets = snli_val_labels\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SNLI_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def snli_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "        \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list)), \n",
    "            torch.LongTensor(length_list), \n",
    "            torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "snli_train_dataset = SNLI_Dataset(snli_train_concat_indices,snli_train_labels)\n",
    "snli_train_loader = torch.utils.data.DataLoader(dataset=snli_train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=snli_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "snli_val_dataset = SNLI_Dataset(snli_val_concat_indices, snli_val_labels)\n",
    "snli_val_loader = torch.utils.data.DataLoader(dataset=snli_val_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=snli_func,\n",
    "                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 76579, 822411, 971542,  ...,      0,      0,      0],\n",
       "         [396290, 794698, 973485,  ...,      0,      0,      0],\n",
       "         [ 76579, 980614, 976110,  ...,      0,      0,      0],\n",
       "         ...,\n",
       "         [ 76579, 822411, 794698,  ...,      0,      0,      0],\n",
       "         [ 76579, 822411, 672856,  ...,      0,      0,      0],\n",
       "         [ 76579, 762111,   2924,  ...,      0,      0,      0]]),\n",
       " tensor([22, 29, 17, 53, 18, 25, 19, 19, 27, 14, 26, 19, 29, 29, 15, 23, 22, 19,\n",
       "         27, 22, 28, 27, 24, 23, 28, 18, 35, 23, 29, 24, 16, 23]),\n",
       " tensor([ 1,  1,  0, -1,  0,  0,  0, -1, -1, -1,  0, -1,  1,  1, -1, -1, -1,  0,\n",
       "          1,  1, -1, -1,  0, -1,  1, -1,  0,  1,  0, -1, -1,  0])]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*snli_val_loader][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "\n",
    "## code taken from lab3\n",
    "## mnli\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "mnli_train_targets = mnli_train_labels\n",
    "mnli_val_targets = mnli_val_labels\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MNLI_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def mnli_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "        \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list)), \n",
    "            torch.LongTensor(length_list), \n",
    "            torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "mnli_train_dataset = MNLI_Dataset(mnli_train_concat_indices,mnli_train_labels)\n",
    "mnli_train_loader = torch.utils.data.DataLoader(dataset=mnli_train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=mnli_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "mnli_val_dataset = MNLI_Dataset(mnli_val_concat_indices, mnli_val_labels)\n",
    "mnli_val_loader = torch.utils.data.DataLoader(dataset=mnli_val_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=mnli_func,\n",
    "                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 16392, 471178, 853891,  ...,      0,      0,      0],\n",
       "         [687446, 945949,   2457,  ...,      0,      0,      0],\n",
       "         [975055, 766556, 944934,  ...,      0,      0,      0],\n",
       "         ...,\n",
       "         [607148,   2941, 698916,  ...,      0,      0,      0],\n",
       "         [594502, 679297, 679718,  ...,      0,      0,      0],\n",
       "         [980141, 979779, 979779,  ...,      0,      0,      0]]),\n",
       " tensor([42, 43, 46, 52, 67, 26, 14, 21, 24, 26, 42, 27, 38, 51, 14, 14, 19, 26,\n",
       "         30, 26, 16, 46, 45, 53, 24, 35, 57, 15, 29, 11, 18, 14]),\n",
       " tensor([ 0,  1, -1,  0,  0,  1,  0,  0,  1, -1,  1,  0,  1, -1, -1, -1, -1, -1,\n",
       "          0,  0,  0,  0, -1,  1, -1,  0,  1,  0,  1, -1,  0,  0])]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*mnli_train_loader][29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model\n",
    "\n",
    "The model is trained on SNLI training set. The best model is chosen using SNLI validation set, then the best model is evaluated on each genre in MultiNLI validation set. \n",
    "\n",
    "We will use an encoder (either a CNN or an RNN) to map each string of text (hypothesis and premise) to a fixed-dimension vector representation. We will interact the two hidden representations and output a 3-class soft- max. (To keep things simple, we will simply concatenate the two repre- sentations, and feed them through a network of 2 fully-connected layers.) For the encoder, we want the following:\n",
    "\n",
    "### Part 2.1: CNN\n",
    "For the CNN, a 2-layer 1-D convolutional network with ReLU activations will suffice. We can perform a max-pool at the end to compress the hidden representation into a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = sentence_embed()\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        embed = self.embedding(x)\n",
    "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = torch.sum(hidden, dim=1)\n",
    "        logits = self.linear(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Search\n",
    "\n",
    "The hyperparameters included in the hyperparameter search space are;\n",
    "\n",
    "- The size of the hidden dimension of the CNN and RNN,\n",
    "- The kernel size of the CNN,\n",
    "- Experiment with different ways of interacting the two encoded sentences (concatenation, element-wise multiplication, outer multiplication etc)\n",
    "- Regularization (e.g. weight decay, dropout).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2 RNN\n",
    "For the RNN, a single-layer, bi-directional GRU will suffice. We can take the last hidden state as the encoder output. (In the case of bi-directional, the last of each direction, although PyTorch takes care of this.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Search\n",
    "\n",
    "The hyperparameters included in the hyperparameter search space are;\n",
    "\n",
    "- The size of the hidden dimension of the CNN and RNN,\n",
    "- Experiment with different ways of interacting the two encoded sentences (concatenation, element-wise multiplication, outer multiplication etc)\n",
    "- Regularization (e.g. weight decay, dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
