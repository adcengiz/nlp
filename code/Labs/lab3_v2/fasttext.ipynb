{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from: http://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# To be used for pre-processing of data\n",
    "## from terminal run \"python -m spacy download en\"\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "# tokenizer = spacy.load(\"en\")\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "First, let's load the dataset from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "newsgroup_train = fetch_20newsgroups(subset='train')\n",
    "newsgroup_test = fetch_20newsgroups(subset='test') # we will use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 10000\n",
      "Val dataset size is 1314\n",
      "Test dataset size is 7532\n"
     ]
    }
   ],
   "source": [
    "train_split = 10000\n",
    "train_data = newsgroup_train.data[:train_split]\n",
    "train_targets = newsgroup_train.target[:train_split]\n",
    "\n",
    "val_data = newsgroup_train.data[train_split:]\n",
    "val_targets = newsgroup_train.target[train_split:]\n",
    "\n",
    "test_data = newsgroup_test.data\n",
    "test_targets = newsgroup_test.target\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasttext library takes a file as input and learn a classification model.\n",
    "The sentences in input file should be in this format: \"_ __label__ _[class] [Text]\" \n",
    "We will prepare the train file and test file in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_newsgroup_file(data, targets, outfile_name):\n",
    "    with open(outfile_name, 'w') as fout:\n",
    "        for i, sent in enumerate(data):\n",
    "            line = \"__label__\" + str(targets[i]) + \" \" + sent.replace('\\n', ' ') + \"\\n\"\n",
    "            fout.write(line)\n",
    "            \n",
    "\n",
    "create_newsgroup_file(train_data, train_targets, 'newsgroups.train') \n",
    "create_newsgroup_file(val_data, val_targets, 'newsgroups.val') \n",
    "create_newsgroup_file(test_data, test_targets, 'newsgroups.test') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check how the file we created look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__7 From: lerxst@wam.umd.edu (where's my thing) Subject: WHAT car is this!? Nntp-Posting-Host: rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: 15   I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2-door sports car, looked to be from the late 60s/ early 70s. It was called a Bricklin. The doors were really small. In addition, the front bumper was separate from the rest of the body. This is  all I know. If anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e-mail.  Thanks, - IL    ---- brought to you by your neighborhood Lerxst ----     \r\n",
      "__label__4 From: guykuo@carson.u.washington.edu (Guy Kuo) Subject: SI Clock Poll - Final Call Summary: Final call for SI clock reports Keywords: SI,acceleration,clock,upgrade Article-I.D.: shelley.1qvfo9INNc3s Organization: University of Washington Lines: 11 NNTP-Posting-Host: carson.u.washington.edu  A fair number of brave souls who upgraded their SI clock oscillator have shared their experiences for this poll. Please send a brief message detailing your experiences with the procedure. Top speed attained, CPU rated speed, add on cards and adapters, heat sinks, hour of usage per day, floppy disk functionality with 800 and 1.4 m floppies are especially requested.  I will be summarizing in the next two days, so please add to the network knowledge base if you have done the clock upgrade and haven't answered this poll. Thanks.  Guy Kuo <guykuo@u.washington.edu> \r\n"
     ]
    }
   ],
   "source": [
    "!head -2 newsgroups.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install FastText if you haven't! \n",
    "Use the following commands to install fasttext.\n",
    "```\n",
    "wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip\n",
    "unzip v0.1.0.zip\n",
    "cd fastText-0.1.0\n",
    "make\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start training the fasttext classifier, and check its performance on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  258366\n",
      "Number of labels: 20\n",
      "Progress: 100.0%  words/sec/thread: 2598655  lr: 0.000000  loss: 2.999971  eta: 0h0m lr: 0.092890  loss: 2.969951  eta: 0h0m 0h0m   loss: 3.016491  eta: 0h0m gress: 39.4%  words/sec/thread: 2570665  lr: 0.060622  loss: 3.016731  eta: 0h0m gress: 43.4%  words/sec/thread: 2576871  lr: 0.056588  loss: 3.016632  eta: 0h0m 53.4%  words/sec/thread: 2593548  lr: 0.046583  loss: 3.015920  eta: 0h0m gress: 70.0%  words/sec/thread: 2600160  lr: 0.030031  loss: 3.009035  eta: 0h0m m gress: 84.1%  words/sec/thread: 2601062  lr: 0.015860  loss: 3.004981  eta: 0h0m s: 84.4%  words/sec/thread: 2601617  lr: 0.015646  loss: 3.004905  eta: 0h0m gress: 88.4%  words/sec/thread: 2599276  lr: 0.011563  loss: 3.004934  eta: 0h0m gress: 92.8%  words/sec/thread: 2600100  lr: 0.007160  loss: 3.002179  eta: 0h0m %  words/sec/thread: 2600920  lr: 0.006514  loss: 3.001157  eta: 0h0m 1775  loss: 3.000236  eta: 0h0m \n"
     ]
    }
   ],
   "source": [
    "# Train fasttext\n",
    "!~/fastText-0.1.0/fastText supervised -input newsgroups.train -output model_newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t1314\r\n",
      "P@1\t0.105\r\n",
      "R@1\t0.105\r\n",
      "Number of examples: 1314\r\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it on validation set\n",
    "!~/fastText-0.1.0/fastText test model_newsgroup.bin newsgroups.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that FastText reports the precision and recall, not accuracy!  \n",
    "The **precision** is the number of correct labels among the labels predicted by fastText.  \n",
    "The **recall** is the number of labels that successfully were predicted, among all the real labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What a horrible model! Do some preprocessing to make it better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from lerxst@wam.umd.edu where 's my thing subject what car is this nntp posting host rac3.wam.umd.edu organization university of maryland college park lines 15    i was wondering if anyone out there could enlighten me on this car i saw the other day it was a 2-door sports car looked to be from the late 60s/ early 70s it was called a bricklin the doors were really small in addition the front bumper was separate from the rest of the body this is   all i know if anyone can tellme a model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please e mail   thanks il     ---- brought to you by your neighborhood lerxst ----\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sent(sent):\n",
    "    temp_sent = ' '.join(sent.split('\\n')) # remove line breaks as fasttext read each sample text as a line\n",
    "    tokens = tokenizer(temp_sent)\n",
    "    pos = [(tok.text, tok.pos_) for tok in tokens]\n",
    "    processed_toks = [tok.text.lower() for tok in tokens if (tok.text not in punctuations)]\n",
    "    \n",
    "    return ' '.join(processed_toks).strip() #[token.text.lower() for token in tokens]\n",
    "    \n",
    "    \n",
    "temp = preprocess_sent(train_data[0])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_newsgroup_file(data, targets, outfile_name):\n",
    "    with open(outfile_name, 'w') as fout:\n",
    "        for i, sent in enumerate(data):\n",
    "            proc_sent = preprocess_sent(sent)\n",
    "            line = \"__label__\" + str(targets[i]) + \" \" + proc_sent + \"\\n\"\n",
    "            fout.write(line)\n",
    "            \n",
    "create_newsgroup_file(train_data, train_targets, 'newsgroups.proc.train') \n",
    "create_newsgroup_file(val_data, val_targets, 'newsgroups.proc.val') \n",
    "create_newsgroup_file(test_data, test_targets, 'newsgroups.proc.test') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  134132\n",
      "Number of labels: 20\n",
      "Progress: 100.0%  words/sec/thread: 3110110  lr: 0.000000  loss: 2.971143  eta: 0h0m m gress: 27.5%  words/sec/thread: 3099689  lr: 0.072543  loss: 3.015791  eta: 0h0m 39.3%  words/sec/thread: 3100043  lr: 0.060684  loss: 3.016589  eta: 0h0m 0.048287  loss: 3.014113  eta: 0h0m s: 63.1%  words/sec/thread: 3096866  lr: 0.036949  loss: 3.006444  eta: 0h0m 63.5%  words/sec/thread: 3095917  lr: 0.036475  loss: 3.005187  eta: 0h0m gress: 73.6%  words/sec/thread: 3097775  lr: 0.026447  loss: 2.991425  eta: 0h0m gress: 79.9%  words/sec/thread: 3106669  lr: 0.020091  loss: 2.990266  eta: 0h0m \n"
     ]
    }
   ],
   "source": [
    "!~/fastText-0.1.0/fastText supervised -input newsgroups.proc.train -output model_newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t1314\r\n",
      "P@1\t0.123\r\n",
      "R@1\t0.123\r\n",
      "Number of examples: 1314\r\n"
     ]
    }
   ],
   "source": [
    "!~/fastText-0.1.0/fastText test model_newsgroup.bin newsgroups.proc.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see tiny improvement but still a bad model. Let's adjust the hyperparameters of the model.\n",
    "Fasttext library uses 5 training epochs by default, which is not enough for learning our data. \n",
    "Let's try adjusting the number of epoch to 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is important to note that the two models above aren't strictly comparable.\n",
    "Each model is randomly initialized at the beginning of the training. So, every time you re-train the model, you will notice that the precision and recall are different.\n",
    "In practice, it's a good idea to train the model with different initializations at least 5 times, and report the min, max, mean, and median stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  134132\n",
      "Number of labels: 20\n",
      "Progress: 100.0%  words/sec/thread: 3013444  lr: 0.000000  loss: 1.249545  eta: 0h0m 14m  words/sec/thread: 1613482  lr: 0.099983  loss: 2.841834  eta: 0h0m h0m  words/sec/thread: 3041454  lr: 0.097465  loss: 3.013645  eta: 0h0m  words/sec/thread: 3031102  lr: 0.096791  loss: 3.015508  eta: 0h0m  words/sec/thread: 3061186  lr: 0.094877  loss: 3.016323  eta: 0h0m  words/sec/thread: 3058415  lr: 0.093825  loss: 3.014880  eta: 0h0m d: 3079127  lr: 0.092948  loss: 3.014586  eta: 0h0m eta: 0h0m %  words/sec/thread: 3068266  lr: 0.091807  loss: 3.010029  eta: 0h0m  words/sec/thread: 3060623  lr: 0.090598  loss: 2.986003  eta: 0h0m h0m s: 10.1%  words/sec/thread: 3039995  lr: 0.089910  loss: 2.944014  eta: 0h0m gress: 13.0%  words/sec/thread: 3053428  lr: 0.087005  loss: 2.904153  eta: 0h0m 057  lr: 0.086289  loss: 2.892912  eta: 0h0m s: 14.3%  words/sec/thread: 3050586  lr: 0.085650  loss: 2.889325  eta: 0h0m gress: 17.9%  words/sec/thread: 3024397  lr: 0.082053  loss: 2.807367  eta: 0h0m s: 18.7%  words/sec/thread: 3019401  lr: 0.081320  loss: 2.770030  eta: 0h0m gress: 20.3%  words/sec/thread: 3026022  lr: 0.079696  loss: 2.754217  eta: 0h0m gress: 22.3%  words/sec/thread: 3027469  lr: 0.077708  loss: 2.708780  eta: 0h0m gress: 23.6%  words/sec/thread: 3013677  lr: 0.076367  loss: 2.596775  eta: 0h0m s: 25.8%  words/sec/thread: 3022687  lr: 0.074214  loss: 2.583964  eta: 0h0m /thread: 3019256  lr: 0.073767  loss: 2.533690  eta: 0h0m 26.3%  words/sec/thread: 3018464  lr: 0.073708  loss: 2.526399  eta: 0h0m gress: 27.2%  words/sec/thread: 3021664  lr: 0.072812  loss: 2.522621  eta: 0h0m 2.519579  eta: 0h0m ords/sec/thread: 3023592  lr: 0.071422  loss: 2.480662  eta: 0h0m gress: 29.1%  words/sec/thread: 3024225  lr: 0.070924  loss: 2.461531  eta: 0h0m 9462  loss: 2.441109  eta: 0h0m gress: 31.1%  words/sec/thread: 3029981  lr: 0.068859  loss: 2.421242  eta: 0h0m 1.9%  words/sec/thread: 3028934  lr: 0.068078  loss: 2.380014  eta: 0h0m gress: 32.9%  words/sec/thread: 3030010  lr: 0.067074  loss: 2.372479  eta: 0h0m gress: 33.4%  words/sec/thread: 3029121  lr: 0.066586  loss: 2.347518  eta: 0h0m   loss: 2.341464  eta: 0h0m   lr: 0.064004  loss: 2.277569  eta: 0h0m ords/sec/thread: 3028312  lr: 0.062356  loss: 2.245131  eta: 0h0m 8.0%  words/sec/thread: 3027499  lr: 0.062031  loss: 2.231095  eta: 0h0m 38.8%  words/sec/thread: 3025360  lr: 0.061154  loss: 2.184643  eta: 0h0m 0.060996  loss: 2.165959  eta: 0h0m gress: 40.6%  words/sec/thread: 3028192  lr: 0.059433  loss: 2.156762  eta: 0h0m a: 0h0m m h0m s: 46.0%  words/sec/thread: 3028681  lr: 0.054046  loss: 2.025070  eta: 0h0m   lr: 0.053636  loss: 1.989341  eta: 0h0m 47.5%  words/sec/thread: 3026557  lr: 0.052454  loss: 1.980965  eta: 0h0m gress: 48.3%  words/sec/thread: 3028719  lr: 0.051679  loss: 1.957853  eta: 0h0m gress: 48.8%  words/sec/thread: 3028205  lr: 0.051181  loss: 1.935436  eta: 0h0m s: 48.8%  words/sec/thread: 3027887  lr: 0.051155  loss: 1.934269  eta: 0h0m 0.3%  words/sec/thread: 3029114  lr: 0.049693  loss: 1.921742  eta: 0h0m  lr: 0.047909  loss: 1.887257  eta: 0h0m gress: 52.6%  words/sec/thread: 3027438  lr: 0.047421  loss: 1.866644  eta: 0h0m   lr: 0.045841  loss: 1.850424  eta: 0h0m gress: 57.9%  words/sec/thread: 3022489  lr: 0.042078  loss: 1.768796  eta: 0h0m gress: 59.4%  words/sec/thread: 3023202  lr: 0.040572  loss: 1.734290  eta: 0h0m gress: 60.9%  words/sec/thread: 3023902  lr: 0.039080  loss: 1.712614  eta: 0h0m 62.2%  words/sec/thread: 3023905  lr: 0.037838  loss: 1.702687  eta: 0h0m 0h0m %  words/sec/thread: 3024834  lr: 0.034933  loss: 1.656853  eta: 0h0m   loss: 1.644214  eta: 0h0m gress: 65.9%  words/sec/thread: 3025069  lr: 0.034142  loss: 1.637406  eta: 0h0m 1.632931  eta: 0h0m s: 67.7%  words/sec/thread: 3024522  lr: 0.032251  loss: 1.610320  eta: 0h0m gress: 68.5%  words/sec/thread: 3023938  lr: 0.031498  loss: 1.588329  eta: 0h0m gress: 69.3%  words/sec/thread: 3024119  lr: 0.030719  loss: 1.585336  eta: 0h0m 0.030050  loss: 1.582624  eta: 0h0m gress: 70.6%  words/sec/thread: 3024822  lr: 0.029370  loss: 1.564003  eta: 0h0m gress: 71.0%  words/sec/thread: 3024140  lr: 0.028998  loss: 1.547658  eta: 0h0m gress: 71.9%  words/sec/thread: 3024711  lr: 0.028092  loss: 1.546460  eta: 0h0m gress: 73.6%  words/sec/thread: 3024200  lr: 0.026415  loss: 1.514562  eta: 0h0m s: 74.7%  words/sec/thread: 3025843  lr: 0.025294  loss: 1.513621  eta: 0h0m m   words/sec/thread: 3027507  lr: 0.023920  loss: 1.489165  eta: 0h0m   words/sec/thread: 3026754  lr: 0.023477  loss: 1.475753  eta: 0h0m s: 78.4%  words/sec/thread: 3027808  lr: 0.021601  loss: 1.466311  eta: 0h0m gress: 78.9%  words/sec/thread: 3027520  lr: 0.021065  loss: 1.459285  eta: 0h0m a: 0h0m gress: 80.6%  words/sec/thread: 3027682  lr: 0.019397  loss: 1.427378  eta: 0h0m gress: 82.7%  words/sec/thread: 3028448  lr: 0.017294  loss: 1.411093  eta: 0h0m s: 83.1%  words/sec/thread: 3028516  lr: 0.016864  loss: 1.410365  eta: 0h0m gress: 88.8%  words/sec/thread: 3020466  lr: 0.011243  loss: 1.343337  eta: 0h0m   words/sec/thread: 3019624  lr: 0.010810  loss: 1.332519  eta: 0h0m 3019329  lr: 0.006638  loss: 1.300800  eta: 0h0m gress: 95.4%  words/sec/thread: 3017752  lr: 0.004567  loss: 1.281933  eta: 0h0m s: 95.9%  words/sec/thread: 3016864  lr: 0.004137  loss: 1.281267  eta: 0h0m 0.002136  loss: 1.260376  eta: 0h0m gress: 99.1%  words/sec/thread: 3013859  lr: 0.000928  loss: 1.254602  eta: 0h0m \n",
      "N\t1314\n",
      "P@1\t0.789\n",
      "R@1\t0.789\n",
      "Number of examples: 1314\n"
     ]
    }
   ],
   "source": [
    "!~/fastText-0.1.0/fastText supervised -input newsgroups.proc.train -output model_newsgroup -epoch 30\n",
    "!~/fastText-0.1.0/fastText test model_newsgroup.bin newsgroups.proc.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! A huge improvement. \n",
    "Learning rate dictates how fast a model learns. By default, it's 0.05. Model will converge faster with bigger learning rate, though bigger learning rate doesn't always mean better.\n",
    "Let's adjust it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  134132\n",
      "Number of labels: 20\n",
      "Progress: 100.0%  words/sec/thread: 3029077  lr: 0.000000  loss: 0.304259  eta: 0h0m 14m d: 2883247  lr: 0.488090  loss: 2.958491  eta: 0h0m  words/sec/thread: 2985736  lr: 0.477887  loss: 2.901876  eta: 0h0m %  words/sec/thread: 3001967  lr: 0.474388  loss: 2.781270  eta: 0h0m  words/sec/thread: 2968698  lr: 0.470701  loss: 2.441050  eta: 0h0m h0m  words/sec/thread: 2994171  lr: 0.456473  loss: 2.179527  eta: 0h0m 2.150629  eta: 0h0m   words/sec/thread: 3004711  lr: 0.449748  loss: 2.124960  eta: 0h0m 2994327  lr: 0.434942  loss: 1.768491  eta: 0h0m gress: 14.4%  words/sec/thread: 2999232  lr: 0.427841  loss: 1.691443  eta: 0h0m   loss: 1.650610  eta: 0h0m 15.8%  words/sec/thread: 3006859  lr: 0.421238  loss: 1.635735  eta: 0h0m gress: 18.1%  words/sec/thread: 3007479  lr: 0.409345  loss: 1.468156  eta: 0h0m gress: 19.3%  words/sec/thread: 3013634  lr: 0.403484  loss: 1.424582  eta: 0h0m s: 19.8%  words/sec/thread: 3009390  lr: 0.400915  loss: 1.374614  eta: 0h0m 0.396852  loss: 1.342530  eta: 0h0m gress: 21.2%  words/sec/thread: 3014308  lr: 0.393767  loss: 1.283958  eta: 0h0m 3012741  lr: 0.388975  loss: 1.272252  eta: 0h0m %  words/sec/thread: 3012752  lr: 0.386233  loss: 1.233931  eta: 0h0m gress: 23.4%  words/sec/thread: 3009870  lr: 0.383117  loss: 1.187067  eta: 0h0m s: 23.8%  words/sec/thread: 3010107  lr: 0.380948  loss: 1.150358  eta: 0h0m m   eta: 0h0m m   lr: 0.363738  loss: 1.052086  eta: 0h0m s: 27.7%  words/sec/thread: 3010239  lr: 0.361521  loss: 1.039613  eta: 0h0m gress: 29.6%  words/sec/thread: 3011305  lr: 0.352197  loss: 0.981747  eta: 0h0m gress: 30.3%  words/sec/thread: 3008577  lr: 0.348673  loss: 0.934100  eta: 0h0m s: 31.3%  words/sec/thread: 3009918  lr: 0.343708  loss: 0.927952  eta: 0h0m s: 31.8%  words/sec/thread: 3011940  lr: 0.340941  loss: 0.917313  eta: 0h0m s: 32.8%  words/sec/thread: 3014306  lr: 0.335973  loss: 0.861424  eta: 0h0m 34.7%  words/sec/thread: 3017366  lr: 0.326618  loss: 0.838199  eta: 0h0m   lr: 0.322024  loss: 0.828586  eta: 0h0m gress: 36.1%  words/sec/thread: 3015125  lr: 0.319281  loss: 0.805220  eta: 0h0m gress: 36.5%  words/sec/thread: 3014879  lr: 0.317433  loss: 0.789357  eta: 0h0m 0.769002  eta: 0h0m 3015461  lr: 0.305236  loss: 0.747433  eta: 0h0m gress: 39.1%  words/sec/thread: 3015943  lr: 0.304342  loss: 0.741999  eta: 0h0m m 0.706434  eta: 0h0m gress: 41.8%  words/sec/thread: 3016102  lr: 0.290776  loss: 0.687318  eta: 0h0m gress: 43.1%  words/sec/thread: 3020374  lr: 0.284274  loss: 0.686194  eta: 0h0m gress: 44.1%  words/sec/thread: 3020113  lr: 0.279411  loss: 0.671655  eta: 0h0m ss: 0.649976  eta: 0h0m gress: 46.8%  words/sec/thread: 3022254  lr: 0.265948  loss: 0.640770  eta: 0h0m 8.1%  words/sec/thread: 3021565  lr: 0.259401  loss: 0.620783  eta: 0h0m  gress: 48.9%  words/sec/thread: 3021762  lr: 0.255537  loss: 0.605919  eta: 0h0m s: 50.3%  words/sec/thread: 3022125  lr: 0.248661  loss: 0.588949  eta: 0h0m rds/sec/thread: 3024765  lr: 0.243900  loss: 0.585334  eta: 0h0m   eta: 0h0m gress: 54.0%  words/sec/thread: 3027871  lr: 0.230145  loss: 0.563088  eta: 0h0m s: 54.3%  words/sec/thread: 3027145  lr: 0.228440  loss: 0.556263  eta: 0h0m 55.3%  words/sec/thread: 3027795  lr: 0.223644  loss: 0.544897  eta: 0h0m gress: 55.7%  words/sec/thread: 3026959  lr: 0.221437  loss: 0.534660  eta: 0h0m s: 56.8%  words/sec/thread: 3028014  lr: 0.215797  loss: 0.530866  eta: 0h0m gress: 57.6%  words/sec/thread: 3028474  lr: 0.212207  loss: 0.526648  eta: 0h0m 0.514785  eta: 0h0m gress: 59.1%  words/sec/thread: 3027447  lr: 0.204745  loss: 0.509747  eta: 0h0m 0h0m gress: 60.4%  words/sec/thread: 3026552  lr: 0.197776  loss: 0.497839  eta: 0h0m gress: 61.7%  words/sec/thread: 3027458  lr: 0.191503  loss: 0.488554  eta: 0h0m gress: 62.0%  words/sec/thread: 3027254  lr: 0.189984  loss: 0.487573  eta: 0h0m gress: 63.3%  words/sec/thread: 3027136  lr: 0.183275  loss: 0.476769  eta: 0h0m gress: 63.8%  words/sec/thread: 3026657  lr: 0.180940  loss: 0.471176  eta: 0h0m gress: 65.3%  words/sec/thread: 3027749  lr: 0.173739  loss: 0.464390  eta: 0h0m m   lr: 0.162651  loss: 0.444470  eta: 0h0m %  words/sec/thread: 3023786  lr: 0.161218  loss: 0.437687  eta: 0h0m   words/sec/thread: 3026784  lr: 0.143124  loss: 0.427296  eta: 0h0m   words/sec/thread: 3026170  lr: 0.139326  loss: 0.420153  eta: 0h0m sec/thread: 3029252  lr: 0.131902  loss: 0.410816  eta: 0h0m gress: 74.1%  words/sec/thread: 3027968  lr: 0.129703  loss: 0.401493  eta: 0h0m gress: 77.8%  words/sec/thread: 3029761  lr: 0.111127  loss: 0.388522  eta: 0h0m a: 0h0m gress: 79.7%  words/sec/thread: 3029383  lr: 0.101336  loss: 0.376815  eta: 0h0m gress: 83.7%  words/sec/thread: 3028928  lr: 0.081723  loss: 0.357810  eta: 0h0m gress: 84.7%  words/sec/thread: 3029230  lr: 0.076410  loss: 0.355956  eta: 0h0m s: 85.3%  words/sec/thread: 3029656  lr: 0.073514  loss: 0.355697  eta: 0h0m gress: 87.1%  words/sec/thread: 3029185  lr: 0.064454  loss: 0.347090  eta: 0h0m 3030140  lr: 0.053048  loss: 0.340512  eta: 0h0m gress: 89.9%  words/sec/thread: 3028970  lr: 0.050491  loss: 0.334277  eta: 0h0m 3417  loss: 0.329185  eta: 0h0m gress: 92.6%  words/sec/thread: 3028464  lr: 0.037028  loss: 0.324059  eta: 0h0m gress: 92.7%  words/sec/thread: 3027874  lr: 0.036539  loss: 0.321984  eta: 0h0m   words/sec/thread: 3031005  lr: 0.027988  loss: 0.320793  eta: 0h0m 94.9%  words/sec/thread: 3030183  lr: 0.025746  loss: 0.317994  eta: 0h0m %  words/sec/thread: 3030425  lr: 0.022950  loss: 0.317808  eta: 0h0m gress: 96.1%  words/sec/thread: 3030732  lr: 0.019526  loss: 0.315891  eta: 0h0m gress: 96.8%  words/sec/thread: 3029535  lr: 0.015959  loss: 0.311762  eta: 0h0m gress: 98.6%  words/sec/thread: 3029336  lr: 0.006829  loss: 0.308454  eta: 0h0m gress: 99.9%  words/sec/thread: 3029407  lr: 0.000657  loss: 0.305168  eta: 0h0m \n",
      "N\t1314\n",
      "P@1\t0.863\n",
      "R@1\t0.863\n",
      "Number of examples: 1314\n"
     ]
    }
   ],
   "source": [
    "!~/fastText-0.1.0/fastText supervised -input newsgroups.proc.train -output model_newsgroup -epoch 30 -lr 0.5\n",
    "!~/fastText-0.1.0/fastText test model_newsgroup.bin newsgroups.proc.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, the results improves! \n",
    "\n",
    "Now, instead of using **bags of words**, let's try using **bags of N-grams**. We'll use **Bigrams (N=2)** here.  \n",
    "N-grams provide a sense of word order. \n",
    "\n",
    "Sentence: \"I love eating pizza\"  \n",
    "Bigrams for the above sentence: \"I love\", \"love eating\", \"eating pizza\".  \n",
    "By looking at the N-grams, it is possible to reconstruct a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  134132\n",
      "Number of labels: 20\n",
      "Progress: 100.0%  words/sec/thread: 1118522  lr: 0.000000  loss: 0.380091  eta: 0h0m m 3.0%  words/sec/thread: 1024208  lr: 0.484844  loss: 3.010075  eta: 0h0m 0h0m   lr: 0.483629  loss: 3.004493  eta: 0h0m 0m m 0.480930  loss: 2.976413  eta: 0h0m h0m 0h0m   words/sec/thread: 547270  lr: 0.478338  loss: 2.959235  eta: 0h0m 0h0m m 0h0m 5.1%  words/sec/thread: 468332  lr: 0.474316  loss: 2.907886  eta: 0h0m 5.5%  words/sec/thread: 443042  lr: 0.472426  loss: 2.887521  eta: 0h0m 0.472054  loss: 2.883266  eta: 0h0m h0m 415121  lr: 0.469254  loss: 2.848401  eta: 0h0m   lr: 0.467964  loss: 2.827346  eta: 0h0m   lr: 0.466598  loss: 2.814551  eta: 0h0m   words/sec/thread: 395094  lr: 0.465687  loss: 2.807956  eta: 0h0m 0.465539  loss: 2.805873  eta: 0h0m   words/sec/thread: 390934  lr: 0.465032  loss: 2.804093  eta: 0h0m m 0h0m 7.9%  words/sec/thread: 372899  lr: 0.460402  loss: 2.742176  eta: 0h0m 2.723889  eta: 0h0m %  words/sec/thread: 365060  lr: 0.458009  loss: 2.696780  eta: 0h0m 2.678373  eta: 0h0m   lr: 0.456339  loss: 2.666314  eta: 0h0m   loss: 2.475317  eta: 0h0m 2.352168  eta: 0h0m   words/sec/thread: 409708  lr: 0.434405  loss: 2.277211  eta: 0h0m   words/sec/thread: 413964  lr: 0.433331  loss: 2.258626  eta: 0h0m   words/sec/thread: 418555  lr: 0.432188  loss: 2.229557  eta: 0h0m   words/sec/thread: 446018  lr: 0.425265  loss: 2.112340  eta: 0h0m   words/sec/thread: 486367  lr: 0.414566  loss: 1.965747  eta: 0h0m 0m   words/sec/thread: 517531  lr: 0.406131  loss: 1.804003  eta: 0h0m   words/sec/thread: 539317  lr: 0.400108  loss: 1.725434  eta: 0h0m   words/sec/thread: 582186  lr: 0.387220  loss: 1.573344  eta: 0h0m ad: 593191  lr: 0.383741  loss: 1.534379  eta: 0h0m ad: 608581  lr: 0.378762  loss: 1.482307  eta: 0h0m   words/sec/thread: 613069  lr: 0.377242  loss: 1.463224  eta: 0h0m   words/sec/thread: 619158  lr: 0.375200  loss: 1.438563  eta: 0h0m 0m 5%  words/sec/thread: 641763  lr: 0.367392  loss: 1.351054  eta: 0h0m   words/sec/thread: 649137  lr: 0.364720  loss: 1.338055  eta: 0h0m   words/sec/thread: 663355  lr: 0.359555  loss: 1.296361  eta: 0h0m ad: 676722  lr: 0.354502  loss: 1.248321  eta: 0h0m ad: 680285  lr: 0.353118  loss: 1.220184  eta: 0h0m 32.8%  words/sec/thread: 722438  lr: 0.335894  loss: 1.116957  eta: 0h0m   words/sec/thread: 726358  lr: 0.334195  loss: 1.106253  eta: 0h0m   words/sec/thread: 741042  lr: 0.327789  loss: 1.065701  eta: 0h0m   eta: 0h0m   words/sec/thread: 760228  lr: 0.318938  loss: 1.019112  eta: 0h0m   loss: 1.005276  eta: 0h0m 1.000317  eta: 0h0m   words/sec/thread: 772194  lr: 0.313004  loss: 0.970771  eta: 0h0m   words/sec/thread: 780899  lr: 0.308765  loss: 0.960203  eta: 0h0m 0m   words/sec/thread: 785385  lr: 0.306458  loss: 0.949320  eta: 0h0m   words/sec/thread: 789989  lr: 0.304068  loss: 0.934231  eta: 0h0m   words/sec/thread: 796929  lr: 0.300401  loss: 0.913122  eta: 0h0m   words/sec/thread: 810467  lr: 0.293340  loss: 0.888498  eta: 0h0m 2.0%  words/sec/thread: 816539  lr: 0.290062  loss: 0.878783  eta: 0h0m h0m 43.3%  words/sec/thread: 828701  lr: 0.283326  loss: 0.850729  eta: 0h0m   lr: 0.279903  loss: 0.837819  eta: 0h0m   loss: 0.831800  eta: 0h0m 44.7%  words/sec/thread: 840528  lr: 0.276439  loss: 0.827863  eta: 0h0m 45.4%  words/sec/thread: 846186  lr: 0.273057  loss: 0.817347  eta: 0h0m   words/sec/thread: 870227  lr: 0.258290  loss: 0.769858  eta: 0h0m   words/sec/thread: 874160  lr: 0.255912  loss: 0.765643  eta: 0h0m h0m   lr: 0.243549  loss: 0.731529  eta: 0h0m   words/sec/thread: 893532  lr: 0.242903  loss: 0.725770  eta: 0h0m   words/sec/thread: 894958  lr: 0.241784  loss: 0.724680  eta: 0h0m h0m   words/sec/thread: 909607  lr: 0.231405  loss: 0.692528  eta: 0h0m   words/sec/thread: 911723  lr: 0.229838  loss: 0.688304  eta: 0h0m 0.685433  eta: 0h0m   words/sec/thread: 917093  lr: 0.225864  loss: 0.679132  eta: 0h0m   words/sec/thread: 921171  lr: 0.222697  loss: 0.670792  eta: 0h0m 0.217959  loss: 0.657183  eta: 0h0m   words/sec/thread: 929561  lr: 0.216305  loss: 0.650510  eta: 0h0m   words/sec/thread: 935014  lr: 0.212141  loss: 0.645870  eta: 0h0m   words/sec/thread: 939056  lr: 0.208947  loss: 0.640158  eta: 0h0m 9733  loss: 0.623526  eta: 0h0m   words/sec/thread: 954283  lr: 0.196586  loss: 0.620082  eta: 0h0m   words/sec/thread: 955253  lr: 0.195714  loss: 0.618725  eta: 0h0m   words/sec/thread: 960962  lr: 0.190881  loss: 0.609498  eta: 0h0m   words/sec/thread: 968620  lr: 0.184196  loss: 0.599340  eta: 0h0m 0h0m   words/sec/thread: 975119  lr: 0.178419  loss: 0.588471  eta: 0h0m   words/sec/thread: 980733  lr: 0.173342  loss: 0.581560  eta: 0h0m ad: 983651  lr: 0.170599  loss: 0.577127  eta: 0h0m h0m   words/sec/thread: 987956  lr: 0.166621  loss: 0.569055  eta: 0h0m   words/sec/thread: 990382  lr: 0.164400  loss: 0.563609  eta: 0h0m   words/sec/thread: 998106  lr: 0.156912  loss: 0.553372  eta: 0h0m 0.548993  eta: 0h0m gress: 69.2%  words/sec/thread: 1001056  lr: 0.153887  loss: 0.543575  eta: 0h0m 70.2%  words/sec/thread: 1006040  lr: 0.149067  loss: 0.539090  eta: 0h0m gress: 70.6%  words/sec/thread: 1007941  lr: 0.147142  loss: 0.537298  eta: 0h0m gress: 71.6%  words/sec/thread: 1012657  lr: 0.142165  loss: 0.530018  eta: 0h0m 72.1%  words/sec/thread: 1015093  lr: 0.139629  loss: 0.526258  eta: 0h0m s: 73.5%  words/sec/thread: 1021717  lr: 0.132642  loss: 0.517509  eta: 0h0m gress: 75.1%  words/sec/thread: 1029246  lr: 0.124487  loss: 0.507001  eta: 0h0m   loss: 0.503414  eta: 0h0m s: 76.0%  words/sec/thread: 1033169  lr: 0.120084  loss: 0.502433  eta: 0h0m 0h0m  gress: 76.7%  words/sec/thread: 1036646  lr: 0.116397  loss: 0.497673  eta: 0h0m gress: 77.0%  words/sec/thread: 1037579  lr: 0.115230  loss: 0.495114  eta: 0h0m gress: 77.1%  words/sec/thread: 1038449  lr: 0.114277  loss: 0.494592  eta: 0h0m gress: 77.4%  words/sec/thread: 1039768  lr: 0.112812  loss: 0.493005  eta: 0h0m gress: 77.8%  words/sec/thread: 1041581  lr: 0.110925  loss: 0.491153  eta: 0h0m gress: 79.5%  words/sec/thread: 1048702  lr: 0.102357  loss: 0.479240  eta: 0h0m 085  lr: 0.095854  loss: 0.472039  eta: 0h0m m 5524  loss: 0.451230  eta: 0h0m gress: 85.2%  words/sec/thread: 1071171  lr: 0.073951  loss: 0.449969  eta: 0h0m s: 86.3%  words/sec/thread: 1075411  lr: 0.068664  loss: 0.443849  eta: 0h0m gress: 87.2%  words/sec/thread: 1078798  lr: 0.063878  loss: 0.437015  eta: 0h0m h0m 0h0m gress: 90.1%  words/sec/thread: 1086342  lr: 0.049725  loss: 0.423255  eta: 0h0m 0h0m   loss: 0.417492  eta: 0h0m   loss: 0.416107  eta: 0h0m gress: 92.4%  words/sec/thread: 1094239  lr: 0.038063  loss: 0.414531  eta: 0h0m 0h0m 95.5%  words/sec/thread: 1104369  lr: 0.022691  loss: 0.401933  eta: 0h0m gress: 97.6%  words/sec/thread: 1111175  lr: 0.012099  loss: 0.392547  eta: 0h0m m \n",
      "N\t1314\n",
      "P@1\t0.868\n",
      "R@1\t0.868\n",
      "Number of examples: 1314\n"
     ]
    }
   ],
   "source": [
    "!~/fastText-0.1.0/fastText supervised -input newsgroups.proc.train -output model_newsgroup \\\n",
    "-epoch 30 -lr 0.5 -wordNgrams 2\n",
    "!~/fastText-0.1.0/fastText test model_newsgroup.bin newsgroups.proc.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may check out other hyperparameters you can adjust on the Fasttext repo: https://github.com/facebookresearch/fastText/blob/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have chosen the best model based on validation performance, we can test how it perform on actual test set.  \n",
    "Remember the lecture? ***Never*** tune your model on test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t7532\r\n",
      "P@1\t0.764\r\n",
      "R@1\t0.764\r\n",
      "Number of examples: 7532\r\n"
     ]
    }
   ],
   "source": [
    "!~/fastText-0.1.0/fastText test model_newsgroup.bin newsgroups.proc.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Try training the fastText using IMDB Large Movie Review Dataset and fine-tune the hyperparameters."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
